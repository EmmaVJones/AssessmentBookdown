# Automated Assessment Methods

```{r automated assessment setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
```

This chapter details the specific steps required to transform the raw data outlined in the [Data Organization](#data-organization) chapter into preliminary assessment results. The general process the automated assessment specifies is to perform data manipulation and organization steps to prepare the data for analysis, apply specific assessment functions to the appropriate data that specify the assessment guidance and Water Quality Standards (WQS), and output a "stations table" that uses data from the assessment window to summarize each station for review and bulk upload into CEDS. See the [Automated Output](#automated-output) section to understand the information provided in the stations table output. 

The automated assessment scripts are trained to systematically apply assessment guidance and WQS to all data for which the appropriate metadata is provided. This supervised system merely applies the rules that developers have programmed to mimic assessment protocols. These scripts do not provide assessment decisions. It is up to the human reviewer (regional assessor) to either accept the automated result or use best professional judgment to e.g. redact questionable data or interpret complex natural systems. 


## Metadata Attribution

The automated assessment process hinges on each station having the appropriate metadata to analyze all raw data. The required metadata for each station include what Water Quality Standards apply to the station and which Assessment Unit(s) describe the station. One or more station can be included in an Assessment Units. It is ultimately the Assessment Units that are used to determine whether or not designated uses are met, which is where the automation stops and the human analysis component is required. 

In practice, metadata are spatially joined to stations by a rigorous data organization, spatial joining, and QA process that is detailed below. This automated process runs on the Assessment Data Analyst's computer and results are provided to regional assessment staff for individual review. This application is known as the [Regional Metadata Validation Tool](https://rconnect.deq.virginia.gov/RegionalAssessmentMetadataValidation/) and is hosted on the R server. It is up to each regional assessor to manually review each suggested metadata link prior to the assessment start date. After stations have completed the manual review process, they can be analyzed using the automated assessment scripts. Detailed intructions on how to use the [Regional Metadata Validation Tool](https://rconnect.deq.virginia.gov/RegionalAssessmentMetadataValidation/) is available in the [Regional Metadata Validation Tool How To](#regional-metadata-validation-tool-how-to) section.

### Distinct Sites

Before station metadata can be linked to stations, a list of unique stations that were sampled in a given assessment window is required. Because multiple data sources are combined for an assessment, each unique station from each data source with data in the given IR window are included in this list. 

For the purposes of the Automated Assessment User Guide, we will overview the process with a snippet of conventionals and citizen monitoring data types. Please see the [official script](https://github.com/EmmaVJones/IR2024/blob/main/1.preprocessData/HowToPreprocessData.Rmd) for more information.

```{r automatedAssessment libraries}
library(tidyverse)
library(sf)
```

```{r automatedAssessment conventionals}
conventionals <- readRDS('exampleData/conventionals.RDS') %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
  dplyr::select(FDT_STA_ID, Latitude, Longitude) %>% 
  mutate(Data_Source = 'DEQ')
conventionals[1:50,] %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```

```{r automatedAssessment citmon}
citmon <-  readRDS('exampleData/citmon.RDS') %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
  dplyr::select(FDT_STA_ID, Latitude, Longitude, Data_Source)
citmon[1:50,] %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```


```{r automatedAssessment distinctSites}
distinctSites <- bind_rows(conventionals, citmon)
distinctSites[1:50,] %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```

This process is repeated for all datasets that have data included in the assessment window. These multiple datasets are combined into a single object named distinctSites that we will then compare to existing WQS and AU information. Where stations in our distinctSites object lack either of these pieces of metadata, we must attribute them.

### Join Assessment Region and Subbasin Information

For rendering purposes in the metadata review application, it is important to have each station linked to the appropriate Assessment Region and Subbasin to limit the amount of data called into the application just to essential information. This information is also important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing. The next chunk reads in the necessary assessment region and subbasin spatial data and spatially joins all sites to these layers. If the sites are missing from the distinctSites_sf object, that means the point plots outside either the assessment region or subbasin polygon. These missingSites are dealt with individually and forced to join to the nearest assessment region and subbasin before rejoining the distinctSites_sf object.

```{r automatedAssessment spatial assessment region subbasin, eval = F}

assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

subbasinLayer <- st_read('GIS/DEQ_VAHUSB_subbasins_EVJ.shp')  %>%
  rename('SUBBASIN' = 'SUBBASIN_1')


distinctSites_sf <- st_as_sf(distinctSites,
                            coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                            remove = F, # don't remove these lat/lon cols from df
                            crs = 4326) %>% # add coordinate reference system, needs to be geographic for now bc entering lat/lng, 
  st_intersection(assessmentLayer ) %>%
  st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) 

# if any joining issues occur, that means that there are stations that fall outside the joined polygon area
# we need to go back in and fix them manually
if(nrow(distinctSites_sf) < nrow(distinctSites)){
  missingSites <- filter(distinctSites, ! FDT_STA_ID %in% distinctSites_sf$FDT_STA_ID) %>%
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
             remove = F, # don't remove these lat/lon cols from df
             crs = 4326) 
  
  closest <- mutate(assessmentLayer[0,], FDT_STA_ID =NA) %>%
    dplyr::select(FDT_STA_ID, everything())
  for(i in seq_len(nrow(missingSites))){
    closest[i,] <- assessmentLayer[which.min(st_distance(assessmentLayer, missingSites[i,])),] %>%
      mutate(FDT_STA_ID = missingSites[i,]$FDT_STA_ID) %>%
      dplyr::select(FDT_STA_ID, everything())
  }
  
  missingSites <- left_join(missingSites, closest %>% st_drop_geometry(),
                            by = 'FDT_STA_ID') %>%
    st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) %>%
    dplyr::select(-c(geometry), geometry) %>%
    dplyr::select(names(distinctSites_sf))
  
  
  distinctSites_sf <- rbind(distinctSites_sf, missingSites)
}

```


### Join Stations to WQS

All stations are then spatially joined to the current WQS spatial layers to link each unique StationID to a unique WQS_ID. These unique WQS_ID's are a concatination of the waterbody type, basin code (e.g. 2A, 2B, 2C, etc.), and a number associated with each segment in the spatial layer.

Waterbody types include:

* RL = Riverine Line
* LP = Lacustrine Polygon
* EL = Estuarine Line
* EP = Estuarine Polygon

Since transitioning the WQS storage from local (individual assessor files) to a centralized system (on the R server for multiple program uses), the number of stations that require WQS attribution decreases significantly each IR cycle. To attribute each station to WQS information, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can send that information to an interactive application for humans to manually verify.

Where WQS_ID information is already available for stations, they are first removed from the WQS snapping process as to not repeat efforts unnecessarily. WQS_ID's are maintained across WQS layer updates, so any new WQS information will be updated when the stations are joined to the WQS metadata.

You can view the available WQSlookup table stored on the server by using the below script. Please see the [DEQ R Methods Encyclopedia](https://rconnect.deq.virginia.gov/MethodsEncyclopedia/connectToConnectPins.html) for information on how to retrieve pinned data from the server.

```{r automatedAssessment WQSlookup, eval=FALSE }
WQStableExisting <- pin_get('ejones/WQSlookup', board = 'rsconnect')

distinctSitesToDoWQS <- filter(distinctSites_sf, ! FDT_STA_ID  %in% WQStableExisting$StationID)
```


Here is the table used to store link information from stations to appropriate WQS.

```{r automatedAssessment WQStable, eval=F}
WQStable <- tibble(StationID = NA, WQS_ID = NA)
```


### Spatially Join Polygons

Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons

Here we find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas. This process also removes any estuarine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).

We will bring in (source) a custom function that runs the analysis for us, feed it our latest WQS information, and let the function run across all input stations. You can see the latest version of the sourced script in [this repository.](https://github.com/EmmaVJones/IR2024/tree/main/1.preprocessData)

```{r automatedAssessment estuary methods, eval = F}
source('preprocessingModules/WQS_estuaryPoly.R')

# Bring in estuary layer
estuarinePolys <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp', 
                          fid_column_name = "OBJECTID") %>%
  st_transform(4326)
  
WQStable <- estuaryPolygonJoin(estuarinePolys, distinctSitesToDoWQS, WQStable)

rm(estuarinePolys) # clean up workspace
```

Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r automatedAssessment remove estuary poly sites, eval = F}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


##### Lake Polygons

Next find any sites that fall into a lake WQS polygon. This method is applied to all subbasins at once as it is a simple spatial join. You can see the latest version of the sourced script in [this repository.](https://github.com/EmmaVJones/IR2024/tree/main/1.preprocessData)


```{r automatedAssessment lake methods, eval = F}
source('preprocessingModules/WQS_lakePoly.R')

lakesPoly <- st_read('../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp',
                     fid_column_name = "OBJECTID") %>%
  st_transform(4326)

WQStable <- lakePolygonJoin(lakesPoly, distinctSitesToDoWQS, WQStable)

rm(lakesPoly) # clean up workspace
```


Remove stations that fell inside lake polygons from the 'to do' list.

```{r automatedAssessment remove lake poly sites, eval = F}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


### Spatially Join WQS Lines

Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS and where stations remain we will try the estuarine WQS snap.

##### Riverine Lines

To do this join, we will buffer all sites that don't fall into a polygon layer by a set sequence of distances. The output will add a field called `Buffer Distance` to the WQStable to indicate distance required for snapping. If more than one segment is found within a set buffer distance, then many rows will be attached to the WQStable with the single identifying station name. It is up to the QA tool to help the user determine which of these UID's are correct and drop the other records.

We then remove any riverine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).

```{r automatedAssessment riverine methods, eval = F}
source('snappingFunctions/snapWQS.R')

riverine <- st_read('../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp',
                    fid_column_name = "OBJECTID") 

WQStable <- snapAndOrganizeWQS(distinctSitesToDoWQS, 'FDT_STA_ID', riverine, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)
rm(riverine) # clean up workspace
```


Remove stations that attached to riverine segments from the 'to do' list.

```{r automatedAssessment remove riverine sites, eval = F}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


We can use this one last opportunity to test stations that didn't connect to the riverine WQS against the estuarine lines WQS as a one last hope of attributing some WQS information. We will take all stations from the WQStable that didn't snap to any WQS segments (`Buffer Distance` =='No connections within 80 m') and add those back in to our distinctSitesToDoWQS list to try to snap them to the estuarine lines spatial data. 

```{r automatedAssessment no riverine snaps, eval = F}
distinctSitesToDoWQS <- filter(WQStable, `Buffer Distance` =='No connections within 80 m') %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %>%
              rename('StationID'= 'FDT_STA_ID'), by='StationID') %>%
  dplyr::select(-c(geometry)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)
                            
```



##### Estuarine Lines

If a site doesn't attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time.
Removes any estuary lines sites from the data frame of unique sites that need WQS information.

```{r automatedAssessment estuarine lines methods, eval = F}
source('snappingFunctions/snapWQS.R')

estuarineLines <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp' , fid_column_name = "OBJECTID") #%>%
  #st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform(4326)

# Only send sites to function that could be in estuarine environment
WQStable <- snapAndOrganizeWQS(filter(distinctSitesToDoWQS, SUBBASIN %in% c("Potomac River",
                                                                            "Rappahannock River", 
                                                                            "Atlantic Ocean Coastal",
                                                                            "Chesapeake Bay Tributaries",
                                                                            "Chesapeake Bay - Mainstem",
                                                                            "James River - Lower",  
                                                                            "Appomattox River",
                                                                            "Chowan River", 
                                                                            "Atlantic Ocean - South" ,
                                                                            "Dismal Swamp/Albemarle Sound")),
                               'StationID', estuarineLines, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)

rm(estuarineLines)
```

Remove stations that attached to estuarine segments from the 'to do' list.

```{r automatedAssessment remove estuary line sites, eval = F}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! StationID %in% WQStable$StationID)
```


Double check no stations were lost in these processes. 

```{r double check no one lost, eval = F}
#Make sure all stations from original distinct station list have some sort of record (blank or populated) int the WQStable.

distinctSitesToDoWQS <- filter(distinctSitesToDo, ! FDT_STA_ID  %in% WQStableExisting$StationID)
distinctSitesToDoWQS$FDT_STA_ID[!(distinctSitesToDoWQS$FDT_STA_ID %in% unique(WQStable$StationID))]
```

### Assign something to WQS_ID so sites will not fall through the cracks when application filtering occurs

We don't want to give everyone all the stations that didn't snap to something, so we need to at least partially assign a WQS_ID so the stations get into the correct subbasin on initial filter.

If a station snapped to nothing, we will assigning it a RL WQS_ID and subbasin it falls into by default.

```{r automatedAssessment blank WQS_ID partially filled in, eval = F}
WQStableMissing <- filter(WQStable, is.na(WQS_ID)) %>%
  # drop from list if actually fixed by snap to another segment
  filter(! StationID %in% filter(WQStable, str_extract(WQS_ID, "^.{2}") %in% c('EL','LP','EP'))$StationID) %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, BASIN_CODE) %>%
              st_drop_geometry(), by = c('StationID' = 'FDT_STA_ID')) %>%
  # some fixes for missing basin codes so they will match proper naming conventions for filtering
  mutate(BASIN_CODE1 = case_when(is.na(BASIN_CODE) ~ str_pad(
    ifelse(grepl('-', str_extract(StationID, "^.{2}")), str_extract(StationID, "^.{1}"), str_extract(StationID, "^.{2}")), 
    width = 2, side = 'left', pad = '0'),
    TRUE ~ as.character(BASIN_CODE)),
    BASIN_CODE2 = str_pad(BASIN_CODE1, width = 2, side = 'left', pad = '0'),
    WQS_ID = paste0('RL_', BASIN_CODE2,'_NA')) %>%
  dplyr::select(-c(BASIN_CODE, BASIN_CODE1, BASIN_CODE2))

WQStable <- filter(WQStable, !is.na(WQS_ID)) %>% 
  filter(! StationID %in% WQStableMissing$StationID) %>%
  bind_rows(WQStableMissing)
```





### Join Stations to AUs



### Where does this information go?

The above analyses help populate the [Regional Metadata Validation Tool](https://rconnect.deq.virginia.gov/RegionalAssessmentMetadataValidation/). After assessors manually review each station that requires WQS/AU information, this data is consolidated and stored on the R server as a pinned dataset (see []() for details on that process). That dataset feeds subsequent tools including the [Riverine Assessment App](https://rconnect.deq.virginia.gov/IR2022riverineAssessmentApplication/), [Lakes Assessment App](https://rconnect.deq.virginia.gov/IR2022LakesAssessmentApp/), and [Bioassessment Dashboard](https://rconnect.deq.virginia.gov/IR2022BioassessmentDashboard/) in addition to querying tools including the [CEDS WQM Data Query Tool](https://rconnect.deq.virginia.gov/CEDSWQMDataQueryTool/) and [CEDS Benthic Data Query Tool](https://rconnect.deq.virginia.gov/CEDSBenthicDataQueryTool/). The data is provided via the internal GIS services in the WQM Stations (All stations with full attributes) layer hosted on the [GIS REST service](https://gis.deq.virginia.gov/arcgis/rest/services/staff/DEQInternalDataViewer/MapServer/104) and [GIS Staff Application](https://gis.deq.virginia.gov/GISStaffApplication/). In addition to the published tools that source this data, the dataset is included in countless data analysis projects and products. The entire dataset may be pulled from the R Connect service using the following script. Please see the [DEQ R Methods Encyclopedia](https://rconnect.deq.virginia.gov/MethodsEncyclopedia/connectToConnectPins.html) for information on how to retrieve pinned data from the server.

```{r automatedAssessment pull pinned WQS data, eval=F}
pin_get('ejones/WQSlookup', board = 'rsconnect') # raw lookup table
pin_get("ejones/WQSlookup-withStandards", board = "rsconnect") # lookup table with WQS information joined 
```


## Organize Metadata

After all stations for a given assessment window have the appropriate WQS and AU information attributed, there are a number of data organization steps that still need to happen before the data are ready for the automated assessment scripts. These steps include:

* Organizing all new WQS/AU metadata from the R server and adding new data to pinned datasets
* Reorganizing the conventionals dataset to match schema for automated assessment scripts
* Add Citizen Monitoring Data to conventionals
* Adding Secchi Depth to conventionals dataset
* Clean up PCB dataset 
* Clean up Fish Tissue dataset

**The sections below are purposefully sparse as the methods are in flux given that new data are available in ODS. Stay tuned to see how these methods flesh out for more streamlined data organization in future cycles.**


### Organizing all new WQS/AU metadata from the R server

A server administrator must retrieve all the WQS/AU information from the attribution application. This directory is consolidated into a single dataset of all new StationID-WQS/AU records. It is important to verify that all expected StationIDs (all distinct stations from the conventionals and citizen monitoring datasets) have WQS and AU information before proceeding. If stations lack this information they will not be assessed accurately.

#### Adding new WQS data to pinned datasets

Once WQS/AU information is available for all stations, the data is appended to the master WQS information pin on the R server in the case of WQS information. AU information can change cycle to cycle, so the IR year for the StationID-AU information is added to the dataset before pinning back to the master AU information pin. 

To expedite the transfer of actual WQS information, another pin contains the actual WQS metadata as well as relevant StationID. All new stations attributed need to be joined to the relevant WQS layer to extract this metadata before it can be appended to the master WQS information pin with standards. The script below details this process by bringing in each necessary WQS layer, joining it to the WQSlookup information, and then pinning it to the R server. 

```{r automated assessment pin WQS, eval = F}
# table with StationID and WQS_ID information that needs actual WQS metadata
WQSlookupToDo <- tibble(StationID = NA, WQS_ID = NA) # blank for example purposes

#bring in Riverine layers, valid for the assessment window
riverine <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'riverine_05082020') %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, riverine) %>%
  filter(!is.na(CLASS)) # drop sites that didn't actually join 
rm(riverine)

lacustrine <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'lakes_reservoirs_05082020') %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, lacustrine) %>%
  filter(!is.na(CLASS)) %>% # drop sites that didn't actually join 
  bind_rows(WQSlookupFull) # add these to the larger dataset
rm(lacustrine)

estuarineLines <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'estuarinelines_05082020') %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, estuarineLines) %>%
  filter(!is.na(CLASS)) %>% # drop sites that didn't actually join 
  bind_rows(WQSlookupFull) # add these to the larger dataset
rm(estuarineLines)

estuarinePolys <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'estuarinepolygons_05082020') %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, estuarinePolys) %>%
  filter(!is.na(CLASS)) %>% # drop sites that didn't actually join 
  bind_rows(WQSlookupFull) # add these to the larger dataset
rm(estuarinePolys)

WQSlookup_withStandards_pin <- bind_rows(WQSlookup_withStandards, WQSlookupFull) # for pin

# pin to server
pin(WQSlookup_withStandards, description = "WQS lookup table with Standards from metadata attribution application", board = "rsconnect")
```



### Reorganize Conventionals Dataset

The official (SAS) conventionals data schema tends to vary from IR to IR. It is essential that this data are provided to the R scripts exactly how the scripts expect data, so each cycle the provided conventionals dataset must be meticulously QAed to ensure all data are of expected name/type. Additionally, in order to automate the assessment of citizen monitoring data, we must augment the provided conventionals dataset to accommodate the level schema for each citizen monitoring parameter. 

The nuanced data manipulation steps required to convert the conventionals data and citizen monitoring data from the provided format to the required format for automated analyses are beyond the scope of this book. Instead, the required data schema for automated assessment is provided below.

```{r automatedAssessment conventional schema}
conventionalsSchema <- readRDS('exampleData/schemaFin.RDS')

conventionalsSchema %>% # preview data
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```

###  Add Citizen Monitoring Data to conventionals

Once the citizen monitoring data is queried, QAed, and matches the required conventionals schema (above), the data can be appended to the conventionals dataset for automated analysis.


### Adding Secchi Depth to conventionals dataset

This data are not included in the official (SAS) conventionals data pull but are required for Trophic State Index (TSI) analyses for some lacustrine stations. The below script will only work if a user's credentials have access to production ODS wqm data. The example script below first finds all unique stations in the conventionals dataset before querying secchi depth information for those stations for the given assessment window. The date range requires updating each assessment cycle.

```{r automated assesssment ODS secchi query, eval=FALSE}
# dataset of all the unique stations that we need to organize
conventionals_distinct <- conventionals %>%
  distinct(FDT_STA_ID, .keep_all = T) %>%
  # remove any data to avoid confusion
  dplyr::select(FDT_STA_ID:FDT_COMMENT, Latitude:Data_Source) %>%
  filter(!is.na(FDT_STA_ID))

# Query secchi data for unique conventionals stations in the appropriate data window
library(pool)
library(dbplyr)
### connect to Production Environment
pool <- dbPool(
  drv = odbc::odbc(),
  Driver = "ODBC Driver 11 for SQL Server",
  Server= "DEQ-SQLODS-PROD,50000",
  dbname = "ODS",
  trusted_connection = "yes"
)

stationSecchiDepth <- pool %>% tbl(in_schema('wqm', "Wqm_Field_Data_View")) %>%
  filter(Fdt_Sta_Id %in% !! conventionals_distinct$FDT_STA_ID & 
           between(as.Date(Fdt_Date_Time), "2014-12-31", "2020-12-31") &
           !is.na(Fdt_Secchi_Depth)) %>% # x >= left & x <= right
  dplyr::select(Fdt_Sta_Id, Fdt_Date_Time, Fdt_Depth, Fdt_Secchi_Depth) %>%
  as_tibble() %>%
  mutate(Date = as.Date(Fdt_Date_Time)) %>%
    dplyr::select(FDT_STA_ID = Fdt_Sta_Id, Date, FDT_DEPTH = Fdt_Depth, Fdt_Secchi_Depth) 
```


Once the secchi data is acquired, it can be attached to the original conventionals dataset carefully as detailed below.

```{r  automated assesssment ODS secchi data smash, eval=FALSE}
conventionalsArchive <- conventionals %>%
  mutate(Date = as.Date(FDT_DATE_TIME))

conventionals <- left_join(conventionalsArchive, stationSecchiDepth, by = c('FDT_STA_ID', 'Date', 'FDT_DEPTH')) %>%
  mutate(SECCHI_DEPTH_M = Fdt_Secchi_Depth) %>%
  dplyr::select(-c(Fdt_Secchi_Depth, Date))
```


### Clean up PCB dataset 

IR 2022 had to use fuzzyjoining to get PCB StationID to a real DEQ StationID for the data to show up in apps.

### Clean up Fish Tissue dataset

Who knows what this will look like. We really need a better system for updating data.

### stationsTablebegin

**No scripts/data here on purpose bc method in flux since moved to CEDS WQA**

The so called "stationsTablebegin" dataset contains all the stations that need to be addressed in a given IR cycle. This is not limited to just stations with data in that window, however. The stations that are included in this dataset are all the unique stations from the current cycle's conventionals dataset (conventionals + citizen monitoring data) and any stations from the previous assessment cycle that might need to be carried over. AU information will accompany a dataset of distinct stations and pertinent metadata that need assessment information for the forthcoming cycle. WQS information is only joined to the conventionals dataset at a later step and does not accompany the aforementioned station information. The format of this dataset mimics the CEDS bulk upload template. 

To help identify any stations that need to be carried over from one cycle to the next, we start with the previous cycle's final stations table and look for any stations that aren't Fully Supporting. These stations are added to the distinct stations from the conventionals dataset. 

The above data are manipulated to match the bulk data upload template. This is where AU information is joined in from the previous assessment cycle. If a station wasn't sampled last assessment (and had AU information from the metadata attribution process) it is then joined to the dataset.

This dataset is the start of the automated assessment process. It is saved as a .csv and used for upload to both the automated assessment scripts and the riverine and lacustrine applications. The reason the data are stored in .csv and not on the server is to enable individual assessors to adjust AU information manually whenever needed. If this data were stored on the server then the Assessment Data Analyst would need to be involved anytime an assessor needed to adjust which AU(s) a station is connected to. This data is available for download from each of the riverine and lacustrine assessment applications.


### Pin Official Clean IR data to R server

By cleaning up all provided data and then pinning to the R server, we can easily distribute and archive all data required and sourced by automated scripts in on secure location. The assessment applications source the pinned data to expedite application rendering time. 

The official assessment data pinned to the R server include:

* Conventionals dataset (including citizen monitoring data)
* Water Column Metals
* Sediment Metals

## Automated Assessment

Now that data are prepped for analysis, the last step is to bring all the data together, complete a few more manipulation steps, and feed the data through a process that assesses the data. To minimize the coding experience required to run the automated assessment process, the operation is written as a loop instead of as a function. Loops are inherently slow and sometimes confusing, but for this specific use case housing all operations inside a single loop increases the visibility of the order of operations and decreases the amount of coding experience users need to understand what is happening when. All individual parameter analyses are programmed as efficient functions and are detailed in the [Individual Parameter Analyses](#individual-parameter-analyses) section.



Bring in pinned data and local data.


```{r automated assessment pull pins}
# official March Data releases for IR 2022
conventionals <- pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")
conventionals_distinct <- pin_get("conventionals-distinct-final", board = "rsconnect")
stations2020IR <- pin_get("stations2020IR-sf-final", board = "rsconnect")
WQMstationFull <- pin_get("WQM-Station-Full", board = "rsconnect")
VSCIresults <- pin_get("VSCIresults", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) )
VCPMI63results <- pin_get("VCPMI63results", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) )
VCPMI65results <- pin_get("VCPMI65results", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) 
WCmetals <- pin_get("WCmetals-2022IRfinal",  board = "rsconnect")
Smetals <- pin_get("Smetals-2022IRfinal",  board = "rsconnect")

markPCB <- read_excel('data/2022 IR PCBDatapull_EVJ.xlsx', sheet = '2022IR Datapull EVJ')
fishPCB <- read_excel('data/FishTissuePCBsMetals_EVJ.xlsx', sheet= 'PCBs')
fishMetals <- read_excel('data/FishTissuePCBsMetals_EVJ.xlsx', sheet= 'Metals')

```


Bring in Station Table Bulk Upload Template

```{r automated assessment bulk upload template}
stationsTemplate <- read_excel('WQA_CEDS_templates/WQA_Bulk_Station_Upload_Final.xlsx',#'WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx', 
                               sheet = 'Stations', col_types = "text")[0,] %>% 
  mutate(LATITUDE = as.numeric(LATITUDE), LONGITUDE = as.numeric(LONGITUDE)) %>% 
  mutate_at(vars(contains('_EXC')), as.integer) %>% 
  mutate_at(vars(contains('_SAMP')), as.integer) %>% 
  # new addition that breaks station table upload template but very helpful for assessors
  mutate(BACTERIADECISION = as.character(NA),
         BACTERIASTATS = as.character(NA),
         `Date Last Sampled` = as.character(NA))
```



## Bring in Station Table from two previous assessments 

```{r stations table 2018}
historicalStationsTable2 <- read_excel('data/tbl_ir_mon_stations2018IRfinal.xlsx')
```


## Bring in lake nutrient standards

```{r lake nutrient standards}
lakeNutStandards <- read_csv('data/9VAC25-260-187lakeNutrientStandards.csv')
```



# Workflow

The following steps complete the automated assessment.

## Bring in user station table data

This information communicates to the scripts which stations should be assessed and where they should be organized (AUs). It also has data from the last cycle to populate the historical station information table in the application

```{r stationTable, echo = FALSE}
#stationTable <- read_csv('processedStationData/stationsTable2022begin_CitmonJames.csv')

stationTable <- read_csv('processedStationData/stationsTable2022begin.csv')# %>% 
  #filter(STATION_ID %in% kristie$StationID)
```

## Individual Parameter Analyses

### Temperature
### Dissolved Oxygen
### pH
### Bacteria
### Nutrients
### Ammonia
### PCB
### Metals
### Fish Tissue
### Benthics

## Automated Output
