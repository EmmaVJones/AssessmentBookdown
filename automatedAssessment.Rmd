# Automated Assessment Methods

```{r automated assessment setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
library(pins)
library(config)
library(lubridate)

# Connect to server
conn <- config::get("connectionSettings") # get configuration settings

board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))
```

This chapter details the specific steps required to transform the raw data outlined in the [Data Organization](#data-organization) chapter into preliminary assessment results. The general process the automated assessment specifies is to perform data manipulation and organization steps to prepare the data for analysis, apply specific assessment functions to the appropriate data that specify the assessment guidance and Water Quality Standards (WQS), and output a "stations table" that uses data from the assessment window to summarize each station for review and bulk upload into CEDS. See the [Automated Output](#automated-output) section to understand the information provided in the stations table output. 

The automated assessment scripts are trained to systematically apply assessment guidance and WQS to all data for which the appropriate metadata is provided. This supervised system merely applies the rules that developers have programmed to mimic assessment protocols. These scripts do not provide assessment decisions. It is up to the human reviewer (regional assessor) to either accept the automated result or use best professional judgment to e.g. redact questionable data or interpret complex natural systems. 


## Metadata Attribution

The automated assessment process hinges on each station having the appropriate metadata to analyze all raw data. The required metadata for each station include what Water Quality Standards apply to the station and which Assessment Unit(s) describe the station. One or more station can be included in an Assessment Units. It is ultimately the Assessment Units that are used to determine whether or not designated uses are met, which is where the automation stops and the human analysis component is required. 

In practice, metadata are spatially joined to stations by a rigorous data organization, spatial joining, and QA process that is detailed below. This automated process runs on the Assessment Data Analyst's computer and results are provided to regional assessment staff for individual review. This application is known as the [Regional Metadata Validation Tool](https://rconnect.deq.virginia.gov/RegionalAssessmentMetadataValidation/) and is hosted on the R server. It is up to each regional assessor to manually review each suggested metadata link prior to the assessment start date. After stations have completed the manual review process, they can be analyzed using the automated assessment scripts. Detailed intructions on how to use the [Regional Metadata Validation Tool](https://rconnect.deq.virginia.gov/RegionalAssessmentMetadataValidation/) is available in the [Regional Metadata Validation Tool How To](#regional-metadata-validation-tool-how-to) section.

### Distinct Sites

Before station metadata can be linked to stations, a list of unique stations that were sampled in a given assessment window is required. Because multiple data sources are combined for an assessment, each unique station from each data source with data in the given IR window are included in this list. 

For the purposes of the Automated Assessment User Guide, we will overview the process with a snippet of conventionals and citizen monitoring data types. Please see the [official script](https://github.com/EmmaVJones/IR2024/blob/main/1.preprocessData/HowToPreprocessData.Rmd) for more information.

```{r automatedAssessment libraries, eval=FALSE}
library(tidyverse)
library(sf)
library(pins)
library(config)
library(lubridate)

# Connect to server
conn <- config::get("connectionSettings") # get configuration settings

board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))
```

Bring conventionals data into your environment.

```{r automatedAssessment conventionals}
conventionals <- pin_get('ejones/conventionals2024draft', board = 'rsconnect') %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
  dplyr::select(FDT_STA_ID, Latitude, Longitude) %>% 
  mutate(Data_Source = 'DEQ')
conventionals[1:50,] %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```

Bring citizen data into your environment.

```{r automatedAssessment citmon}
citmon <-  readRDS('exampleData/citmon.RDS') %>% 
  distinct(FDT_STA_ID, .keep_all = T) %>% 
  dplyr::select(FDT_STA_ID, Latitude, Longitude, Data_Source)
citmon[1:50,] %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```

Create an object of unique sites from these two datasets.

```{r automatedAssessment distinctSites}
distinctSites <- bind_rows(conventionals, citmon)
distinctSites[1:50,] %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```

This process is repeated for all datasets that have data included in the assessment window. These multiple datasets are combined into a single object named distinctSites that we will then compare to existing WQS and AU information. Where stations in our distinctSites object lack either of these pieces of metadata, we must attribute them.

### Join Assessment Region and Subbasin Information

For rendering purposes in the metadata review application, it is important to have each station linked to the appropriate Assessment Region and Subbasin to limit the amount of data called into the application just to essential information. This information is also important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing. The next chunk reads in the necessary assessment region and subbasin spatial data and spatially joins all sites to these layers. If the sites are missing from the distinctSites_sf object, that means the point plots outside either the assessment region or subbasin polygon. These missingSites are dealt with individually and forced to join to the nearest assessment region and subbasin before rejoining the distinctSites_sf object.

```{r automatedAssessment spatial assessment region subbasin, eval = F}

assessmentLayer <- st_read('GIS/AssessmentRegions_VA84_basins.shp') %>%
  st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection

subbasinLayer <- st_read('GIS/DEQ_VAHUSB_subbasins_EVJ.shp')  %>%
  rename('SUBBASIN' = 'SUBBASIN_1')


distinctSites_sf <- st_as_sf(distinctSites,
                            coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
                            remove = F, # don't remove these lat/lon cols from df
                            crs = 4326) %>% # add coordinate reference system, needs to be geographic for now bc entering lat/lng, 
  st_intersection(assessmentLayer ) %>%
  st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) 

# if any joining issues occur, that means that there are stations that fall outside the joined polygon area
# we need to go back in and fix them manually
if(nrow(distinctSites_sf) < nrow(distinctSites)){
  missingSites <- filter(distinctSites, ! FDT_STA_ID %in% distinctSites_sf$FDT_STA_ID) %>%
    st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
             remove = F, # don't remove these lat/lon cols from df
             crs = 4326) 
  
  closest <- mutate(assessmentLayer[0,], FDT_STA_ID =NA) %>%
    dplyr::select(FDT_STA_ID, everything())
  for(i in seq_len(nrow(missingSites))){
    closest[i,] <- assessmentLayer[which.min(st_distance(assessmentLayer, missingSites[i,])),] %>%
      mutate(FDT_STA_ID = missingSites[i,]$FDT_STA_ID) %>%
      dplyr::select(FDT_STA_ID, everything())
  }
  
  missingSites <- left_join(missingSites, closest %>% st_drop_geometry(),
                            by = 'FDT_STA_ID') %>%
    st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) %>%
    dplyr::select(-c(geometry), geometry) %>%
    dplyr::select(names(distinctSites_sf))
  
  
  distinctSites_sf <- rbind(distinctSites_sf, missingSites)
}

```


### Join Stations to WQS

All stations are then spatially joined to the current WQS spatial layers to link each unique StationID to a unique WQS_ID. These unique WQS_ID's are a concatination of the waterbody type, basin code (e.g. 2A, 2B, 2C, etc.), and a number associated with each segment in the spatial layer.

Waterbody types include:

* RL = Riverine Line
* LP = Lacustrine Polygon
* EL = Estuarine Line
* EP = Estuarine Polygon

Since transitioning the WQS storage from local (individual assessor files) to a centralized system (on the R server for multiple program uses), the number of stations that require WQS attribution decreases significantly each IR cycle. To attribute each station to WQS information, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can send that information to an interactive application for humans to manually verify.

Where WQS_ID information is already available for stations, they are first removed from the WQS snapping process as to not repeat efforts unnecessarily. WQS_ID's are maintained across WQS layer updates, so any new WQS information will be updated when the stations are joined to the WQS metadata.

You can view the available WQSlookup table stored on the server by using the below script. Please see the [DEQ R Methods Encyclopedia](https://rconnect.deq.virginia.gov/MethodsEncyclopedia/connectToConnectPins.html) for information on how to retrieve pinned data from the server.

```{r automatedAssessment WQSlookup, eval=FALSE }
WQStableExisting <- pin_get('ejones/WQSlookup', board = 'rsconnect')

distinctSitesToDoWQS <- filter(distinctSites_sf, ! FDT_STA_ID  %in% WQStableExisting$StationID)
```


Here is the table used to store link information from stations to appropriate WQS.

```{r automatedAssessment WQStable, eval=F}
WQStable <- tibble(StationID = NA, WQS_ID = NA)
```


#### Spatially Join WQS Polygons

Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

###### Estuarine Polygons (WQS)

Here we find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas. This process also removes any estuarine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).

We will bring in (source) a custom function that runs the analysis for us, feed it our latest WQS information, and let the function run across all input stations. You can see the latest version of the sourced script in [this repository.](https://github.com/EmmaVJones/IR2024/tree/main/1.preprocessData)

```{r automatedAssessment estuary methods, eval = F}
source('preprocessingModules/WQS_estuaryPoly.R')

# Bring in estuary layer
estuarinePolys <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp', 
                          fid_column_name = "OBJECTID") %>%
  st_transform(4326)
  
WQStable <- estuaryPolygonJoin(estuarinePolys, distinctSitesToDoWQS, WQStable)

rm(estuarinePolys) # clean up workspace
```

Remove stations that fell inside estuarine polygons from the 'to do' list.

```{r automatedAssessment remove estuary poly sites WQS, eval = F}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


##### Lake Polygons (WQS)

Next find any sites that fall into a lake WQS polygon. This method is applied to all subbasins at once as it is a simple spatial join. You can see the latest version of the sourced script in [this repository.](https://github.com/EmmaVJones/IR2024/tree/main/1.preprocessData)


```{r automatedAssessment lake methods, eval = F}
source('preprocessingModules/WQS_lakePoly.R')

lakesPoly <- st_read('../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp',
                     fid_column_name = "OBJECTID") %>%
  st_transform(4326)

WQStable <- lakePolygonJoin(lakesPoly, distinctSitesToDoWQS, WQStable)

rm(lakesPoly) # clean up workspace
```


Remove stations that fell inside lake polygons from the 'to do' list.

```{r automatedAssessment remove lake poly sites, eval = F}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


#### Spatially Join WQS Lines

Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS and where stations remain we will try the estuarine WQS snap.

##### Riverine Lines (WQS)

To do this join, we will buffer all sites that don't fall into a polygon layer by a set sequence of distances. The output will add a field called `Buffer Distance` to the WQStable to indicate distance required for snapping. If more than one segment is found within a set buffer distance, then many rows will be attached to the WQStable with the single identifying station name. It is up to the QA tool to help the user determine which of these UID's are correct and drop the other records.

We then remove any riverine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS).

```{r automatedAssessment riverine methods, eval = F}
source('snappingFunctions/snapWQS.R')

riverine <- st_read('../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp',
                    fid_column_name = "OBJECTID") 

WQStable <- snapAndOrganizeWQS(distinctSitesToDoWQS, 'FDT_STA_ID', riverine, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)
rm(riverine) # clean up workspace
```


Remove stations that attached to riverine segments from the 'to do' list.

```{r automatedAssessment remove riverine sites, eval = F}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID)
```


We can use this one last opportunity to test stations that didn't connect to the riverine WQS against the estuarine lines WQS as a one last hope of attributing some WQS information. We will take all stations from the WQStable that didn't snap to any WQS segments (`Buffer Distance` =='No connections within 80 m') and add those back in to our distinctSitesToDoWQS list to try to snap them to the estuarine lines spatial data. 

```{r automatedAssessment no riverine snaps, eval = F}
distinctSitesToDoWQS <- filter(WQStable, `Buffer Distance` =='No connections within 80 m') %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %>%
              rename('StationID'= 'FDT_STA_ID'), by='StationID') %>%
  dplyr::select(-c(geometry)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = T, # don't remove these lat/lon cols from df
           crs = 4326)
                            
```



##### Estuarine Lines (WQS)

If a site doesn't attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time.
Removes any estuary lines sites from the data frame of unique sites that need WQS information.

```{r automatedAssessment estuarine lines methods, eval = F}
source('snappingFunctions/snapWQS.R')

estuarineLines <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp' , fid_column_name = "OBJECTID") #%>%
  #st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform(4326)

# Only send sites to function that could be in estuarine environment
WQStable <- snapAndOrganizeWQS(filter(distinctSitesToDoWQS, SUBBASIN %in% c("Potomac River",
                                                                            "Rappahannock River", 
                                                                            "Atlantic Ocean Coastal",
                                                                            "Chesapeake Bay Tributaries",
                                                                            "Chesapeake Bay - Mainstem",
                                                                            "James River - Lower",  
                                                                            "Appomattox River",
                                                                            "Chowan River", 
                                                                            "Atlantic Ocean - South" ,
                                                                            "Dismal Swamp/Albemarle Sound")),
                               'StationID', estuarineLines, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)

rm(estuarineLines)
```

Remove stations that attached to estuarine segments from the 'to do' list.

```{r automatedAssessment remove estuary line sites, eval = F}
distinctSitesToDoWQS <- filter(distinctSitesToDoWQS, ! StationID %in% WQStable$StationID)
```


Double check no stations were lost in these processes. 

```{r double check no one lost, eval = F}
#Make sure all stations from original distinct station list have some sort of record (blank or populated) int the WQStable.

distinctSitesToDoWQS <- filter(distinctSitesToDo, ! FDT_STA_ID  %in% WQStableExisting$StationID)
distinctSitesToDoWQS$FDT_STA_ID[!(distinctSitesToDoWQS$FDT_STA_ID %in% unique(WQStable$StationID))]
```

#### Assign something to WQS_ID so sites will not fall through the cracks when application filtering occurs

We don't want to give all the assessors statewide all the stations that didn't snap to something, so we need to at least partially assign a WQS_ID so the stations get into the correct subbasin on initial filter.

If a station snapped to nothing, we will assigning it a RL WQS_ID and subbasin it falls into by default.

```{r automatedAssessment blank WQS_ID partially filled in, eval = F}
WQStableMissing <- filter(WQStable, is.na(WQS_ID)) %>%
  # drop from list if actually fixed by snap to another segment
  filter(! StationID %in% filter(WQStable, str_extract(WQS_ID, "^.{2}") %in% c('EL','LP','EP'))$StationID) %>%
  left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, BASIN_CODE) %>%
              st_drop_geometry(), by = c('StationID' = 'FDT_STA_ID')) %>%
  # some fixes for missing basin codes so they will match proper naming conventions for filtering
  mutate(BASIN_CODE1 = case_when(is.na(BASIN_CODE) ~ str_pad(
    ifelse(grepl('-', str_extract(StationID, "^.{2}")), str_extract(StationID, "^.{1}"), str_extract(StationID, "^.{2}")), 
    width = 2, side = 'left', pad = '0'),
    TRUE ~ as.character(BASIN_CODE)),
    BASIN_CODE2 = str_pad(BASIN_CODE1, width = 2, side = 'left', pad = '0'),
    WQS_ID = paste0('RL_', BASIN_CODE2,'_NA')) %>%
  dplyr::select(-c(BASIN_CODE, BASIN_CODE1, BASIN_CODE2))

WQStable <- filter(WQStable, !is.na(WQS_ID)) %>% 
  filter(! StationID %in% WQStableMissing$StationID) %>%
  bind_rows(WQStableMissing)
```





### Join Stations to AUs

Assessment Units (AU) are different from WQS attribution steps because these links can change cycle to cycle as new AUs are broken off from existing AUs to more appropriately represent . Thus, a single record of all AUs linked to StationIDs like the WQSlookup table is not a good solution for this use case. Instead the AU to StationID link is stored in a more temporary format (Station Table Excel file) such that regional assessment staff can easily update the AU information on the fly as they assess stations. 

Generally, the logic holds that **the provided AUs are a starting point for an upcoming assessment cycle, not necessarily the final AU for said assessment cycle.** The process to link unique stations in an assessment window (`distinctSites_sf`) to AU information begins by joining all sites in an upcoming window to the Stations Table from the most recent assessment cycle. If a station does not join to a previous assessment cycle Stations Table that means that the station either has not been sample before (e.g. a new station for Probabilistic Monitoring or a special study) or has not been sampled in a long time such that is outside the last assessment window and has not been carried over from a previous assessment cycle for any reason. It is these sites that did not join to the last cycle's Stations Table where we will focus our efforts in the subsequent spatial joining steps. 

#### Identify which stations need AU information

First, bring in the final Stations Table from the most recently completed IR cycle. For this example, we are sourcing the IR2022 final Stations Table (presented as a spatial object in a file geodatabase but we will strip off the spatial data first thing since it is unnecessary for this purpose). We are also going to rename the friendly publication field names to a more standardized format that the automated assessment functions were built upon (read: we are changing field names to match previous versions of the Stations Table schema since the assessment functions were built on that data schema).

```{r automatedAssessment 2022 final stations, eval=FALSE}

final2022 <- st_read('C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_ir22_wqms.gdb',
                     layer = 'va_ir22_wqms') %>% 
  st_drop_geometry() %>% # only need tabular data from here out
  # change names of ID305B columns to format required by automated methods
  rename(ID305B_1 = Assessment_Unit_ID_1, TYPE_1 = Station_Type_1,
         ID305B_2 = Assessment_Unit_ID_2, TYPE_2 = Station_Type_2,
         ID305B_3 = Assessment_Unit_ID_3, TYPE_3 = Station_Type_3,
         ID305B_4 = Assessment_Unit_ID_4, TYPE_4 = Station_Type_4,
         ID305B_5 = Assessment_Unit_ID_5, TYPE_5 = Station_Type_5,
         ID305B_6 = Assessment_Unit_ID_6, TYPE_6 = Station_Type_6,
         ID305B_7 = Assessment_Unit_ID_7, TYPE_7 = Station_Type_7,
         ID305B_8 = Assessment_Unit_ID_8, TYPE_8 = Station_Type_8,
         ID305B_9 = Assessment_Unit_ID_9, TYPE_9 = Station_Type_9,
         ID305B_10 = Assessment_Unit_ID_10, TYPE_10 = Station_Type_10)
```

Now we will join distinct sites to AU information to get all available data to start the assessment process. Note: Assessors may attribute stations to different VAHU6's compared to strictly where the site is located spatially to communicate that said stations (usually that lie close to VAHU6 border) are used to make assessment decisions about the designated VAHU6. For this reason, we use the VAHU6 designation from the previous assessment cycle over the VAHU6 retrieved from CEDS. If the station does not have a record in the previous assessment cycle Stations Table, the VAHU6 designation stored in CEDS is used. 

The last rows of the below chunk ensure that each station is only listed once in the resultant table. In previous assessment cycles, stations could be assessed for multiple waterbody types (e.g. riverine and lacustrine assessment uses). Since the assessment database was moved from MS Access to CEDS WQA, this duplication is no longer allowed and thus each station should only have one record.

```{r automatedAssessment AU join, eval=FALSE}
distinctSites_AUall <- distinctSites_sf %>% 
  st_drop_geometry() %>%
  left_join(final2022 %>% 
              dplyr::select(-c(Latitude, Longitude)), # drop duplicate lat/lng fields to avoid join issues
            by = c('FDT_STA_ID' = 'Station_ID')) %>%
  dplyr::select(FDT_STA_ID : VAHU6.y) %>% # drop the last cycle's results, not important now
  mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.y))) %>% # use last cycle's VAHU6 designation over CEDS designation by default if available
  dplyr::select(-c(VAHU6.x, VAHU6.y)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>% ungroup()

# Find any duplicates
View(filter(distinctSites_AUall, n >1)) # 0 rows, cool

# above n> 1 used to be stations that were riverine and lacustrine makes sense, these sites are being used for riverine and lacustrine assessment
# for IR2024 this should all be cleaned up bc new WQA CEDS rules, but always good to double check
```

Next we will organize stations by whether or not they have AU data based on the previous join. We will call the stations that need AU information `distinctSites_AUtoDo`. We only test this using the ID305B_1 column because we only need each station to be attributed to at least one AU.  

```{r automatedAssessment AU haves and have nots, eval = F}
distinctSites_AUtoDo <- filter(distinctSites_AUall, is.na(ID305B_1)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
           remove = F, # don't remove these lat/lon cols from df
           crs = 4326)

# These sites are all good for automated assessment  (once we join WQS info from WQSlookup table)
distinctSites_AU <- filter(distinctSites_AUall, !is.na(ID305B_1)) 

# Quick QA: double check the math works out
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```

As with the WQS spatial attribution process, we will first join these sites to polygon layers and then line features to minimize any unnecessary computational load. 

#### Spatially Join AU Polygons

Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first.

##### Estuarine Polygons (AU)

We will now find any sites that fall into an estuary AU polygon. This method is only applied to subbasins that intersect estuarine areas and then removes any estuarine sites from the data frame of unique sites that need AU information (`distinctSites_AUtoDo`). The chunk below sources a custom function for handling AU polygon information [available for download here](https://github.com/EmmaVJones/IR2024/blob/main/1.preprocessData/preprocessingModules/AU_Poly.R). 

```{r automatedAssessment estuary methods AU, eval=FALSE}
source('preprocessingModules/AU_Poly.R')

# Bring in estuary layer
estuaryPolysAU <- st_read('../va_aus_estuarine.shp') %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427
  
estuaryPolysAUjoin <- polygonJoinAU(estuaryPolysAU, distinctSites_AUtoDo, estuaryTorF = T) %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n(),
         `Buffer Distance` = 'In polygon') %>%
  ungroup() 

rm(estuaryPolysAU) # clean up workspace
```

Now we add the newly identified estuary stations to `distinctSites_AU`.

```{r automatedAssessment add estuary AU sites, eval=FALSE}
distinctSites_AU <- bind_rows(distinctSites_AU, estuaryPolysAUjoin %>% st_drop_geometry())
```


And then remove stations that fell inside estuarine polygons from our 'to do' list.

```{r automatedAssessment remove estuary poly sites AU, eval=FALSE}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% estuaryPolysAUjoin$FDT_STA_ID)

nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)
```


##### Lake Polygons (AU)

Next we identify any sites that fall into a lake AU polygon. This method is applied to all subbasins, unlike the previous estuary step. We then remove any lake sites from the data frame of unique sites that need AU information. The chunk below sources the same custom function for handling AU polygon information as above [available for download here](https://github.com/EmmaVJones/IR2024/blob/main/1.preprocessData/preprocessingModules/AU_Poly.R). 


```{r automatedAssessment lake methods AU, eval=FALSE}
source('preprocessingModules/AU_Poly.R')

# Bring in Lakes layer
lakesPolyAU <-  st_read('../va_aus_reservoir.shp') %>%
   st_transform( 4326 ) %>%
   st_cast("MULTIPOLYGON") # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427

lakesPolysAUjoin <- polygonJoinAU(lakesPolyAU, distinctSites_AUtoDo, estuaryTorF = T)%>%
  mutate(ID305B_1 = ID305B,
         `Buffer Distance` = 'In polygon') %>%
  dplyr::select(names(distinctSites_AU)) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup() 

rm(lakesPolyAU) # clean up workspace
```

Now add the lake stations to `distinctSites_AU`.

```{r automatedAssessment add lake AU sites, eval=FALSE}
distinctSites_AU <- bind_rows(distinctSites_AU, lakesPolysAUjoin %>% st_drop_geometry())
```

And we remove stations that fell inside lake polygons from the 'to do' list. This is also a good time to double check that we haven't lost any sites in the above process.

```{r automatedAssessment remove lake poly sites AU, eval=FALSE}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% lakesPolysAUjoin$FDT_STA_ID)

# Double check all sites are still there
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)


rm(lakesPolysAUjoin);rm(estuaryPolysAUjoin) # clean up workspace
```



#### Spatially Join AU Lines

Now on to the more computationally heavy AU line snapping methods. We can only try to attach riverine AUs since there is not a published estuarine lines AU spatial layer (like there is for WQS information). All sites that do not snap to a riverine line within a set buffer distance will be flagged with comment saying so and the regional assessors will sort out the most appropriate AU manually in the [Regional Metadata Validation Tool](https://rconnect.deq.virginia.gov/RegionalAssessmentMetadataValidation/). 

##### Riverine Lines (AU)

Time to run all the sites that didn't fall into an AU polygon through our [polyline snapping buffering script](https://github.com/EmmaVJones/IR2024/blob/main/1.preprocessData/snappingFunctions/snapPointToStreamNetwork.R). This function is slightly different from the WQS buffering function in that it is flexible enough link any chosen fields from the two input arguments. The output of the function will add a field called `Buffer Distance` to `distinctSites_AU` to indicate the  distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, that many rows will be attached to the `snapTable` with the identifying station name. It is up to the [Regional Metadata Validation Tool](https://rconnect.deq.virginia.gov/RegionalAssessmentMetadataValidation/) to help the user determine which of these AU's are correct and subsequently drop all other records.


```{r automatedAssessment riverine methods AU, eval=FALSE}
source('snappingFunctions/snapPointToStreamNetwork.R')

riverineAU <- st_read('../va_aus_riverine.shp') %>%
     st_transform(102003)
  
snapTable <- snapAndOrganize(distinctSites_AUtoDo, 'FDT_STA_ID', riverineAU, 
                             bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                             tibble(StationID = character(), ID305B = character(), `Buffer Distance` = character()),
                             "ID305B")


snapTable <- snapTable %>%
  left_join(distinctSites_AUtoDo, by = c('StationID' = 'FDT_STA_ID')) %>% # get station information
  rename('FDT_STA_ID' = 'StationID') %>%
  mutate(ID305B_1 = ID305B) %>%
  dplyr::select(names(distinctSites_AU), `Buffer Distance`) %>%
  group_by(FDT_STA_ID) %>%
  mutate(n = n()) %>%
  ungroup()
  
  
rm(riverineAU) # clean up workspace
```

Now we can add these sites to the sites with AU information.

```{r automatedAssessment add to AU table, eval=FALSE}
distinctSites_AU <- bind_rows(distinctSites_AU , snapTable )
```

And then remove stations that attached to riverine segments from the 'to do' list.

```{r automatedAssessment remove riverine snapped AU sites, eval=FALSE}
distinctSites_AUtoDo <- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% snapTable$FDT_STA_ID)
```

We don't have estuarine lines AU information, so the sites that don't connect to any AU's at the max buffer distance will have to be sorted out by the assessors.

```{r automatedAssessment final distinctSites_AU, eval=FALSE}
distinctSites_AU <- distinctSites_AU %>%
  group_by(FDT_STA_ID) %>%
  mutate(n=n())
```

We then make sure all stations from original distinct station list (`distinctSites_AUall`) have some sort of record (blank or populated) in the `distinctSites_AU` dataset.

```{r automatedAssessment double check no one lost AU, eval=FALSE}

# check everyone dealt with
nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall)

distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))]

if(nrow(distinctSites_AUtoDo) == 0){rm(distinctSites_AUtoDo)} # clean up workspace if QA check good
```


One last step to make sure buffer distances save correctly for future mapping needs.

```{r automatedAssessment buffer distance to factor, eval=FALSE}
unique(distinctSites_AU$`Buffer Distance`)

distinctSites_AU$`Buffer Distance` <- as.character(distinctSites_AU$`Buffer Distance`)
```

So, what does the end result look like? 

```{r automatedAssessment sites ready for app review, eval=FALSE}
distinctSites_AU[1:50,] %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```

```{r automatedAssessment sites ready for app review real, echo=FALSE}
distinctSites_AU <- readRDS('exampleData/distinctSites_AU.RDS') 

distinctSites_AU[1:50,] %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```

### Where does this information go?

The above analyses help populate the [Regional Metadata Validation Tool](https://rconnect.deq.virginia.gov/RegionalAssessmentMetadataValidation/). After assessors manually review each station that requires WQS/AU information, this data is consolidated and stored on the R server as a pinned dataset (see []() for details on that process). That dataset feeds subsequent tools including the [Riverine Assessment App](https://rconnect.deq.virginia.gov/IR2022riverineAssessmentApplication/), [Lakes Assessment App](https://rconnect.deq.virginia.gov/IR2022LakesAssessmentApp/), and [Bioassessment Dashboard](https://rconnect.deq.virginia.gov/IR2022BioassessmentDashboard/) in addition to querying tools including the [CEDS WQM Data Query Tool](https://rconnect.deq.virginia.gov/CEDSWQMDataQueryTool/) and [CEDS Benthic Data Query Tool](https://rconnect.deq.virginia.gov/CEDSBenthicDataQueryTool/). The data is provided via the internal GIS services in the WQM Stations (All stations with full attributes) layer hosted on the [GIS REST service](https://gis.deq.virginia.gov/arcgis/rest/services/staff/DEQInternalDataViewer/MapServer/104) and [GIS Staff Application](https://gis.deq.virginia.gov/GISStaffApplication/). In addition to the published tools that source this data, the dataset is included in countless data analysis projects and products. The entire dataset may be pulled from the R Connect service using the following script. Please see the [DEQ R Methods Encyclopedia](https://rconnect.deq.virginia.gov/MethodsEncyclopedia/connectToConnectPins.html) for information on how to retrieve pinned data from the server.

```{r automatedAssessment pull pinned WQS data, eval=F}
pin_get('ejones/WQSlookup', board = 'rsconnect') # raw lookup table
pin_get("ejones/WQSlookup-withStandards", board = "rsconnect") # lookup table with WQS information joined 
```


## Organize Metadata

After all stations for a given assessment window have the appropriate WQS and AU information attributed, there are a number of data organization steps that still need to happen before the data are ready for the automated assessment scripts. These steps include:

* Reorganize the conventionals dataset to match schema for automated assessment scripts
* Add Secchi Depth to conventionals dataset
* Add Citizen Monitoring Data to conventionals
* Ensure all stations have necessary WQS and AU information
    + Organize all new WQS/AU metadata from the R server and adding new data to pinned datasets
    + Create a stationsTablebegin dataset
        + Carry forward any stations required from last cycle
* Clean up PCB dataset 
* Clean up Fish Tissue dataset

**The sections below are purposefully sparse as the methods are in flux given that new data are available in ODS. Stay tuned to see how these methods flesh out for more streamlined data organization in future cycles.**



### Reorganize Conventionals Dataset

The official (SAS) conventionals data schema tends to vary from IR to IR. It is essential that this data are provided to the R scripts exactly how the scripts expect data, so each cycle the provided conventionals dataset must be meticulously QAed to ensure all data are of expected name/type. Additionally, in order to automate the assessment of citizen monitoring data, we must augment the provided conventionals dataset to accommodate the level schema for each citizen monitoring parameter. 

The nuanced data manipulation steps required to convert the conventionals data and citizen monitoring data from the provided format to the required format for automated analyses are beyond the scope of this book. Instead, the required data schema for automated assessment is provided below.

```{r automatedAssessment conventional schema}
conventionalsSchema <- readRDS('exampleData/schemaFin.RDS')

conventionalsSchema %>% # preview data
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```


### Adding Secchi Depth to conventionals dataset

This data are not included in the official (SAS) conventionals data pull but are required for Trophic State Index (TSI) analyses for some lacustrine stations. The below script will only work if a user's credentials have access to production ODS wqm data. The example script below first finds all unique stations in the conventionals dataset before querying secchi depth information for those stations for the given assessment window. The date range requires updating each assessment cycle.

```{r automated assesssment ODS secchi query, eval=FALSE}
# dataset of all the unique stations that we need to organize
conventionals_distinct <- conventionals %>%
  distinct(FDT_STA_ID, .keep_all = T) %>%
  # remove any data to avoid confusion
  dplyr::select(FDT_STA_ID:FDT_COMMENT, Latitude:Data_Source) %>%
  filter(!is.na(FDT_STA_ID))

# Query secchi data for unique conventionals stations in the appropriate data window
library(pool)
library(dbplyr)
### connect to Production Environment
pool <- dbPool(
  drv = odbc::odbc(),
  Driver = "ODBC Driver 11 for SQL Server",
  Server= "DEQ-SQLODS-PROD,50000",
  dbname = "ODS",
  trusted_connection = "yes"
)

stationSecchiDepth <- pool %>% tbl(in_schema('wqm', "Wqm_Field_Data_View")) %>%
  filter(Fdt_Sta_Id %in% !! conventionals_distinct$FDT_STA_ID & 
           between(as.Date(Fdt_Date_Time), "2014-12-31", "2020-12-31") &
           !is.na(Fdt_Secchi_Depth)) %>% # x >= left & x <= right
  dplyr::select(Fdt_Sta_Id, Fdt_Date_Time, Fdt_Depth, Fdt_Secchi_Depth) %>%
  as_tibble() %>%
  mutate(Date = as.Date(Fdt_Date_Time)) %>%
    dplyr::select(FDT_STA_ID = Fdt_Sta_Id, Date, FDT_DEPTH = Fdt_Depth, Fdt_Secchi_Depth) 
```


Once the secchi data is acquired, it can be attached to the original conventionals dataset carefully as detailed below.

```{r  automated assesssment ODS secchi data smash, eval=FALSE}
conventionalsArchive <- conventionals %>%
  mutate(Date = as.Date(FDT_DATE_TIME))

conventionals <- left_join(conventionalsArchive, stationSecchiDepth, by = c('FDT_STA_ID', 'Date', 'FDT_DEPTH')) %>%
  mutate(SECCHI_DEPTH_M = Fdt_Secchi_Depth) %>%
  dplyr::select(-c(Fdt_Secchi_Depth, Date))
```

###  Add Citizen Monitoring Data to conventionals

Once the citizen monitoring data is queried, QAed, and matches the required conventionals schema (above), the data can be appended to the conventionals dataset for automated analysis.

### Ensure all stations have necessary WQS and AU information

#### Organizing all new WQS/AU metadata from the R server

A server administrator must retrieve all the WQS/AU information from the attribution application. This directory is consolidated into a single dataset of all new StationID-WQS/AU records. It is important to verify that all expected StationIDs (all distinct stations from the conventionals and citizen monitoring datasets) have WQS and AU information before proceeding. If stations lack this information they will not be assessed accurately.

##### Adding new WQS data to pinned datasets

Once WQS_ID/AU information is available for all new stations in an assessment cycle, the data are appended to the master WQS information pin on the R server in the case of WQS information. AU information can change cycle to cycle, so the IR year for the StationID-AU information is added to the dataset before pinning back to the master AU information pin. 

To expedite the transfer of actual WQS information (not just WQS_IDs), another pin contains the actual WQS metadata as well as relevant StationID. This pin is called WQSlookup-withStandards. All new stations attributed need to be joined to the relevant WQS layer to extract this metadata before it can be appended to the master WQS information pin  with standards (WQSlookup-withStandards). The script below details this process by bringing in each necessary WQS layer, joining it to the WQSlookup information, and then pinning it to the R server. 

```{r automated assessment pin WQS, eval = F}
# table with StationID and WQS_ID information that needs actual WQS metadata
WQSlookupToDo <- tibble(StationID = NA, WQS_ID = NA) # blank for example purposes

#bring in Riverine layers, valid for the assessment window
riverine <- st_read('../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp',
                    fid_column_name = "OBJECTID")  %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, riverine, by = 'WQS_ID') %>%
  filter(!is.na(CLASS)) # drop sites that didn't actually join 
rm(riverine) # clean up workspace

lacustrine <- st_read('../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp',
                     fid_column_name = "OBJECTID") %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, lacustrine, by = 'WQS_ID') %>%
  filter(!is.na(CLASS)) %>% # drop sites that didn't actually join 
  bind_rows(WQSlookupFull) # add these to the larger dataset
rm(lacustrine) # clean up workspace

estuarineLines <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp' , fid_column_name = "OBJECTID") %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, estuarineLines, by = 'WQS_ID') %>%
  filter(!is.na(CLASS)) %>% # drop sites that didn't actually join 
  bind_rows(WQSlookupFull) # add these to the larger dataset
rm(estuarineLines) # clean up workspace

estuarinePolys <- st_read('../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp', 
                          fid_column_name = "OBJECTID") %>%
  st_drop_geometry() # only care about data not geometry

WQSlookupFull <- left_join(WQSlookupToDo, estuarinePolys, by = 'WQS_ID') %>%
  filter(!is.na(CLASS)) %>% # drop sites that didn't actually join 
  bind_rows(WQSlookupFull) # add these to the larger dataset
rm(estuarinePolys) # clean up workspace

WQSlookup_withStandards_pin <- bind_rows(WQSlookup_withStandards, WQSlookupFull) # for pin

```

The WQSlookup_withStandards_pin object is then pinned to the R server to be sourced by numerous other data users/products. This pin is available by using the following script.

```{r automated assessment pin get WQS, eval = F}
WQSlookup_withStandards <- pin_get('ejones/WQSlookup-withStandards', board = 'rsconnect')
```

##### Adding new AU data to pinned datasets

The new stations in an assessment cycle that were manually attributed to the AU layer from the previous cycle need this information added to the AUlookup table pin stored on the R server. After an assessor adds these stations into WQA CEDS with AU information, this table is no longer used to source AU information for a station, but it is required for the start of an assessment cycle for all stations that were not included in any previous assessments. 

The server administrator must pull the AU attribution information contained in the Metadata App from the R server and append this information to the existing AUlookup table pin. The data may be retrieved by anyone using the following script. 

```{r automated assessment pin AU, eval = F}
AUlookupArchive <- pin_get('ejones/AUlookup', board = 'rsconnect')
```

#### Create a stationsTablebegin dataset

The so called 'stationsTablebegin' dataset is the key input to the automated assessment scripts. This dataset tells the scripts all the stations it should look to assess. It is necessary to use this dataset as the list of stations to assess rather than any other individual dataset (e.g. conventionals, citmon/nonagency, PCB, etc.) because stations that need to be touched during a given assessment period might not have information in any of the individual datasets. This dataset also contains any stations from a previous cycle that must be carried over to the new cycle, carryover stations. These carryover stations may not have any data in the current window and thus would never appear in any individual dataset organized so far. The stationsTablebegin dataset contains not only all the stations that need to be addressed in an assessment cycle, but also at least one Assessment Unit per station for organizational purposes. 

Because station WQS information are only used for comparing individual parameters to criteria, we do not actually store WQS metadata in the stationsTablebegin dataset. This information is only joined to the conventionals dataset at a later step in the automated assessment process to maintain a dataset most similar to the CEDS bulk upload template.

Starting with the conventionals_distinct dataset (which includes citmon/non agency data at this point), we can begin to create our dataset of all the unique stations that we need to organize for the current cycle (stationsTablebegin).

```{r automated assessment conventionals_distinct, eval = F}
# conventionals_distinct <- conventionals %>%
#   distinct(FDT_STA_ID, .keep_all = T) %>%
#   # remove any data to avoid confusion
#   dplyr::select(FDT_STA_ID:FDT_COMMENT, Latitude:Data_Source) %>%
#   filter(!is.na(FDT_STA_ID))
conventionals_distinct <- pin_get('ejones/conventionals2024_distinctdraft', board = 'rsconnect') %>% 
  filter(!is.na(Latitude) | !is.na(Longitude))


# and make a spatial version
conventionals_sf <- conventionals_distinct %>%
  st_as_sf(coords = c("Longitude", "Latitude"), 
               remove = F, # don't remove these lat/lon cols from df
               crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng

```

We want the stationsTablebegin dataset to look like our CEDS Bulk Upload template to make future steps easier.

The biggest changes from the older stations table format to this new bulk upload template is the addition of the 10 ID305B and station type columns as well as the lacustrine designation. 

```{r automated assessment bulk upload template, eval = F}
stationsTemplate <- read_excel('data/WQA_CEDS_templates/WQA_Bulk_Station_Upload_Final.xlsx',#'WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx', 
                               sheet = 'Stations', col_types = "text")[0,] # just want structure and not the draft data, force everything to character for now
```

We want to populate as much of the information from the bulk upload template for each station to make the regional assessors' lives easier. We will start by filling in as much as we can about each station that had an entry in last cycle. We will pull this information from the WQA area of ODS. You need ODS access to retrieve this information for yourself. Later steps will allow you to source the output of these query/manipulation steps if you do not have ODS access.

##### Last cycle AU information

Let's start by populating this template with information we can grab from the previous assessment cycle stations table information.

Connect to the ODS production environment.

```{r automated assessment ODS connection, eval = F}
library(pool)
library(dbplyr)
### Production Environment
pool <- dbPool(
  drv = odbc::odbc(),
  Driver = "ODBC Driver 11 for SQL Server", #"SQL Server Native Client 11.0", 
  Server= "DEQ-SQLODS-PROD,50000",
  dbname = "ODS",
  trusted_connection = "yes"
)
```


##### Carry forward any stations required from last cycle

Find all stations from the last IR cycle that should be carried forward for review this cycle, either impaired last time or with the comment field containing "carr%" string (e.g. carry over, carried over, etc.). Start by querying stations in IR2022 and join in their AU information and station parameters. The key to these joins is the 'WXA_STATION_DETAIL_ID'/'Station Detail Id' field.

```{r automated assessment carryover stations, eval = F}
# data prep
stations <- pool %>% tbl(in_schema('wqa', "Wqa_Station_Details_View")) %>%
  filter(WSD_CYCLE == 2022) %>% 
  as_tibble() %>% 
  distinct(WXA_STATION_DETAIL_ID, .keep_all = T) %>%  # distinct on this variable for AU join or duplicated rows for stations
  filter(WSD_STATION_ID != '4AGSE013.78') # problem site OIS needs to deal with

# WQA geospatial data only seems to have citmon/non agency station locations. All other stations (DEQ) need to be queried from the WQM side of ODS
stationsGeospatial_wqa <- pool %>% tbl(in_schema('wqa', 'Wqa_Stations_Geospatial_Data_View')) %>%
  filter(Station_Id %in% !! stations$STA_NAME) %>% #stations$WSD_STATION_ID) %>%
  as_tibble() %>% 
  dplyr::select(Station_Id, Latitude, Longitude) 

# WQM geospatial data for DEQ stations
stationsGeospatial_wqm <- pool %>% tbl(in_schema('wqm', 'WQM_Sta_GIS_View')) %>% 
  filter(Station_Id %in% !! stations$WSD_STATION_ID) %>% 
  as_tibble() %>% 
  dplyr::select(Station_Id, Latitude, Longitude) 

# combine geospatial data into one object for easier joining
stationsGeospatial <- bind_rows(stationsGeospatial_wqa, stationsGeospatial_wqm) %>% 
  distinct(Station_Id, .keep_all = T) %>% 
  mutate(Station_Id = case_when(Station_Id == 'Griggs Pond' ~ toupper(Station_Id),
                                Station_Id == 'Sims Metal 003' ~ 'SIMS METAL 003',
                                TRUE ~ as.character(Station_Id))) # Joining problems later if we don't capitalize the names Griggs Pond and Simms Metal 003 as they are elsewhere in ODS

# QUery AU information
AU <- pool %>% tbl(in_schema('wqa', '[WQA 305b]')) %>% # **note** need to put views with spaces in name in brackets for SQL to work
  filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %>% 
  as_tibble()

stationDetails <- pool %>% tbl(in_schema('wqa', '[WQA Station Parameters Pivot]')) %>% # **note** need to put views with spaces in name in brackets for SQL to work
  filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %>% 
  as_tibble()

stationType <- pool %>% tbl(in_schema('wqa', '[WQA Station Detail Types]')) %>% # **note** need to put views with spaces in name in brackets for SQL to work
  filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %>% 
  as_tibble()

stationsAU <- left_join(stations, AU, by = c('WXA_STATION_DETAIL_ID' = 'Station Detail Id')) %>% 
  left_join(stationType, by =  c('WXA_STATION_DETAIL_ID' = 'Station Detail Id')) %>% 
  left_join(stationsGeospatial, by = c('STA_NAME' = 'Station_Id')) %>% # Make sure you join on STA_NAME and not WSD_STATION_ID here!!!
  left_join(stationDetails,  by = c('WXA_STATION_DETAIL_ID' = 'Station Detail Id')) 

# actual analysis, find all stations with impaired parameters
impairedStations <- stationsAU %>% 
  filter_at(.vars = vars(contains("Status Code")),
            .vars_predicate = any_vars(str_detect(., 'IM')))

# Cast a wide net: string search for any stations that were carried over from more previous cycles by looking for 
#  variations of the phrase "Carry" in the station comment field
carryoverStations <- stationsAU %>% 
  filter(str_detect(WSD_COMMENTS, 'carr'))

# combine lists and remove duplicates
stationsFromLastCycle <- bind_rows(impairedStations, carryoverStations) %>% 
  distinct(WXA_STATION_DETAIL_ID, .keep_all = T)

# clean up workspace
rm(list = c('stations','stationsAU', 'stationDetails', 'impairedStations', 'carryoverStations', 
            'AU', 'stationType', 'stationsGeospatial_wqa', 'stationsGeospatial_wqm'))
```

Now we need to clean up this data to match the bulk upload data template. We will also strip out the data from previous cycles to not confuse anyone from cycle to cycle.

```{r automated assessment match bulk upload template, eval = F}
stationsTable2024begin <- stationsFromLastCycle %>% 
  dplyr::select(STATION_ID = STA_NAME,  #### OR COULD USE WSD_STATION_ID
                ID305B_1 = `ID305B 1`,
                ID305B_2 = `ID305B 2`,
                ID305B_3 = `ID305B 3`,
                ID305B_4 = `ID305B 4`,
                ID305B_5 = `ID305B 5`,
                ID305B_6 = `ID305B 6`,
                ID305B_7 = `ID305B 7`,
                ID305B_8 = `ID305B 8`,
                ID305B_9 = `ID305B 9`,
                ID305B_10 = `ID305B 10`,
                WATER_TYPE = WWT_WATER_TYPE_DESC,
                SALINITY = WSC_DESCRIPTION,
                LACUSTRINE = WSD_LAC_ZONE_YN,
                REGION = STA_REGION,
                TYPE_1 = `Station Type 1`,
                TYPE_2 = `Station Type 2`,
                TYPE_3 = `Station Type 3`,
                TYPE_4 = `Station Type 4`,
                TYPE_5 = `Station Type 5`,
                TYPE_6 = `Station Type 6`,
                TYPE_7 = `Station Type 7`,
                TYPE_8 = `Station Type 8`,
                TYPE_9 = `Station Type 9`,
                TYPE_10 = `Station Type 10`,
                LATITUDE = Latitude,
                LONGITUDE = Longitude,
                WATERSHED = STA_WATERSHED,
                VAHU6 = STA_VA_HU6) %>% 
  mutate(REGION = case_when(REGION == 'NVRO' ~ 'NRO',
                            REGION == 'WCRO' ~ 'BRRO',
                            TRUE~ as.character(REGION))) %>% 
  dplyr::select(any_of(names(stationsTemplate)))

stationsTable2024begin <- bind_rows(stationsTemplate %>% 
                                      mutate(LATITUDE = as.numeric(LATITUDE),
                                             LONGITUDE = as.numeric(LONGITUDE)), 
                                    stationsTable2024begin) # add back in missing columns

```

Quick QA check for any missing geospatial data. 

```{r automated assessment missing geospatial, eval = F}
missingGeospatial <- filter(stationsTable2024begin, is.na(LATITUDE) | is.na(LONGITUDE))

# clean up workspace
rm(list = c('stationsFromLastCycle','missingGeospatial'))
```

##### AU information from Last Cylce

Now we need to get the same metadata for all the stations from the conventionals (and citmon/non agency) dataset from the current cycle. We can make our lives easier by only doing this work for new stations from the conventionals dataset (i.e. dropping all stations from our "to do list" that already have this information from our last step). This chunk will also reformat the queried data into the bulk upload template format so it can be combined with the stationsTable2024begin dataset created above.

```{r automated assessment conventionals to do, eval =  F}
stationsToDo <- filter(conventionals_distinct, ! FDT_STA_ID %in% stationsTable2024begin$STATION_ID)

# use the same method from above with a new station list
stations <- pool %>% tbl(in_schema('wqa', "Wqa_Station_Details_View")) %>%
  filter(WSD_STATION_ID %in% !! stationsToDo$FDT_STA_ID) %>% # pull all data from stations identified above
  as_tibble() %>% # get that data local before doing more complicated things than SQL wants to handle
  # keep only the most recent record for each station by grouping and then filtering
  group_by(WSD_STATION_ID) %>% 
  filter(WSD_CYCLE == max(WSD_CYCLE )) %>% 
  distinct(WXA_STATION_DETAIL_ID, .keep_all = T) %>%  # still need only 1 record per site
  ungroup() # ungroup so the WSD_STATION_ID column doesn't come along for the ride to future steps where not necessary
  
# WQA geospatial data only seems to have citmon/non agency station locations. All other stations (DEQ) need to be queried from the WQM side of ODS
stationsGeospatial_wqa <- pool %>% tbl(in_schema('wqa', 'Wqa_Stations_Geospatial_Data_View')) %>%
  filter(Station_Id %in% !! stations$STA_NAME) %>% #stations$WSD_STATION_ID) %>%
  as_tibble() %>% 
  dplyr::select(Station_Id, Latitude, Longitude) 

# WQM geospatial data for DEQ stations
stationsGeospatial_wqm <- pool %>% tbl(in_schema('wqm', 'WQM_Sta_GIS_View')) %>% 
  filter(Station_Id %in% !! stations$WSD_STATION_ID) %>% 
  as_tibble() %>% 
  dplyr::select(Station_Id, Latitude, Longitude) 

# combine geospatial data into one object for easier joining
stationsGeospatial <- bind_rows(stationsGeospatial_wqa, stationsGeospatial_wqm) %>% 
  distinct(Station_Id, .keep_all = T)

AU <- pool %>% tbl(in_schema('wqa', '[WQA 305b]')) %>% # **note** need to put views with spaces in name in brackets for SQL to work
  filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %>% 
  as_tibble()

stationDetails <- pool %>% tbl(in_schema('wqa', '[WQA Station Parameters Pivot]')) %>% # **note** need to put views with spaces in name in brackets for SQL to work
  filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %>% 
  as_tibble()

stationType <- pool %>% tbl(in_schema('wqa', '[WQA Station Detail Types]')) %>% # **note** need to put views with spaces in name in brackets for SQL to work
  filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %>% 
  as_tibble()

stationsAU <- left_join(stations, AU, by = c('WXA_STATION_DETAIL_ID' = 'Station Detail Id')) %>% 
  left_join(stationType, by =  c('WXA_STATION_DETAIL_ID' = 'Station Detail Id')) %>% 
  left_join(stationsGeospatial, by = c('STA_NAME' = 'Station_Id')) %>% # Make sure you join on STA_NAME and not WSD_STATION_ID here!!!
  left_join(stationDetails,  by = c('WXA_STATION_DETAIL_ID' = 'Station Detail Id')) %>% 
  # reorganize to fit the data template
  dplyr::select(STATION_ID = STA_NAME,  #### OR COULD USE WSD_STATION_ID
                ID305B_1 = `ID305B 1`,
                ID305B_2 = `ID305B 2`,
                ID305B_3 = `ID305B 3`,
                ID305B_4 = `ID305B 4`,
                ID305B_5 = `ID305B 5`,
                ID305B_6 = `ID305B 6`,
                ID305B_7 = `ID305B 7`,
                ID305B_8 = `ID305B 8`,
                ID305B_9 = `ID305B 9`,
                ID305B_10 = `ID305B 10`,
                WATER_TYPE = WWT_WATER_TYPE_DESC,
                SALINITY = WSC_DESCRIPTION,
                LACUSTRINE = WSD_LAC_ZONE_YN,
                REGION = STA_REGION,
                TYPE_1 = `Station Type 1`,
                TYPE_2 = `Station Type 2`,
                TYPE_3 = `Station Type 3`,
                TYPE_4 = `Station Type 4`,
                TYPE_5 = `Station Type 5`,
                TYPE_6 = `Station Type 6`,
                TYPE_7 = `Station Type 7`,
                TYPE_8 = `Station Type 8`,
                TYPE_9 = `Station Type 9`,
                TYPE_10 = `Station Type 10`,
                LATITUDE = Latitude,
                LONGITUDE = Longitude,
                WATERSHED = STA_WATERSHED,
                VAHU6 = STA_VA_HU6) %>% 
  mutate(REGION = case_when(REGION == 'NVRO' ~ 'NRO',
                            REGION == 'WCRO' ~ 'BRRO',
                            TRUE~ as.character(REGION))) %>% 
  dplyr::select(any_of(names(stationsTemplate)))

# Smash in with the rest of the sites already organized
stationsTable2024begin <- bind_rows(stationsTable2024begin,
                                    stationsAU) 

# clean up workspace
rm(list = c('stations', 'stationDetails', 'stationsAU',
            'AU', 'stationType', 'stationsGeospatial_wqa', 'stationsGeospatial_wqm'))

```

##### AU information for New Stations

So what stations do we have left? These are stations that are in the conventionals dataset but don't have any historical records in CEDS WQA. We will reorganize them into the template format that we need and add AU information from the pinned data on the R server.

```{r automated assessment conventionals last sites to do, eval =  F}
stationsToDo <- filter(conventionals_distinct, ! FDT_STA_ID %in% stationsTable2024begin$STATION_ID) %>% 
  # glean what we can to populate the data template
  dplyr::select(STATION_ID = FDT_STA_ID,
                WATER_TYPE = STA_LV1_CODE,
                TYPE_1 = STA_LV2_CODE,
                LATITUDE = Latitude,
                LONGITUDE = Longitude) %>% 
  mutate(# throw in a flag for swamp but still need to force it to a waterbody type the assessment tools understand
         TYPE_1 =  case_when(WATER_TYPE == 'SWAMP' ~ paste(TYPE_1, 'SWAMP', sep = ': '),
                             TRUE~ as.character(TYPE_1)),
         WATER_TYPE = ifelse(WATER_TYPE == 'SWAMP', NA, WATER_TYPE)) %>% 
  dplyr::select(any_of(names(stationsTemplate))) %>% 
  # Spatially join info we don't trust from CEDS WQM, first turn this into a spatial object
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), 
               remove = F, # don't remove these lat/lon cols from df
               crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng

# get pinned AU data from the R server
AUlookup <- pin_get('ejones/AUlookup', board = 'rsconnect') %>% 
  filter(CYCLE == 2024) # only get sites attributed by assessors for this cycle

# get spatial data from the R server
vahu6 <- st_as_sf(pin_get("ejones/AssessmentRegions_VA84_basins", board = "rsconnect"))
dcr11 <- st_as_sf(pin_get("ejones/dcr11", board = "rsconnect"))

# Spatially join info we don't trust from CEDS WQM
stationsLeft <- st_join(stationsToDo, vahu6) %>% 
  dplyr::select(STATION_ID:LONGITUDE, VAHU6, REGION = ASSESS_REG) %>% 
  st_join(dcr11) %>% 
  dplyr::select(STATION_ID:REGION, WATERSHED = ANCODE) %>% 
  st_drop_geometry() %>% # turn back into tibble
  left_join(dplyr::select(AUlookup, FDT_STA_ID, ID305B_1 ), by = c('STATION_ID' = 'FDT_STA_ID')) # join in AU info from the pinned AUlookup dataset (populated by assessors in metadata attribution app)

# smash into template
stationsTable2024begin <- bind_rows(stationsTable2024begin,
                                     stationsLeft)

# clean up workspace
rm(list = c('stationsLeft', 'stationsToDo', 'stationsTemplate',
            'stationsGeospatial', 'dcr11', 'vahu6', 'AUlookup'))
```

This dataset is pinned to the R server for anyone to use. You can retreive it using the following script.

```{r automated assessment stationsTable2024begin}
stationsTablebegin <- pin_get('ejones/stationsTable2024begin', board = 'rsconnect')

stationsTablebegin %>% # preview first 50 rows
  DT::datatable(rownames = F, options = list(dom = 'lftip', pageLength = 5, scrollX = TRUE))
```



### Clean up PCB dataset 

IR 2022 had to use fuzzyjoining to get PCB StationID to a real DEQ StationID for the data to show up in apps. Data TBD so not writing cleanup methods yet.

This data is pinned to server when ready to be sourced by assessment apps/analysts.

### Clean up Fish Tissue dataset

Who knows what this will look like. We really need a better system for updating data. No scripts included on how to do this yet since the product changes cycle to cycle.

This data is pinned to server when ready to be sourced by assessment apps/analysts.


### Pin Official Clean IR data to R server

By cleaning up all provided data and then pinning to the R server, we can easily distribute and archive all data required and sourced by automated scripts in on secure location. The assessment applications source the pinned data to expedite application rendering time. 

The official assessment data pinned to the R server include:

* Conventionals dataset (including citizen monitoring data)
* Water Column Metals
* Sediment Metals
* VAHU6 spatial data
* WQS information for each station

## Automated Assessment

Now that data are prepped for analysis, the last step is to bring all the data together, complete a few more manipulation steps, and feed the data through a process that assesses the data. To minimize the coding experience required to run the automated assessment process, the operation is written as a loop instead of as a function. Loops are inherently slow and sometimes confusing, but for this specific use case housing all operations inside a single loop increases the visibility of the order of operations and decreases the amount of coding experience users need to understand what is happening when. All individual parameter analyses are programmed as efficient functions and are detailed in the [Individual Parameter Analyses](#individual-parameter-analyses) section.



Bring in pinned data and local data. This part is still being actively written,  stay tuned.


```{r automated assessment pull pins, eval = F}
# official March Data releases for IR 2022
conventionals <- pin_get('ejones/conventionals2024draft', board = 'rsconnect') 
conventionals_distinct <- pin_get('ejones/conventionals2024_distinctdraft', board = 'rsconnect') 

#old will update
stations2020IR <- pin_get("stations2020IR-sf-final", board = "rsconnect")
WQMstationFull <- pin_get("WQM-Station-Full", board = "rsconnect")
VSCIresults <- pin_get("VSCIresults", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) )
VCPMI63results <- pin_get("VCPMI63results", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) )
VCPMI65results <- pin_get("VCPMI65results", board = "rsconnect") %>%
  filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) 
WCmetals <- pin_get("WCmetals-2022IRfinal",  board = "rsconnect")
Smetals <- pin_get("Smetals-2022IRfinal",  board = "rsconnect")
markPCB <- read_excel('data/2022 IR PCBDatapull_EVJ.xlsx', sheet = '2022IR Datapull EVJ')
fishPCB <- read_excel('data/FishTissuePCBsMetals_EVJ.xlsx', sheet= 'PCBs')
fishMetals <- read_excel('data/FishTissuePCBsMetals_EVJ.xlsx', sheet= 'Metals')

```


Bring in Station Table Bulk Upload Template

```{r automated assessment bulk upload template 2, eval = F}
stationsTemplate <- read_excel('WQA_CEDS_templates/WQA_Bulk_Station_Upload_Final.xlsx',#'WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx', 
                               sheet = 'Stations', col_types = "text")[0,] %>% 
  mutate(LATITUDE = as.numeric(LATITUDE), LONGITUDE = as.numeric(LONGITUDE)) %>% 
  mutate_at(vars(contains('_EXC')), as.integer) %>% 
  mutate_at(vars(contains('_SAMP')), as.integer) %>% 
  # new addition that breaks station table upload template but very helpful for assessors
  mutate(BACTERIADECISION = as.character(NA),
         BACTERIASTATS = as.character(NA),
         `Date Last Sampled` = as.character(NA))
```



Bring in Station Table from two previous assessments 

```{r automated assessment stations table 2018, eval = F}
historicalStationsTable2 <- read_excel('data/tbl_ir_mon_stations2018IRfinal.xlsx')
```


Bring in lake nutrient standards

```{r lake nutrient standards, eval = F}
lakeNutStandards <- read_csv('data/9VAC25-260-187lakeNutrientStandards.csv')
```



Workflow

The following steps complete the automated assessment.

Bring in user station table data

This information communicates to the scripts which stations should be assessed and where they should be organized (AUs). It also has data from the last cycle to populate the historical station information table in the application

```{r stationTable, eval = FALSE}
#stationTable <- read_csv('processedStationData/stationsTable2022begin_CitmonJames.csv')

stationTable <- read_csv('processedStationData/stationsTable2022begin.csv')# %>% 
  #filter(STATION_ID %in% kristie$StationID)
```



## Individual Parameter Analyses

### Temperature
### Dissolved Oxygen
### pH
### Bacteria
### Nutrients
### Ammonia
### PCB
### Metals
### Fish Tissue
### Benthics

## Automated Output


This dataset is the end result of the automated assessment process. It is saved as a .csv and used for upload to both the riverine and lacustrine applications. The reason the data are stored in .csv and not on the server is to enable individual assessors to adjust AU information manually whenever needed (e.g. if an AU needs to be split). If this data were stored on the server, then the Assessment Data Analyst would need to be involved any time an assessor needed to adjust which AU(s) a station is connected to. This data is available for download from each of the riverine and lacustrine assessment applications.


