[["index.html", "DEQ Water Quality Automated Assessment User Guide Chapter 1 Introduction", " DEQ Water Quality Automated Assessment User Guide DEQ Automated Assessment Team Last Updated: 2022-10-28 Chapter 1 Introduction This manual serves as a companion document to DEQs Water Quality Assessment (WQA) automated assessment methodology for completion of the biennial 305b/303d Integrated Report. Virginia is among a handful of state agencies that have organized concerted efforts to systematically automate and expedite the water quality assessment process. These entities have all taken different approaches to best meet their state-specific analysis and reporting needs. Through the Tools for Automated Data Assessment (TADA) working group, EPA is organizing an effort to share code and create national tools to assist partners with assessment analyses. Considering the technological advances in water monitoring that have increased the quantity and types of data requiring consideration during an assessment period, as well as the static or decreasing staff time allocated to the assessment process, automation appears the only solution to keep up with Clean Water Act requirements and keep the public informed of the status of waterbody health and restoration timelines for those waterbodies not meeting water quality standards. By embracing automated assessment procedures, entities aim to provide constituents with higher quality, standardized, and transparent water quality assessment results while adhering to federally mandated deadlines. Virginia has taken a hybrid approach to the process of automating the water quality assessments. DEQ relies upon an automated process to manipulate hundreds of thousands of water quality data collected throughout the state during a given assessment period (six year window). These scripts evaluate each record for exceedances of appropriate Water Quality Standards. Regional assessment staff may use interactive web-based analytical tools provide more context to tabular results of exceedance analyses to assist with Quality Assurance/Quality Control (QA/QC) prior to submitting data to EPAs Assessment, Total Maximum Daily Load (TMDL) Tracking and Implementation System (ATTAINS) system through DEQs internal Comprehensive Environmental Data System (CEDS). This approach maximizes the benefits of each partner in the assessment process, allowing computers to excel at systematic and expedited data manipulation and analysis at scale and allowing humans to digest multiple lines of evidence to identify any potential data collection or analysis errors or areas of environmental concern. "],["HowToUseDocument.html", "1.1 How to Use Document", " 1.1 How to Use Document The following report is an agency effort to facilitate the further adoption of automated assessment methods statewide. The intention of this project is to provide a non-programmer audience with accessible and understandable narratives describing the automated water quality assessment process. This document is not a comprehensive WQA Guidance Manual, nor is it an introduction to using the R programming language. Here, we document the overall automated assessment process, explaining reasons certain decisions were made, and how to unpack analyses. "],["ProjectHistory.html", "1.2 Project History", " 1.2 Project History The agency began efforts toward automating components of the IR with a dissolved metals assessment written in SAS. These tabular results were provided to regional assessment staff along with raw data queries encompassing data stored in CEDS for each IR data window. The Freshwater Probabilistic Monitoring program began automating analyses and a final report for inclusion in the 2016 IR using the open source R programming language. These practices have been further refined each IR cycle, improving report quality and graphics as well as significantly reducing the amount of time required to generate a report after data collection. These methods were identified as a possible solution to the ongoing challenges in the WQA program to complete vast amounts of data analysis on increasingly short timelines with limited staff. An effort to systematically analyze all water quality data stored in CEDS for assessments began in 2017. As a pilot project, lakes and reservoirs in the Blue Ridge Regional Office (BRRO) assessment region were selected for first waterbody type to undergo automation and receive an interactive web-based tool to assist regional assessment staff for the 2018 IR. These initial automated assessment efforts were completed using the open source R programming language and interactive applications were built using the Shiny package. Rivers and streams followed suit for the 2020 IR, joining lacustrine waterbodies with automated assessment methods and interactive assessment tools. The 2020 IR automated methods were scaled from just the BRRO region to statewide applicability. A pilot project to incorporate citizen monitoring data requiring assessment was undertaken in the BRRO assessment region for the 2020 IR. This pilot proved effective in standardizing and increasing efficiency of incorporating this disparate data and was officially adopted as a process for future IR windows. After thorough QA from regional assessment staff across the state, the riverine and lacustrine automated assessment tools were rebooted for the 2022 IR with increased functionality and the ability to assess more parameters, including those not stored in CEDS. However, due to delays organizing citizen monitoring data statewide, automated results for these data were not provided for the 2022 IR. A new database schema for archiving station-specific water quality standards and assessment unit information was implemented. This system benefited the assessment process as well as numerous DEQ water programs that previously did not have access to WQS information at that spatial scale. Appendices and fact sheets for the IR were generated using R and Rmarkdown for the first time during the 2022 IR. The 2024 IR further refines improvements to the riverine and lacustrine automated assessment methods and interactive tools. By partnering with the Chesapeake Monitoring Cooperative (CMC), DEQ leveraged an existing public-facing citizen monitoring data portal to expand utility outside the Chesapeake Bay watershed and incorporate all of Virginias citizen monitoring data. These data are now automatically cleaned and stored by the CMC and can more easily be incorporated in the automated assessment process with web scraping techniques. To date, no estuarine-specific assessment methods have been completed, but the WQA program is investigating utility and potential adoption among regions with estuarine waters. "],["Acknowledgements.html", "1.3 Acknowledgements", " 1.3 Acknowledgements Many Water Quality Assessment (WQA) and Water Quality Standards (WQA) staff have contributed to this effort: Emma Jones (emma.jones@deq.virginia.gov) Kristen Bretz (kristen.bretz@deq.virginia.gov) Jason Hill (jason.hill@deq.virginia.gov) Sandy Mueller (sandra.mueller@deq.virginia.gov) Cleo Baker (cleo.baker@deq.virginia.gov) Amanda Shaver (amanda.shaver@deq.virginia.gov) Tish Robertson (tish.robertson@deq.virginia.gov) Paula Main (paula.main@deq.virginia.gov) Mary Dail (mary.dail@deq.virginia.gov) Martha Chapman (martha.chapman@deq.virginia.gov) Sara Jordan (sara.jordan@deq.virginia.gov) Kristie Britt (kristie.britt@deq.virginia.gov) Jennifer Palmore (jennifer.palmore@deq.virginia.gov) Kelley West (kelley.west@deq.virginia.gov) Rebecca Shoemaker (rebecca.shoemaker@deq.virginia.gov) Please direct any project questions to Emma Jones (emma.jones@deq.virginia.gov). "],["data-organization.html", "Chapter 2 Data Organization", " Chapter 2 Data Organization The assessment process requires hundreds of thousands of rows of data collected and stored by DEQ, other state agencies, and citizen partners. These disparate data sources require various levels of data manipulation prior to any analysis. SAS and R are used for the majority of the assessment data cleaning and manipulation processes. 2.0.0.1 Data Location Most data used for assessments are stored in DEQs internal Comprehensive Environmental Data System (CEDS) and made available through a direct connection to the reporting database known as ODS. The agency is working towards storing all data required for assessments in CEDS, but as of the time of writing the following datasets are stored in locations outside of CEDS: Fish Tissue Data PCB Data VDH Data Citizen Monitoring Data Station Metadata It is important to note that although the data that consitute the conventionals dataset are derive from CEDS/ODS, official assessment records of this data are only stored locally in Microsoft Excel outputs. 2.0.0.2 Data Availablility Most of the following data are provided at the beginning of the assessment process (approximately March of an assessment year). Any delays in data availability have ripple effects on the ability of regional assessors to complete their work on time. Should data be provided for assessment after the expected availability date, assessors may not be able to include said data in a given assessment window. Exceptions to the March data availability date usually apply to Citizen Monitoring, bioassessment, and fish tissue data. Citizen Monitoring data have historically been provided to regional assessment staff in April of a given assessment year. Bioassessment data for the most recent two years of an assessment window trickles in to the assessment process through the summer of a given assessment year due to the lengthy identification process associated with benthic macroinvertebrate samples. Fish tissue data requires protracted laboratory analyses before it is provided back to DEQ from contractor labs for assessment purposes. Due to these data delays, regional assessment staff generally have to delay certain assessment steps until all data is available for a given station/Assessment Unit, making automated assessment methods ever more important when data are provided. "],["conventionals-data.html", "2.1 Conventionals Data", " 2.1 Conventionals Data The conventionals dataset contains the bulk of the data analyzed during any given assessment window. Historically, this dataset has been queried using SAS and a direct connection to the raw monitoring data in the ODS reporting database (the database that makes CEDS data accessible to reporting tools). Recently, efforts to streamline the assessment process and standardize data provided across agency programs produced an effort to convert these SAS scripts into R. The conventionals query combines WQM field and analyte data with data handling steps that standardize data discrepancies like multiple lab values returned for identical sample date/times, full parameter speciation but no total value, etc. This R query and data standardization method is considered to be under development as code review is a constant part of data improvement strategies. The current version of the R based conventionals query is available here. In order to access the data the conventionals function calls, users must have a direct connection to ODS from their environment. Please see the DEQ R Methods Encyclopedia article on ODS for more information. Once the official conventionals dataset is provided for a given assessment window, it is stored on the R server as a pinned dataset to preserve a standardized data version (the data of record) in a centralized location accessible by all DEQ staff. This expedites the amount of time it takes to pull the data into an R environment as well as improves assessment application rendering time. An example conventionals dataset is available on the R server and may be retrieved using the script below. Please see the DEQ R Methods Encyclopedia article on pinned data sources for more infomation on how to connect your local system to the R server to query this data. This version of the dataset is used for feature enhancements and application development for the IR2024 cycle. However, this is not the official IR2024 conventionals dataset. conventionals &lt;- pin_get(&#39;ejones/conventionals2024draft&#39;, board = &#39;rsconnect&#39;) conventionals[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["water-column-metals.html", "2.2 Water Column Metals", " 2.2 Water Column Metals query from Roger, for now "],["sediment-metals.html", "2.3 Sediment Metals", " 2.3 Sediment Metals query from Roger, for now "],["fish-tissue-data.html", "2.4 Fish Tissue Data", " 2.4 Fish Tissue Data provided by Rick &amp; Gabe, for now "],["pcb-data.html", "2.5 PCB Data", " 2.5 PCB Data From Mark Richards, not in CEDS "],["vdh-data.html", "2.6 VDH Data", " 2.6 VDH Data Beach closure, HAB event "],["citizen-monitoring-data.html", "2.7 Citizen Monitoring Data", " 2.7 Citizen Monitoring Data Citizen Monitoring data have historically been provided to DEQ in various data formats and schema using numerous digital and analog storage methods. This data system required multiple iterations of lengthy data standardization and QA/QC processes in order to ensure the data were utilized for assessments. A standardized system requiring citizen groups to either upload their data to the Chesapeake Monitoring Cooperative (CMC) Data Explorer and DEQ scraping the CMC API using automated R scripts or provide all data to DEQ in a standardized template has replaced previous data receiving methods. Citizen data not stored in the CMC database live on DEQ staff computers and require further storage solutions. A sample citizen monitoring dataset is available in the exampleData directory for you to download and use when practicing the automated scripts. citmon &lt;- readRDS(&#39;exampleData/citmon.RDS&#39;) citmon[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["station-metadata.html", "2.8 Station Metadata", " 2.8 Station Metadata Station metadata are critical to ensuring that stations are assessed appropriately whether an assessment is conducted by hand or with automation. These metadata include the appropriate Assessment Unit and Water Quality Standard information that apply to the station. After this information is identified for a given station, it may be assessed according the the assessment rules that apply to those standards for the specified waterbody type. Metadata are provided for stations through a combination of automated spatial analyses and human review. No metadata are ever linked to stations without regional assessor review. The Metadata Attribution section covers the details of attributing metadata to each station. "],["low-flow-7q10-data.html", "2.9 Low Flow (7Q10) Data", " 2.9 Low Flow (7Q10) Data The 2024 IR is the first assessment window to use a standardized low flow analysis process. Information presented below is still under review by regional assessment staff. The workflow first analyzes a 7Q10 low flow statistic for all available gages in VA based on the last 50 years of water data. The functions called to analyze the xQy flow statistics are identical to DEQs Water Permitting protocols and were written by Connor Brogan. The water data is provided by the USGS NWIS data repository and is called in the xQy function by the USGS dataRetreival package. Important assumptions of the xQy program are identified below. Once flow statistics are generated for all available gages statewide, this information is compared to available flow data during a given assessment window. Any gages identified below the 7Q10 statistic are flagged for the appropriate time period. This information is spatially joined to the assessment watersheds (VAHU6) to extrapolate available flow data to areas without gaging stations. This is not an ideal extrapolation of flow data, but it serves as a decent initial flag for assessors to know when/where to investigate further. These temporal low flow flags are joined to individual site monitoring data by VAHU6 and VAHU5 during the automated assessment process. If parameters used to assess aquatic life condition are collected during low flow periods, then the data are flagged inside the assessment applications, indicating further review is necessary prior to accepting the automated assessment exceedance calculations for that site. 2.9.1 7Q10 Method The method for identifying low flow information for the assessment period is detailed below. The DFLOW_CoreFunctions_EVJ.R script is an assessment-specific adaptation of Connor Brogans xQy protocols that allow for minor adjustments to the DFLOW procedure to allow for assessment purposes (for more information on these changes, see the Important 7Q10 Calculation Notes section below. This analysis needs to be performed on or after April 2 of each assessment window cutoff to ensure the entire final water year is included in the analysis. The results are posted on the R server for inclusion in the automated assessment methods. library(tidyverse) library(zoo) library(dataRetrieval) library(e1071) library(sf) library(leaflet) library(inlmisc) library(DT) source(&#39;DFLOW_CoreFunctions_EVJ.R&#39;) 2.9.2 USGS Site Data Gathering All USGS gage information sampled in the last 50 years need to be collected from USGS NWIS. We can use the whatNWISsites() function to identify which sites have daily discharge data (00060) in a designated area (stateCd = VA). sites &lt;- whatNWISsites(stateCd=&quot;VA&quot;, parameterCd=&quot;00060&quot;, hasDataTypeCd=&quot;dv&quot;) %&gt;% filter(site_tp_cd %in% c(&#39;ST&#39;, &#39;SP&#39;)) # only keep ST (stream) and SP (spring) sites sites_sf &lt;- sites %&gt;% st_as_sf(coords = c(&quot;dec_long_va&quot;, &quot;dec_lat_va&quot;), remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng Now we will pull daily flow data for each site identified and calculate 7Q10. This is saved in the local environment as a list object with each gage a unique list element. # store it somewhere flowAnalysis &lt;- list() for(i in unique(sites$site_no)){ print(i) siteFlow &lt;- xQy_EVJ(gageID = i,#USGS Gage ID DS=&quot;1972-03-31&quot;,#Date to limit the lower end of usgs gage data download in yyyy-mm-dd DE=&quot;2023-04-01&quot;,#Date to limit the upper end of USGS gage data download in yyyy-mm-dd WYS=&quot;04-01&quot;,#The start of the analysis season in mm-dd. Defaults to April 1. WYE=&quot;03-31&quot;,#The end of the analysis season in mm-dd. Defaults to March 31. x=7,#If you want to include a different xQY then the defaults, enter x here y=10, onlyUseAcceptedData = F ) flowAnalysis[i] &lt;- list(siteFlow) } Using the purrr library, we can extract just the flow metric information for each gage and store in a tibble for use later. # extract 7Q10 by gageNo x7Q10 &lt;- map_df(flowAnalysis, &quot;Flows&quot;) # EVJ added in gageNo to xQy_EVJ() x7Q10[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) And we need the actual daily flow data to compare to the low flow metrics, so we will extract that next. # now to extract flow data already pulled by function flows &lt;- map_df(flowAnalysis, &quot;outdat&quot;) flows[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Since we only really care about flow data from our assessment window, lets extract just the flow data and filter to our IR window of interest. We can then join the low flow metrics by gage number and flag any daily average flow data that falls below the gages 7Q10 metric. # now just grab flow data in assessment window, join in 7Q10, identify any measures below 7Q10 assessmentFlows &lt;- map_df(flowAnalysis, &quot;outdat&quot;) %&gt;% filter(between(Date, as.Date(&quot;2017-01-01&quot;), as.Date(&quot;2022-12-31&quot;))) %&gt;% left_join(x7Q10, by = c(&#39;Gage ID&#39; = &quot;gageNo&quot;)) %&gt;% mutate(`7Q10 Flag` = case_when(Flow &lt;= n7Q10 ~ &#39;7Q10 Flag&#39;, TRUE ~ as.character(NA))) assessmentFlows[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Here we limit our assessmentFlows object to just the rows where a 7Q10 flag is encountered. We can review these low flow events by organizing them by gage and date. # anything below 7Q10? lowAssessmentFlows &lt;- filter(assessmentFlows, `7Q10 Flag` == &#39;7Q10 Flag&#39;) unique(lowAssessmentFlows$`Gage ID`) # what gages do these occur at? # organize low flow events by Gage ID lowAssessmentFlows %&gt;% arrange(`Gage ID`, Date))[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Next, lets review the low flow gages visually on a map. First, we need to transform this low flow information into a spatial object. # see where spatially lowFlowSites &lt;- lowAssessmentFlows %&gt;% distinct(`Gage ID`) %&gt;% left_join(sites_sf, by = c(&#39;Gage ID&#39; = &#39;site_no&#39;)) %&gt;% st_as_sf() We will bring in assessment watersheds to better understand how these low flow events happen across the landscape. vahu6 &lt;- st_read(&#39;../data/GIS/VA_SUBWATERSHED_6TH_ORDER_STG.shp&#39;) # this version of vahu6 layer goes outside state boundary vahu5 &lt;- vahu6 %&gt;% group_by(VAHU5) %&gt;% summarise() And here is a map of the assessment watersheds (VAHU5 and VAHU6) with all Virginia USGS gages (USGS sites) and just USGS gages with low flow events in the IR window (Low Flow USGS sites). CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE, options= leafletOptions(zoomControl = TRUE,minZoom = 5, maxZoom = 20, preferCanvas = TRUE)) %&gt;% setView(-79.1, 37.7, zoom=7) %&gt;% addCircleMarkers(data = sites_sf, color=&#39;gray&#39;, fillColor=&#39;gray&#39;, radius = 4, fillOpacity = 0.8,opacity=0.8,weight = 2,stroke=T, group=&quot;USGS sites&quot;, label = ~site_no) %&gt;% addCircleMarkers(data = lowFlowSites, color=&#39;gray&#39;, fillColor=&#39;red&#39;, radius = 4, fillOpacity = 0.8,opacity=0.8,weight = 2,stroke=T, group=&quot;Low Flow USGS sites&quot;, label = ~`Gage ID`) %&gt;% addPolygons(data= vahu5, color = &#39;black&#39;, weight = 1, fillColor= &#39;blue&#39;, fillOpacity = 0.5,stroke=0.1, group=&quot;vahu5&quot;, label = ~VAHU5) %&gt;% hideGroup(&quot;vahu5&quot;) %&gt;% addPolygons(data= vahu6, color = &#39;black&#39;, weight = 1, fillColor= &#39;blue&#39;, fillOpacity = 0.5,stroke=0.1, group=&quot;vahu6&quot;, label = ~VAHU6) %&gt;% hideGroup(&quot;vahu6&quot;) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&quot;Low Flow USGS sites&quot;,&quot;USGS sites&quot;,&quot;vahu5&quot;,&quot;vahu6&quot;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) Now we need to join the watershed information to the low flow analysis so we can easily incorporate this information to all monitoring sites that fall in the watershed. lowFlowSitesHUC &lt;- st_intersection(lowFlowSites, vahu6) %&gt;% # this layer incorporates VAHU5 and VAHU6 information so dont need to join twice st_drop_geometry() %&gt;% # for this analysis we don&#39;t actually need the spatial information dplyr::select(`Gage ID` = `Gage.ID`, # spatial joins change tibble names, changing back to name we want agency_cd:dec_long_va, VAHU5, VAHU6) lowAssessmentFlows &lt;- left_join(lowAssessmentFlows, lowFlowSitesHUC, by = &#39;Gage ID&#39;) %&gt;% dplyr::select(Agency, `Gage ID`, `Station Name` = station_nm, `Site Type` = site_tp_cd, Latitude = dec_lat_va, Longitude = dec_long_va, Date:Status, `Water Year` = WY, n7Q10, `7Q10 Flag`, VAHU5, VAHU6) This information is pinned to the R server so we can use it during the automated assessment process. pin_get(&#39;ejones/AssessmentWindowLowFlows&#39;, &#39;rsconnect&#39;)[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) 2.9.3 Important 7Q10 Calculation Notes Information from the chief flow statistic programmer (Connor Brogan) about the assumptions of the 7Q10 function used in this analysis: Desired exceedance probability evaluation in check.fs() must be between 0 and 1 Only complete analysis years are included for calculation of both the Harmonic Mean and all low flows. Years with minimum flow of 0 are removed, but compensated for via USGS probability adjustment Must have at least 2 analysis years of complete data (line 133) to calculate xQy flows (not necessary for Harmonic Mean) Provisional gage flow data is removed Negative flows (e.g. tidally reversed) are treated as NA following the USGS SW Toolbox Function only uses gage data where the water year is within the range of the years of DS and DE The gage data is filtered to only include data after the first date of the first analysis season and before the last date of the last analysis season. For instance, if WYS = 04-01 and WYE = 03-31 and DS and DE were 1972 to 2022, then the gage data would be limited to the dates between and including 1972-04-01 and 2022-03-31 The start and end dates of the data must include one at least one instance of WYE The last few days in the analysis season are removed to ensure statistical independence between years Analysis season must have sufficient days to calculate a minimum flow such that at least 7-days are required to calculate a 7-day flow Based on the changes Emma Jones made to the original function for Water Quality Assessment purposes, here are important assumptions to know (numbered according to system above): Must have at least 10 analysis years of complete data to calculate xQy flows Provisional gage flow data is accepted. This is important so water years can be calculated as soon as possible for assessment purposes. Waiting until all data are approved will result in too little time for assessors to review the data. Storm events usually are corrected in the provisional to accepted stage in the USGS QA process, so since we are interested in low flow events, this is not a major concern when using provisional data. "],["automated-assessment-methods.html", "Chapter 3 Automated Assessment Methods", " Chapter 3 Automated Assessment Methods This chapter details the specific steps required to transform the raw data outlined in the Data Organization chapter into preliminary assessment results. The general process the automated assessment specifies is to perform data manipulation and organization steps to prepare the data for analysis, apply specific assessment functions to the appropriate data that specify the assessment guidance and Water Quality Standards (WQS), and output a stations table that uses data from the assessment window to summarize each station for review and bulk upload into CEDS. See the Automated Output section to understand the information provided in the stations table output. The automated assessment scripts are trained to systematically apply assessment guidance and WQS to all data for which the appropriate metadata is provided. This supervised system merely applies the rules that developers have programmed to mimic assessment protocols. These scripts do not provide assessment decisions. It is up to the human reviewer (regional assessor) to either accept the automated result or use best professional judgment to e.g. redact questionable data or interpret complex natural systems. "],["metadata-attribution.html", "3.1 Metadata Attribution", " 3.1 Metadata Attribution The automated assessment process hinges on each station having the appropriate metadata to analyze all raw data. The required metadata for each station include what Water Quality Standards apply to the station and which Assessment Unit(s) describe the station. One or more station can be included in an Assessment Units. It is ultimately the Assessment Units that are used to determine whether or not designated uses are met, which is where the automation stops and the human analysis component is required. In practice, metadata are spatially joined to stations by a rigorous data organization, spatial joining, and QA process that is detailed below. This automated process runs on the Assessment Data Analysts computer and results are provided to regional assessment staff for individual review. This application is known as the Regional Metadata Validation Tool and is hosted on the R server. It is up to each regional assessor to manually review each suggested metadata link prior to the assessment start date. After stations have completed the manual review process, they can be analyzed using the automated assessment scripts. Detailed intructions on how to use the Regional Metadata Validation Tool is available in the Regional Metadata Validation Tool How To section. 3.1.1 Distinct Sites Before station metadata can be linked to stations, a list of unique stations that were sampled in a given assessment window is required. Because multiple data sources are combined for an assessment, each unique station from each data source with data in the given IR window are included in this list. For the purposes of the Automated Assessment User Guide, we will overview the process with a snippet of conventionals and citizen monitoring data types. Please see the official script for more information. library(tidyverse) library(sf) library(pins) library(config) library(lubridate) # Connect to server conn &lt;- config::get(&quot;connectionSettings&quot;) # get configuration settings board_register_rsconnect(key = conn$CONNECT_API_KEY, #Sys.getenv(&quot;CONNECT_API_KEY&quot;), server = conn$CONNECT_SERVER)#Sys.getenv(&quot;CONNECT_SERVER&quot;)) Bring conventionals data into your environment. conventionals &lt;- pin_get(&#39;ejones/conventionals2024draft&#39;, board = &#39;rsconnect&#39;) %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% dplyr::select(FDT_STA_ID, Latitude, Longitude) %&gt;% mutate(Data_Source = &#39;DEQ&#39;) conventionals[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Bring citizen data into your environment. citmon &lt;- readRDS(&#39;exampleData/citmon.RDS&#39;) %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% dplyr::select(FDT_STA_ID, Latitude, Longitude, Data_Source) citmon[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Create an object of unique sites from these two datasets. distinctSites &lt;- bind_rows(conventionals, citmon) distinctSites[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) This process is repeated for all datasets that have data included in the assessment window. These multiple datasets are combined into a single object named distinctSites that we will then compare to existing WQS and AU information. Where stations in our distinctSites object lack either of these pieces of metadata, we must attribute them. 3.1.2 Join Assessment Region and Subbasin Information For rendering purposes in the metadata review application, it is important to have each station linked to the appropriate Assessment Region and Subbasin to limit the amount of data called into the application just to essential information. This information is also important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing. The next chunk reads in the necessary assessment region and subbasin spatial data and spatially joins all sites to these layers. If the sites are missing from the distinctSites_sf object, that means the point plots outside either the assessment region or subbasin polygon. These missingSites are dealt with individually and forced to join to the nearest assessment region and subbasin before rejoining the distinctSites_sf object. assessmentLayer &lt;- st_read(&#39;GIS/AssessmentRegions_VA84_basins.shp&#39;) %&gt;% st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection subbasinLayer &lt;- st_read(&#39;GIS/DEQ_VAHUSB_subbasins_EVJ.shp&#39;) %&gt;% rename(&#39;SUBBASIN&#39; = &#39;SUBBASIN_1&#39;) distinctSites_sf &lt;- st_as_sf(distinctSites, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) %&gt;% # add coordinate reference system, needs to be geographic for now bc entering lat/lng, st_intersection(assessmentLayer ) %&gt;% st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) # if any joining issues occur, that means that there are stations that fall outside the joined polygon area # we need to go back in and fix them manually if(nrow(distinctSites_sf) &lt; nrow(distinctSites)){ missingSites &lt;- filter(distinctSites, ! FDT_STA_ID %in% distinctSites_sf$FDT_STA_ID) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) closest &lt;- mutate(assessmentLayer[0,], FDT_STA_ID =NA) %&gt;% dplyr::select(FDT_STA_ID, everything()) for(i in seq_len(nrow(missingSites))){ closest[i,] &lt;- assessmentLayer[which.min(st_distance(assessmentLayer, missingSites[i,])),] %&gt;% mutate(FDT_STA_ID = missingSites[i,]$FDT_STA_ID) %&gt;% dplyr::select(FDT_STA_ID, everything()) } missingSites &lt;- left_join(missingSites, closest %&gt;% st_drop_geometry(), by = &#39;FDT_STA_ID&#39;) %&gt;% st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) %&gt;% dplyr::select(-c(geometry), geometry) %&gt;% dplyr::select(names(distinctSites_sf)) distinctSites_sf &lt;- rbind(distinctSites_sf, missingSites) } 3.1.3 Join Stations to WQS All stations are then spatially joined to the current WQS spatial layers to link each unique StationID to a unique WQS_ID. These unique WQS_IDs are a concatination of the waterbody type, basin code (e.g. 2A, 2B, 2C, etc.), and a number associated with each segment in the spatial layer. Waterbody types include: RL = Riverine Line LP = Lacustrine Polygon EL = Estuarine Line EP = Estuarine Polygon Since transitioning the WQS storage from local (individual assessor files) to a centralized system (on the R server for multiple program uses), the number of stations that require WQS attribution decreases significantly each IR cycle. To attribute each station to WQS information, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can send that information to an interactive application for humans to manually verify. Where WQS_ID information is already available for stations, they are first removed from the WQS snapping process as to not repeat efforts unnecessarily. WQS_IDs are maintained across WQS layer updates, so any new WQS information will be updated when the stations are joined to the WQS metadata. You can view the available WQSlookup table stored on the server by using the below script. Please see the DEQ R Methods Encyclopedia for information on how to retrieve pinned data from the server. WQStableExisting &lt;- pin_get(&#39;ejones/WQSlookup&#39;, board = &#39;rsconnect&#39;) distinctSitesToDoWQS &lt;- filter(distinctSites_sf, ! FDT_STA_ID %in% WQStableExisting$StationID) Here is the table used to store link information from stations to appropriate WQS. WQStable &lt;- tibble(StationID = NA, WQS_ID = NA) 3.1.3.1 Spatially Join WQS Polygons Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first. 3.1.3.1.0.1 Estuarine Polygons (WQS) Here we find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas. This process also removes any estuarine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS). We will bring in (source) a custom function that runs the analysis for us, feed it our latest WQS information, and let the function run across all input stations. You can see the latest version of the sourced script in this repository. source(&#39;preprocessingModules/WQS_estuaryPoly.R&#39;) # Bring in estuary layer estuarinePolys &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_transform(4326) WQStable &lt;- estuaryPolygonJoin(estuarinePolys, distinctSitesToDoWQS, WQStable) rm(estuarinePolys) # clean up workspace Remove stations that fell inside estuarine polygons from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) 3.1.3.1.1 Lake Polygons (WQS) Next find any sites that fall into a lake WQS polygon. This method is applied to all subbasins at once as it is a simple spatial join. You can see the latest version of the sourced script in this repository. source(&#39;preprocessingModules/WQS_lakePoly.R&#39;) lakesPoly &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_transform(4326) WQStable &lt;- lakePolygonJoin(lakesPoly, distinctSitesToDoWQS, WQStable) rm(lakesPoly) # clean up workspace Remove stations that fell inside lake polygons from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) 3.1.3.2 Spatially Join WQS Lines Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS and where stations remain we will try the estuarine WQS snap. 3.1.3.2.1 Riverine Lines (WQS) To do this join, we will buffer all sites that dont fall into a polygon layer by a set sequence of distances. The output will add a field called Buffer Distance to the WQStable to indicate distance required for snapping. If more than one segment is found within a set buffer distance, then many rows will be attached to the WQStable with the single identifying station name. It is up to the QA tool to help the user determine which of these UIDs are correct and drop the other records. We then remove any riverine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS). source(&#39;snappingFunctions/snapWQS.R&#39;) riverine &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) WQStable &lt;- snapAndOrganizeWQS(distinctSitesToDoWQS, &#39;FDT_STA_ID&#39;, riverine, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters WQStable) rm(riverine) # clean up workspace Remove stations that attached to riverine segments from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) We can use this one last opportunity to test stations that didnt connect to the riverine WQS against the estuarine lines WQS as a one last hope of attributing some WQS information. We will take all stations from the WQStable that didnt snap to any WQS segments (Buffer Distance ==No connections within 80 m) and add those back in to our distinctSitesToDoWQS list to try to snap them to the estuarine lines spatial data. distinctSitesToDoWQS &lt;- filter(WQStable, `Buffer Distance` ==&#39;No connections within 80 m&#39;) %&gt;% left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %&gt;% rename(&#39;StationID&#39;= &#39;FDT_STA_ID&#39;), by=&#39;StationID&#39;) %&gt;% dplyr::select(-c(geometry)) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = T, # don&#39;t remove these lat/lon cols from df crs = 4326) 3.1.3.2.2 Estuarine Lines (WQS) If a site doesnt attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time. Removes any estuary lines sites from the data frame of unique sites that need WQS information. source(&#39;snappingFunctions/snapWQS.R&#39;) estuarineLines &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp&#39; , fid_column_name = &quot;OBJECTID&quot;) #%&gt;% #st_transform(102003) # forcing to albers from start bc such a huge layer #st_transform(4326) # Only send sites to function that could be in estuarine environment WQStable &lt;- snapAndOrganizeWQS(filter(distinctSitesToDoWQS, SUBBASIN %in% c(&quot;Potomac River&quot;, &quot;Rappahannock River&quot;, &quot;Atlantic Ocean Coastal&quot;, &quot;Chesapeake Bay Tributaries&quot;, &quot;Chesapeake Bay - Mainstem&quot;, &quot;James River - Lower&quot;, &quot;Appomattox River&quot;, &quot;Chowan River&quot;, &quot;Atlantic Ocean - South&quot; , &quot;Dismal Swamp/Albemarle Sound&quot;)), &#39;StationID&#39;, estuarineLines, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters WQStable) rm(estuarineLines) Remove stations that attached to estuarine segments from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! StationID %in% WQStable$StationID) Double check no stations were lost in these processes. #Make sure all stations from original distinct station list have some sort of record (blank or populated) int the WQStable. distinctSitesToDoWQS &lt;- filter(distinctSitesToDo, ! FDT_STA_ID %in% WQStableExisting$StationID) distinctSitesToDoWQS$FDT_STA_ID[!(distinctSitesToDoWQS$FDT_STA_ID %in% unique(WQStable$StationID))] 3.1.3.3 Assign something to WQS_ID so sites will not fall through the cracks when application filtering occurs We dont want to give all the assessors statewide all the stations that didnt snap to something, so we need to at least partially assign a WQS_ID so the stations get into the correct subbasin on initial filter. If a station snapped to nothing, we will assigning it a RL WQS_ID and subbasin it falls into by default. WQStableMissing &lt;- filter(WQStable, is.na(WQS_ID)) %&gt;% # drop from list if actually fixed by snap to another segment filter(! StationID %in% filter(WQStable, str_extract(WQS_ID, &quot;^.{2}&quot;) %in% c(&#39;EL&#39;,&#39;LP&#39;,&#39;EP&#39;))$StationID) %&gt;% left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, BASIN_CODE) %&gt;% st_drop_geometry(), by = c(&#39;StationID&#39; = &#39;FDT_STA_ID&#39;)) %&gt;% # some fixes for missing basin codes so they will match proper naming conventions for filtering mutate(BASIN_CODE1 = case_when(is.na(BASIN_CODE) ~ str_pad( ifelse(grepl(&#39;-&#39;, str_extract(StationID, &quot;^.{2}&quot;)), str_extract(StationID, &quot;^.{1}&quot;), str_extract(StationID, &quot;^.{2}&quot;)), width = 2, side = &#39;left&#39;, pad = &#39;0&#39;), TRUE ~ as.character(BASIN_CODE)), BASIN_CODE2 = str_pad(BASIN_CODE1, width = 2, side = &#39;left&#39;, pad = &#39;0&#39;), WQS_ID = paste0(&#39;RL_&#39;, BASIN_CODE2,&#39;_NA&#39;)) %&gt;% dplyr::select(-c(BASIN_CODE, BASIN_CODE1, BASIN_CODE2)) WQStable &lt;- filter(WQStable, !is.na(WQS_ID)) %&gt;% filter(! StationID %in% WQStableMissing$StationID) %&gt;% bind_rows(WQStableMissing) 3.1.4 Join Stations to AUs Assessment Units (AU) are different from WQS attribution steps because these links can change cycle to cycle as new AUs are broken off from existing AUs to more appropriately represent . Thus, a single record of all AUs linked to StationIDs like the WQSlookup table is not a good solution for this use case. Instead the AU to StationID link is stored in a more temporary format (Station Table Excel file) such that regional assessment staff can easily update the AU information on the fly as they assess stations. Generally, the logic holds that the provided AUs are a starting point for an upcoming assessment cycle, not necessarily the final AU for said assessment cycle. The process to link unique stations in an assessment window (distinctSites_sf) to AU information begins by joining all sites in an upcoming window to the Stations Table from the most recent assessment cycle. If a station does not join to a previous assessment cycle Stations Table that means that the station either has not been sample before (e.g. a new station for Probabilistic Monitoring or a special study) or has not been sampled in a long time such that is outside the last assessment window and has not been carried over from a previous assessment cycle for any reason. It is these sites that did not join to the last cycles Stations Table where we will focus our efforts in the subsequent spatial joining steps. 3.1.4.1 Identify which stations need AU information First, bring in the final Stations Table from the most recently completed IR cycle. For this example, we are sourcing the IR2022 final Stations Table (presented as a spatial object in a file geodatabase but we will strip off the spatial data first thing since it is unnecessary for this purpose). We are also going to rename the friendly publication field names to a more standardized format that the automated assessment functions were built upon (read: we are changing field names to match previous versions of the Stations Table schema since the assessment functions were built on that data schema). final2022 &lt;- st_read(&#39;C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_ir22_wqms.gdb&#39;, layer = &#39;va_ir22_wqms&#39;) %&gt;% st_drop_geometry() %&gt;% # only need tabular data from here out # change names of ID305B columns to format required by automated methods rename(ID305B_1 = Assessment_Unit_ID_1, TYPE_1 = Station_Type_1, ID305B_2 = Assessment_Unit_ID_2, TYPE_2 = Station_Type_2, ID305B_3 = Assessment_Unit_ID_3, TYPE_3 = Station_Type_3, ID305B_4 = Assessment_Unit_ID_4, TYPE_4 = Station_Type_4, ID305B_5 = Assessment_Unit_ID_5, TYPE_5 = Station_Type_5, ID305B_6 = Assessment_Unit_ID_6, TYPE_6 = Station_Type_6, ID305B_7 = Assessment_Unit_ID_7, TYPE_7 = Station_Type_7, ID305B_8 = Assessment_Unit_ID_8, TYPE_8 = Station_Type_8, ID305B_9 = Assessment_Unit_ID_9, TYPE_9 = Station_Type_9, ID305B_10 = Assessment_Unit_ID_10, TYPE_10 = Station_Type_10) Now we will join distinct sites to AU information to get all available data to start the assessment process. Note: Assessors may attribute stations to different VAHU6s compared to strictly where the site is located spatially to communicate that said stations (usually that lie close to VAHU6 border) are used to make assessment decisions about the designated VAHU6. For this reason, we use the VAHU6 designation from the previous assessment cycle over the VAHU6 retrieved from CEDS. If the station does not have a record in the previous assessment cycle Stations Table, the VAHU6 designation stored in CEDS is used. The last rows of the below chunk ensure that each station is only listed once in the resultant table. In previous assessment cycles, stations could be assessed for multiple waterbody types (e.g. riverine and lacustrine assessment uses). Since the assessment database was moved from MS Access to CEDS WQA, this duplication is no longer allowed and thus each station should only have one record. distinctSites_AUall &lt;- distinctSites_sf %&gt;% st_drop_geometry() %&gt;% left_join(final2022 %&gt;% dplyr::select(-c(Latitude, Longitude)), # drop duplicate lat/lng fields to avoid join issues by = c(&#39;FDT_STA_ID&#39; = &#39;Station_ID&#39;)) %&gt;% dplyr::select(FDT_STA_ID : VAHU6.y) %&gt;% # drop the last cycle&#39;s results, not important now mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.y))) %&gt;% # use last cycle&#39;s VAHU6 designation over CEDS designation by default if available dplyr::select(-c(VAHU6.x, VAHU6.y)) %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n = n()) %&gt;% ungroup() # Find any duplicates View(filter(distinctSites_AUall, n &gt;1)) # 0 rows, cool # above n&gt; 1 used to be stations that were riverine and lacustrine makes sense, these sites are being used for riverine and lacustrine assessment # for IR2024 this should all be cleaned up bc new WQA CEDS rules, but always good to double check Next we will organize stations by whether or not they have AU data based on the previous join. We will call the stations that need AU information distinctSites_AUtoDo. We only test this using the ID305B_1 column because we only need each station to be attributed to at least one AU. distinctSites_AUtoDo &lt;- filter(distinctSites_AUall, is.na(ID305B_1)) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # These sites are all good for automated assessment (once we join WQS info from WQSlookup table) distinctSites_AU &lt;- filter(distinctSites_AUall, !is.na(ID305B_1)) # Quick QA: double check the math works out nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) As with the WQS spatial attribution process, we will first join these sites to polygon layers and then line features to minimize any unnecessary computational load. 3.1.4.2 Spatially Join AU Polygons Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first. 3.1.4.2.1 Estuarine Polygons (AU) We will now find any sites that fall into an estuary AU polygon. This method is only applied to subbasins that intersect estuarine areas and then removes any estuarine sites from the data frame of unique sites that need AU information (distinctSites_AUtoDo). The chunk below sources a custom function for handling AU polygon information available for download here. source(&#39;preprocessingModules/AU_Poly.R&#39;) # Bring in estuary layer estuaryPolysAU &lt;- st_read(&#39;../va_aus_estuarine.shp&#39;) %&gt;% st_transform( 4326 ) %&gt;% st_cast(&quot;MULTIPOLYGON&quot;) # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427 estuaryPolysAUjoin &lt;- polygonJoinAU(estuaryPolysAU, distinctSites_AUtoDo, estuaryTorF = T) %&gt;% mutate(ID305B_1 = ID305B) %&gt;% dplyr::select(names(distinctSites_AU)) %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n = n(), `Buffer Distance` = &#39;In polygon&#39;) %&gt;% ungroup() rm(estuaryPolysAU) # clean up workspace Now we add the newly identified estuary stations to distinctSites_AU. distinctSites_AU &lt;- bind_rows(distinctSites_AU, estuaryPolysAUjoin %&gt;% st_drop_geometry()) And then remove stations that fell inside estuarine polygons from our to do list. distinctSites_AUtoDo &lt;- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% estuaryPolysAUjoin$FDT_STA_ID) nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) 3.1.4.2.2 Lake Polygons (AU) Next we identify any sites that fall into a lake AU polygon. This method is applied to all subbasins, unlike the previous estuary step. We then remove any lake sites from the data frame of unique sites that need AU information. The chunk below sources the same custom function for handling AU polygon information as above available for download here. source(&#39;preprocessingModules/AU_Poly.R&#39;) # Bring in Lakes layer lakesPolyAU &lt;- st_read(&#39;../va_aus_reservoir.shp&#39;) %&gt;% st_transform( 4326 ) %&gt;% st_cast(&quot;MULTIPOLYGON&quot;) # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427 lakesPolysAUjoin &lt;- polygonJoinAU(lakesPolyAU, distinctSites_AUtoDo, estuaryTorF = F)%&gt;% mutate(ID305B_1 = ID305B, `Buffer Distance` = &#39;In polygon&#39;) %&gt;% dplyr::select(names(distinctSites_AU)) %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n = n()) %&gt;% ungroup() rm(lakesPolyAU) # clean up workspace Now add the lake stations to distinctSites_AU. distinctSites_AU &lt;- bind_rows(distinctSites_AU, lakesPolysAUjoin %&gt;% st_drop_geometry()) And we remove stations that fell inside lake polygons from the to do list. This is also a good time to double check that we havent lost any sites in the above process. distinctSites_AUtoDo &lt;- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% lakesPolysAUjoin$FDT_STA_ID) # Double check all sites are still there nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) rm(lakesPolysAUjoin);rm(estuaryPolysAUjoin) # clean up workspace 3.1.4.3 Spatially Join AU Lines Now on to the more computationally heavy AU line snapping methods. We can only try to attach riverine AUs since there is not a published estuarine lines AU spatial layer (like there is for WQS information). All sites that do not snap to a riverine line within a set buffer distance will be flagged with comment saying so and the regional assessors will sort out the most appropriate AU manually in the Regional Metadata Validation Tool. 3.1.4.3.1 Riverine Lines (AU) Time to run all the sites that didnt fall into an AU polygon through our polyline snapping buffering script. This function is slightly different from the WQS buffering function in that it is flexible enough link any chosen fields from the two input arguments. The output of the function will add a field called Buffer Distance to distinctSites_AU to indicate the distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, that many rows will be attached to the snapTable with the identifying station name. It is up to the Regional Metadata Validation Tool to help the user determine which of these AUs are correct and subsequently drop all other records. source(&#39;snappingFunctions/snapPointToStreamNetwork.R&#39;) riverineAU &lt;- st_read(&#39;../va_aus_riverine.shp&#39;) %&gt;% st_transform(102003) snapTable &lt;- snapAndOrganize(distinctSites_AUtoDo, &#39;FDT_STA_ID&#39;, riverineAU, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters tibble(StationID = character(), ID305B = character(), `Buffer Distance` = character()), &quot;ID305B&quot;) snapTable &lt;- snapTable %&gt;% left_join(distinctSites_AUtoDo, by = c(&#39;StationID&#39; = &#39;FDT_STA_ID&#39;)) %&gt;% # get station information rename(&#39;FDT_STA_ID&#39; = &#39;StationID&#39;) %&gt;% mutate(ID305B_1 = ID305B) %&gt;% dplyr::select(names(distinctSites_AU), `Buffer Distance`) %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n = n()) %&gt;% ungroup() rm(riverineAU) # clean up workspace Now we can add these sites to the sites with AU information. distinctSites_AU &lt;- bind_rows(distinctSites_AU , snapTable ) And then remove stations that attached to riverine segments from the to do list. distinctSites_AUtoDo &lt;- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% snapTable$FDT_STA_ID) We dont have estuarine lines AU information, so the sites that dont connect to any AUs at the max buffer distance will have to be sorted out by the assessors. distinctSites_AU &lt;- distinctSites_AU %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n=n()) We then make sure all stations from original distinct station list (distinctSites_AUall) have some sort of record (blank or populated) in the distinctSites_AU dataset. # check everyone dealt with nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))] if(nrow(distinctSites_AUtoDo) == 0){rm(distinctSites_AUtoDo)} # clean up workspace if QA check good One last step to make sure buffer distances save correctly for future mapping needs. unique(distinctSites_AU$`Buffer Distance`) distinctSites_AU$`Buffer Distance` &lt;- as.character(distinctSites_AU$`Buffer Distance`) So, what does the end result look like? distinctSites_AU[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) 3.1.5 Where does this information go? The above analyses help populate the Regional Metadata Validation Tool. After assessors manually review each station that requires WQS/AU information, this data is consolidated and stored on the R server as a pinned dataset (see for details on that process). That dataset feeds subsequent tools including the Riverine Assessment App, Lakes Assessment App, and Bioassessment Dashboard in addition to querying tools including the CEDS WQM Data Query Tool and CEDS Benthic Data Query Tool. The data is provided via the internal GIS services in the WQM Stations (All stations with full attributes) layer hosted on the GIS REST service and GIS Staff Application. In addition to the published tools that source this data, the dataset is included in countless data analysis projects and products. The entire dataset may be pulled from the R Connect service using the following script. Please see the DEQ R Methods Encyclopedia for information on how to retrieve pinned data from the server. pin_get(&#39;ejones/WQSlookup&#39;, board = &#39;rsconnect&#39;) # raw lookup table pin_get(&quot;ejones/WQSlookup-withStandards&quot;, board = &quot;rsconnect&quot;) # lookup table with WQS information joined "],["organize-metadata.html", "3.2 Organize Metadata", " 3.2 Organize Metadata After all stations for a given assessment window have the appropriate WQS and AU information attributed, there are a number of data organization steps that still need to happen before the data are ready for the automated assessment scripts. These steps include: Reorganize the conventionals dataset to match schema for automated assessment scripts Add Secchi Depth to conventionals dataset Add Citizen Monitoring Data to conventionals Ensure all stations have necessary WQS and AU information Organize all new WQS/AU metadata from the R server and adding new data to pinned datasets Create a stationsTablebegin dataset Carry forward any stations required from last cycle Clean up PCB dataset Clean up Fish Tissue dataset The sections below are purposefully sparse as the methods are in flux given that new data are available in ODS. Stay tuned to see how these methods flesh out for more streamlined data organization in future cycles. 3.2.1 Reorganize Conventionals Dataset The official (SAS) conventionals data schema tends to vary from IR to IR. It is essential that this data are provided to the R scripts exactly how the scripts expect data, so each cycle the provided conventionals dataset must be meticulously QAed to ensure all data are of expected name/type. Additionally, in order to automate the assessment of citizen monitoring data, we must augment the provided conventionals dataset to accommodate the level schema for each citizen monitoring parameter. The nuanced data manipulation steps required to convert the conventionals data and citizen monitoring data from the provided format to the required format for automated analyses are beyond the scope of this book. Instead, the required data schema for automated assessment is provided below. conventionalsSchema &lt;- readRDS(&#39;exampleData/schemaFin.RDS&#39;) conventionalsSchema %&gt;% # preview data DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) 3.2.2 Adding Secchi Depth to conventionals dataset This data are not included in the official (SAS) conventionals data pull but are required for Trophic State Index (TSI) analyses for some lacustrine stations. The below script will only work if a users credentials have access to production ODS wqm data. The example script below first finds all unique stations in the conventionals dataset before querying secchi depth information for those stations for the given assessment window. The date range requires updating each assessment cycle. # dataset of all the unique stations that we need to organize conventionals_distinct &lt;- conventionals %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% # remove any data to avoid confusion dplyr::select(FDT_STA_ID:FDT_COMMENT, Latitude:Data_Source) %&gt;% filter(!is.na(FDT_STA_ID)) # Query secchi data for unique conventionals stations in the appropriate data window library(pool) library(dbplyr) ### connect to Production Environment pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;, Server= &quot;DEQ-SQLODS-PROD,50000&quot;, dbname = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) stationSecchiDepth &lt;- pool %&gt;% tbl(in_schema(&#39;wqm&#39;, &quot;Wqm_Field_Data_View&quot;)) %&gt;% filter(Fdt_Sta_Id %in% !! conventionals_distinct$FDT_STA_ID &amp; between(as.Date(Fdt_Date_Time), &quot;2014-12-31&quot;, &quot;2020-12-31&quot;) &amp; !is.na(Fdt_Secchi_Depth)) %&gt;% # x &gt;= left &amp; x &lt;= right dplyr::select(Fdt_Sta_Id, Fdt_Date_Time, Fdt_Depth, Fdt_Secchi_Depth) %&gt;% as_tibble() %&gt;% mutate(Date = as.Date(Fdt_Date_Time)) %&gt;% dplyr::select(FDT_STA_ID = Fdt_Sta_Id, Date, FDT_DEPTH = Fdt_Depth, Fdt_Secchi_Depth) Once the secchi data is acquired, it can be attached to the original conventionals dataset carefully as detailed below. conventionalsArchive &lt;- conventionals %&gt;% mutate(Date = as.Date(FDT_DATE_TIME)) conventionals &lt;- left_join(conventionalsArchive, stationSecchiDepth, by = c(&#39;FDT_STA_ID&#39;, &#39;Date&#39;, &#39;FDT_DEPTH&#39;)) %&gt;% mutate(SECCHI_DEPTH_M = Fdt_Secchi_Depth) %&gt;% dplyr::select(-c(Fdt_Secchi_Depth, Date)) 3.2.3 Add Citizen Monitoring Data to conventionals Once the citizen monitoring data is queried, QAed, and matches the required conventionals schema (above), the data can be appended to the conventionals dataset for automated analysis. 3.2.4 Ensure all stations have necessary WQS and AU information 3.2.4.1 Organizing all new WQS/AU metadata from the R server A server administrator must retrieve all the WQS/AU information from the attribution application. This directory is consolidated into a single dataset of all new StationID-WQS/AU records. It is important to verify that all expected StationIDs (all distinct stations from the conventionals and citizen monitoring datasets) have WQS and AU information before proceeding. If stations lack this information they will not be assessed accurately. 3.2.4.1.1 Adding new WQS data to pinned datasets Once WQS_ID/AU information is available for all new stations in an assessment cycle, the data are appended to the master WQS information pin on the R server in the case of WQS information. AU information can change cycle to cycle, so the IR year for the StationID-AU information is added to the dataset before pinning back to the master AU information pin. To expedite the transfer of actual WQS information (not just WQS_IDs), another pin contains the actual WQS metadata as well as relevant StationID. This pin is called WQSlookup-withStandards. All new stations attributed need to be joined to the relevant WQS layer to extract this metadata before it can be appended to the master WQS information pin with standards (WQSlookup-withStandards). The script below details this process by bringing in each necessary WQS layer, joining it to the WQSlookup information, and then pinning it to the R server. # table with StationID and WQS_ID information that needs actual WQS metadata WQSlookupToDo &lt;- tibble(StationID = NA, WQS_ID = NA) # blank for example purposes #bring in Riverine layers, valid for the assessment window riverine &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_drop_geometry() # only care about data not geometry WQSlookupFull &lt;- left_join(WQSlookupToDo, riverine, by = &#39;WQS_ID&#39;) %&gt;% filter(!is.na(CLASS)) # drop sites that didn&#39;t actually join rm(riverine) # clean up workspace lacustrine &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_drop_geometry() # only care about data not geometry WQSlookupFull &lt;- left_join(WQSlookupToDo, lacustrine, by = &#39;WQS_ID&#39;) %&gt;% filter(!is.na(CLASS)) %&gt;% # drop sites that didn&#39;t actually join bind_rows(WQSlookupFull) # add these to the larger dataset rm(lacustrine) # clean up workspace estuarineLines &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp&#39; , fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_drop_geometry() # only care about data not geometry WQSlookupFull &lt;- left_join(WQSlookupToDo, estuarineLines, by = &#39;WQS_ID&#39;) %&gt;% filter(!is.na(CLASS)) %&gt;% # drop sites that didn&#39;t actually join bind_rows(WQSlookupFull) # add these to the larger dataset rm(estuarineLines) # clean up workspace estuarinePolys &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_drop_geometry() # only care about data not geometry WQSlookupFull &lt;- left_join(WQSlookupToDo, estuarinePolys, by = &#39;WQS_ID&#39;) %&gt;% filter(!is.na(CLASS)) %&gt;% # drop sites that didn&#39;t actually join bind_rows(WQSlookupFull) # add these to the larger dataset rm(estuarinePolys) # clean up workspace WQSlookup_withStandards_pin &lt;- bind_rows(WQSlookup_withStandards, WQSlookupFull) # for pin The WQSlookup_withStandards_pin object is then pinned to the R server to be sourced by numerous other data users/products. This pin is available by using the following script. WQSlookup_withStandards &lt;- pin_get(&#39;ejones/WQSlookup-withStandards&#39;, board = &#39;rsconnect&#39;) 3.2.4.1.2 Adding new AU data to pinned datasets The new stations in an assessment cycle that were manually attributed to the AU layer from the previous cycle need this information added to the AUlookup table pin stored on the R server. After an assessor adds these stations into WQA CEDS with AU information, this table is no longer used to source AU information for a station, but it is required for the start of an assessment cycle for all stations that were not included in any previous assessments. The server administrator must pull the AU attribution information contained in the Metadata App from the R server and append this information to the existing AUlookup table pin. The data may be retrieved by anyone using the following script. AUlookupArchive &lt;- pin_get(&#39;ejones/AUlookup&#39;, board = &#39;rsconnect&#39;) 3.2.4.2 Create a stationsTablebegin dataset The so called stationsTablebegin dataset is the key input to the automated assessment scripts. This dataset tells the scripts all the stations it should look to assess. It is necessary to use this dataset as the list of stations to assess rather than any other individual dataset (e.g. conventionals, citmon/nonagency, PCB, etc.) because stations that need to be touched during a given assessment period might not have information in any of the individual datasets. This dataset also contains any stations from a previous cycle that must be carried over to the new cycle, carryover stations. These carryover stations may not have any data in the current window and thus would never appear in any individual dataset organized so far. The stationsTablebegin dataset contains not only all the stations that need to be addressed in an assessment cycle, but also at least one Assessment Unit per station for organizational purposes. Because station WQS information are only used for comparing individual parameters to criteria, we do not actually store WQS metadata in the stationsTablebegin dataset. This information is only joined to the conventionals dataset at a later step in the automated assessment process to maintain a dataset most similar to the CEDS bulk upload template. Starting with the conventionals_distinct dataset (which includes citmon/non agency data at this point), we can begin to create our dataset of all the unique stations that we need to organize for the current cycle (stationsTablebegin). # conventionals_distinct &lt;- conventionals %&gt;% # distinct(FDT_STA_ID, .keep_all = T) %&gt;% # # remove any data to avoid confusion # dplyr::select(FDT_STA_ID:FDT_COMMENT, Latitude:Data_Source) %&gt;% # filter(!is.na(FDT_STA_ID)) conventionals_distinct &lt;- pin_get(&#39;ejones/conventionals2024_distinctdraft&#39;, board = &#39;rsconnect&#39;) %&gt;% filter(!is.na(Latitude) | !is.na(Longitude)) # and make a spatial version conventionals_sf &lt;- conventionals_distinct %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng We want the stationsTablebegin dataset to look like our CEDS Bulk Upload template to make future steps easier. The biggest changes from the older stations table format to this new bulk upload template is the addition of the 10 ID305B and station type columns as well as the lacustrine designation. stationsTemplate &lt;- read_excel(&#39;data/WQA_CEDS_templates/WQA_Bulk_Station_Upload_Final.xlsx&#39;,#&#39;WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx&#39;, sheet = &#39;Stations&#39;, col_types = &quot;text&quot;)[0,] # just want structure and not the draft data, force everything to character for now We want to populate as much of the information from the bulk upload template for each station to make the regional assessors lives easier. We will start by filling in as much as we can about each station that had an entry in last cycle. We will pull this information from the WQA area of ODS. You need ODS access to retrieve this information for yourself. Later steps will allow you to source the output of these query/manipulation steps if you do not have ODS access. 3.2.4.2.1 Last cycle AU information Lets start by populating this template with information we can grab from the previous assessment cycle stations table information. Connect to the ODS production environment. library(pool) library(dbplyr) ### Production Environment pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;, #&quot;SQL Server Native Client 11.0&quot;, Server= &quot;DEQ-SQLODS-PROD,50000&quot;, dbname = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) 3.2.4.2.2 Carry forward any stations required from last cycle Find all stations from the last IR cycle that should be carried forward for review this cycle, either impaired last time or with the comment field containing carr% string (e.g. carry over, carried over, etc.). Start by querying stations in IR2022 and join in their AU information and station parameters. The key to these joins is the WXA_STATION_DETAIL_ID/Station Detail Id field. # data prep stations &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &quot;Wqa_Station_Details_View&quot;)) %&gt;% filter(WSD_CYCLE == 2022) %&gt;% as_tibble() %&gt;% distinct(WXA_STATION_DETAIL_ID, .keep_all = T) %&gt;% # distinct on this variable for AU join or duplicated rows for stations filter(WSD_STATION_ID != &#39;4AGSE013.78&#39;) # problem site OIS needs to deal with # WQA geospatial data only seems to have citmon/non agency station locations. All other stations (DEQ) need to be queried from the WQM side of ODS stationsGeospatial_wqa &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;Wqa_Stations_Geospatial_Data_View&#39;)) %&gt;% filter(Station_Id %in% !! stations$STA_NAME) %&gt;% #stations$WSD_STATION_ID) %&gt;% as_tibble() %&gt;% dplyr::select(Station_Id, Latitude, Longitude) # WQM geospatial data for DEQ stations stationsGeospatial_wqm &lt;- pool %&gt;% tbl(in_schema(&#39;wqm&#39;, &#39;WQM_Sta_GIS_View&#39;)) %&gt;% filter(Station_Id %in% !! stations$WSD_STATION_ID) %&gt;% as_tibble() %&gt;% dplyr::select(Station_Id, Latitude, Longitude) # combine geospatial data into one object for easier joining stationsGeospatial &lt;- bind_rows(stationsGeospatial_wqa, stationsGeospatial_wqm) %&gt;% distinct(Station_Id, .keep_all = T) %&gt;% mutate(Station_Id = case_when(Station_Id == &#39;Griggs Pond&#39; ~ toupper(Station_Id), Station_Id == &#39;Sims Metal 003&#39; ~ &#39;SIMS METAL 003&#39;, TRUE ~ as.character(Station_Id))) # Joining problems later if we don&#39;t capitalize the names Griggs Pond and Simms Metal 003 as they are elsewhere in ODS # QUery AU information AU &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA 305b]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationDetails &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA Station Parameters Pivot]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationType &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA Station Detail Types]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationsAU &lt;- left_join(stations, AU, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% left_join(stationType, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% left_join(stationsGeospatial, by = c(&#39;STA_NAME&#39; = &#39;Station_Id&#39;)) %&gt;% # Make sure you join on STA_NAME and not WSD_STATION_ID here!!! left_join(stationDetails, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) # actual analysis, find all stations with impaired parameters impairedStations &lt;- stationsAU %&gt;% filter_at(.vars = vars(contains(&quot;Status Code&quot;)), .vars_predicate = any_vars(str_detect(., &#39;IM&#39;))) # Cast a wide net: string search for any stations that were carried over from more previous cycles by looking for # variations of the phrase &quot;Carry&quot; in the station comment field carryoverStations &lt;- stationsAU %&gt;% filter(str_detect(WSD_COMMENTS, &#39;carr&#39;)) # combine lists and remove duplicates stationsFromLastCycle &lt;- bind_rows(impairedStations, carryoverStations) %&gt;% distinct(WXA_STATION_DETAIL_ID, .keep_all = T) # clean up workspace rm(list = c(&#39;stations&#39;,&#39;stationsAU&#39;, &#39;stationDetails&#39;, &#39;impairedStations&#39;, &#39;carryoverStations&#39;, &#39;AU&#39;, &#39;stationType&#39;, &#39;stationsGeospatial_wqa&#39;, &#39;stationsGeospatial_wqm&#39;)) Now we need to clean up this data to match the bulk upload data template. We will also strip out the data from previous cycles to not confuse anyone from cycle to cycle. stationsTable2024begin &lt;- stationsFromLastCycle %&gt;% dplyr::select(STATION_ID = STA_NAME, #### OR COULD USE WSD_STATION_ID ID305B_1 = `ID305B 1`, ID305B_2 = `ID305B 2`, ID305B_3 = `ID305B 3`, ID305B_4 = `ID305B 4`, ID305B_5 = `ID305B 5`, ID305B_6 = `ID305B 6`, ID305B_7 = `ID305B 7`, ID305B_8 = `ID305B 8`, ID305B_9 = `ID305B 9`, ID305B_10 = `ID305B 10`, WATER_TYPE = WWT_WATER_TYPE_DESC, SALINITY = WSC_DESCRIPTION, LACUSTRINE = WSD_LAC_ZONE_YN, REGION = STA_REGION, TYPE_1 = `Station Type 1`, TYPE_2 = `Station Type 2`, TYPE_3 = `Station Type 3`, TYPE_4 = `Station Type 4`, TYPE_5 = `Station Type 5`, TYPE_6 = `Station Type 6`, TYPE_7 = `Station Type 7`, TYPE_8 = `Station Type 8`, TYPE_9 = `Station Type 9`, TYPE_10 = `Station Type 10`, LATITUDE = Latitude, LONGITUDE = Longitude, WATERSHED = STA_WATERSHED, VAHU6 = STA_VA_HU6) %&gt;% mutate(REGION = case_when(REGION == &#39;NVRO&#39; ~ &#39;NRO&#39;, REGION == &#39;WCRO&#39; ~ &#39;BRRO&#39;, TRUE~ as.character(REGION))) %&gt;% dplyr::select(any_of(names(stationsTemplate))) stationsTable2024begin &lt;- bind_rows(stationsTemplate %&gt;% mutate(LATITUDE = as.numeric(LATITUDE), LONGITUDE = as.numeric(LONGITUDE)), stationsTable2024begin) # add back in missing columns Quick QA check for any missing geospatial data. missingGeospatial &lt;- filter(stationsTable2024begin, is.na(LATITUDE) | is.na(LONGITUDE)) # clean up workspace rm(list = c(&#39;stationsFromLastCycle&#39;,&#39;missingGeospatial&#39;)) 3.2.4.2.3 AU information from Last Cylce Now we need to get the same metadata for all the stations from the conventionals (and citmon/non agency) dataset from the current cycle. We can make our lives easier by only doing this work for new stations from the conventionals dataset (i.e. dropping all stations from our to do list that already have this information from our last step). This chunk will also reformat the queried data into the bulk upload template format so it can be combined with the stationsTable2024begin dataset created above. stationsToDo &lt;- filter(conventionals_distinct, ! FDT_STA_ID %in% stationsTable2024begin$STATION_ID) # use the same method from above with a new station list stations &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &quot;Wqa_Station_Details_View&quot;)) %&gt;% filter(WSD_STATION_ID %in% !! stationsToDo$FDT_STA_ID) %&gt;% # pull all data from stations identified above as_tibble() %&gt;% # get that data local before doing more complicated things than SQL wants to handle # keep only the most recent record for each station by grouping and then filtering group_by(WSD_STATION_ID) %&gt;% filter(WSD_CYCLE == max(WSD_CYCLE )) %&gt;% distinct(WXA_STATION_DETAIL_ID, .keep_all = T) %&gt;% # still need only 1 record per site ungroup() # ungroup so the WSD_STATION_ID column doesn&#39;t come along for the ride to future steps where not necessary # WQA geospatial data only seems to have citmon/non agency station locations. All other stations (DEQ) need to be queried from the WQM side of ODS stationsGeospatial_wqa &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;Wqa_Stations_Geospatial_Data_View&#39;)) %&gt;% filter(Station_Id %in% !! stations$STA_NAME) %&gt;% #stations$WSD_STATION_ID) %&gt;% as_tibble() %&gt;% dplyr::select(Station_Id, Latitude, Longitude) # WQM geospatial data for DEQ stations stationsGeospatial_wqm &lt;- pool %&gt;% tbl(in_schema(&#39;wqm&#39;, &#39;WQM_Sta_GIS_View&#39;)) %&gt;% filter(Station_Id %in% !! stations$WSD_STATION_ID) %&gt;% as_tibble() %&gt;% dplyr::select(Station_Id, Latitude, Longitude) # combine geospatial data into one object for easier joining stationsGeospatial &lt;- bind_rows(stationsGeospatial_wqa, stationsGeospatial_wqm) %&gt;% distinct(Station_Id, .keep_all = T) AU &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA 305b]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationDetails &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA Station Parameters Pivot]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationType &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA Station Detail Types]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationsAU &lt;- left_join(stations, AU, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% left_join(stationType, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% left_join(stationsGeospatial, by = c(&#39;STA_NAME&#39; = &#39;Station_Id&#39;)) %&gt;% # Make sure you join on STA_NAME and not WSD_STATION_ID here!!! left_join(stationDetails, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% # reorganize to fit the data template dplyr::select(STATION_ID = STA_NAME, #### OR COULD USE WSD_STATION_ID ID305B_1 = `ID305B 1`, ID305B_2 = `ID305B 2`, ID305B_3 = `ID305B 3`, ID305B_4 = `ID305B 4`, ID305B_5 = `ID305B 5`, ID305B_6 = `ID305B 6`, ID305B_7 = `ID305B 7`, ID305B_8 = `ID305B 8`, ID305B_9 = `ID305B 9`, ID305B_10 = `ID305B 10`, WATER_TYPE = WWT_WATER_TYPE_DESC, SALINITY = WSC_DESCRIPTION, LACUSTRINE = WSD_LAC_ZONE_YN, REGION = STA_REGION, TYPE_1 = `Station Type 1`, TYPE_2 = `Station Type 2`, TYPE_3 = `Station Type 3`, TYPE_4 = `Station Type 4`, TYPE_5 = `Station Type 5`, TYPE_6 = `Station Type 6`, TYPE_7 = `Station Type 7`, TYPE_8 = `Station Type 8`, TYPE_9 = `Station Type 9`, TYPE_10 = `Station Type 10`, LATITUDE = Latitude, LONGITUDE = Longitude, WATERSHED = STA_WATERSHED, VAHU6 = STA_VA_HU6) %&gt;% mutate(REGION = case_when(REGION == &#39;NVRO&#39; ~ &#39;NRO&#39;, REGION == &#39;WCRO&#39; ~ &#39;BRRO&#39;, TRUE~ as.character(REGION))) %&gt;% dplyr::select(any_of(names(stationsTemplate))) # Smash in with the rest of the sites already organized stationsTable2024begin &lt;- bind_rows(stationsTable2024begin, stationsAU) # clean up workspace rm(list = c(&#39;stations&#39;, &#39;stationDetails&#39;, &#39;stationsAU&#39;, &#39;AU&#39;, &#39;stationType&#39;, &#39;stationsGeospatial_wqa&#39;, &#39;stationsGeospatial_wqm&#39;)) 3.2.4.2.4 AU information for New Stations So what stations do we have left? These are stations that are in the conventionals dataset but dont have any historical records in CEDS WQA. We will reorganize them into the template format that we need and add AU information from the pinned data on the R server. stationsToDo &lt;- filter(conventionals_distinct, ! FDT_STA_ID %in% stationsTable2024begin$STATION_ID) %&gt;% # glean what we can to populate the data template dplyr::select(STATION_ID = FDT_STA_ID, WATER_TYPE = STA_LV1_CODE, TYPE_1 = STA_LV2_CODE, LATITUDE = Latitude, LONGITUDE = Longitude) %&gt;% mutate(# throw in a flag for swamp but still need to force it to a waterbody type the assessment tools understand TYPE_1 = case_when(WATER_TYPE == &#39;SWAMP&#39; ~ paste(TYPE_1, &#39;SWAMP&#39;, sep = &#39;: &#39;), TRUE~ as.character(TYPE_1)), WATER_TYPE = ifelse(WATER_TYPE == &#39;SWAMP&#39;, NA, WATER_TYPE)) %&gt;% dplyr::select(any_of(names(stationsTemplate))) %&gt;% # Spatially join info we don&#39;t trust from CEDS WQM, first turn this into a spatial object st_as_sf(coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng # get pinned AU data from the R server AUlookup &lt;- pin_get(&#39;ejones/AUlookup&#39;, board = &#39;rsconnect&#39;) %&gt;% filter(CYCLE == 2024) # only get sites attributed by assessors for this cycle # get spatial data from the R server vahu6 &lt;- st_as_sf(pin_get(&quot;ejones/AssessmentRegions_VA84_basins&quot;, board = &quot;rsconnect&quot;)) dcr11 &lt;- st_as_sf(pin_get(&quot;ejones/dcr11&quot;, board = &quot;rsconnect&quot;)) # Spatially join info we don&#39;t trust from CEDS WQM stationsLeft &lt;- st_join(stationsToDo, vahu6) %&gt;% dplyr::select(STATION_ID:LONGITUDE, VAHU6, REGION = ASSESS_REG) %&gt;% st_join(dcr11) %&gt;% dplyr::select(STATION_ID:REGION, WATERSHED = ANCODE) %&gt;% st_drop_geometry() %&gt;% # turn back into tibble left_join(dplyr::select(AUlookup, FDT_STA_ID, ID305B_1 ), by = c(&#39;STATION_ID&#39; = &#39;FDT_STA_ID&#39;)) # join in AU info from the pinned AUlookup dataset (populated by assessors in metadata attribution app) # smash into template stationsTable2024begin &lt;- bind_rows(stationsTable2024begin, stationsLeft) # clean up workspace rm(list = c(&#39;stationsLeft&#39;, &#39;stationsToDo&#39;, &#39;stationsTemplate&#39;, &#39;stationsGeospatial&#39;, &#39;dcr11&#39;, &#39;vahu6&#39;, &#39;AUlookup&#39;)) This dataset is pinned to the R server for anyone to use. You can retreive it using the following script. stationsTablebegin &lt;- pin_get(&#39;ejones/stationsTable2024begin&#39;, board = &#39;rsconnect&#39;) stationsTablebegin %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) ## Warning in instance$preRenderHook(instance): It seems your data is too big ## for client-side DataTables. You may consider server-side processing: https:// ## rstudio.github.io/DT/server.html 3.2.5 Clean up PCB dataset IR 2022 had to use fuzzyjoining to get PCB StationID to a real DEQ StationID for the data to show up in apps. Data TBD so not writing cleanup methods yet. This data is pinned to server when ready to be sourced by assessment apps/analysts. 3.2.6 Clean up Fish Tissue dataset Who knows what this will look like. We really need a better system for updating data. No scripts included on how to do this yet since the product changes cycle to cycle. This data is pinned to server when ready to be sourced by assessment apps/analysts. 3.2.7 Pin Official Clean IR data to R server By cleaning up all provided data and then pinning to the R server, we can easily distribute and archive all data required and sourced by automated scripts in on secure location. The assessment applications source the pinned data to expedite application rendering time. The official assessment data pinned to the R server include: Conventionals dataset (including citizen monitoring data) Water Column Metals Sediment Metals VAHU6 spatial data WQS information for each station "],["automated-assessment.html", "3.3 Automated Assessment", " 3.3 Automated Assessment Now that data are prepped for analysis, the last step is to bring all the data together, complete a few more manipulation steps, and feed the data through a process that assesses the data. To minimize the coding experience required to run the automated assessment process, the operation is written as a loop instead of as a function. Loops are inherently slow and sometimes confusing, but for this specific use case housing all operations inside a single loop increases the visibility of the order of operations and decreases the amount of coding experience users need to understand what is happening when. All individual parameter analyses are programmed as efficient functions and are detailed in the Individual Parameter Analyses section. Bring in pinned data and local data. This part is still being actively written, stay tuned. # official March Data releases for IR 2022 conventionals &lt;- pin_get(&#39;ejones/conventionals2024draft&#39;, board = &#39;rsconnect&#39;) conventionals_distinct &lt;- pin_get(&#39;ejones/conventionals2024_distinctdraft&#39;, board = &#39;rsconnect&#39;) #old will update stations2020IR &lt;- pin_get(&quot;stations2020IR-sf-final&quot;, board = &quot;rsconnect&quot;) WQMstationFull &lt;- pin_get(&quot;WQM-Station-Full&quot;, board = &quot;rsconnect&quot;) VSCIresults &lt;- pin_get(&quot;VSCIresults&quot;, board = &quot;rsconnect&quot;) %&gt;% filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) VCPMI63results &lt;- pin_get(&quot;VCPMI63results&quot;, board = &quot;rsconnect&quot;) %&gt;% filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) VCPMI65results &lt;- pin_get(&quot;VCPMI65results&quot;, board = &quot;rsconnect&quot;) %&gt;% filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) WCmetals &lt;- pin_get(&quot;WCmetals-2022IRfinal&quot;, board = &quot;rsconnect&quot;) Smetals &lt;- pin_get(&quot;Smetals-2022IRfinal&quot;, board = &quot;rsconnect&quot;) markPCB &lt;- read_excel(&#39;data/2022 IR PCBDatapull_EVJ.xlsx&#39;, sheet = &#39;2022IR Datapull EVJ&#39;) fishPCB &lt;- read_excel(&#39;data/FishTissuePCBsMetals_EVJ.xlsx&#39;, sheet= &#39;PCBs&#39;) fishMetals &lt;- read_excel(&#39;data/FishTissuePCBsMetals_EVJ.xlsx&#39;, sheet= &#39;Metals&#39;) Bring in Station Table Bulk Upload Template stationsTemplate &lt;- read_excel(&#39;WQA_CEDS_templates/WQA_Bulk_Station_Upload_Final.xlsx&#39;,#&#39;WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx&#39;, sheet = &#39;Stations&#39;, col_types = &quot;text&quot;)[0,] %&gt;% mutate(LATITUDE = as.numeric(LATITUDE), LONGITUDE = as.numeric(LONGITUDE)) %&gt;% mutate_at(vars(contains(&#39;_EXC&#39;)), as.integer) %&gt;% mutate_at(vars(contains(&#39;_SAMP&#39;)), as.integer) %&gt;% # new addition that breaks station table upload template but very helpful for assessors mutate(BACTERIADECISION = as.character(NA), BACTERIASTATS = as.character(NA), `Date Last Sampled` = as.character(NA)) Bring in Station Table from two previous assessments historicalStationsTable2 &lt;- read_excel(&#39;data/tbl_ir_mon_stations2018IRfinal.xlsx&#39;) Bring in lake nutrient standards lakeNutStandards &lt;- read_csv(&#39;data/9VAC25-260-187lakeNutrientStandards.csv&#39;) Workflow The following steps complete the automated assessment. Bring in user station table data This information communicates to the scripts which stations should be assessed and where they should be organized (AUs). It also has data from the last cycle to populate the historical station information table in the application #stationTable &lt;- read_csv(&#39;processedStationData/stationsTable2022begin_CitmonJames.csv&#39;) stationTable &lt;- read_csv(&#39;processedStationData/stationsTable2022begin.csv&#39;)# %&gt;% #filter(STATION_ID %in% kristie$StationID) "],["individual-parameter-analyses.html", "3.4 Individual Parameter Analyses", " 3.4 Individual Parameter Analyses 3.4.1 Temperature 3.4.2 Dissolved Oxygen 3.4.3 pH 3.4.4 Bacteria 3.4.5 Nutrients 3.4.6 Ammonia 3.4.7 PCB 3.4.8 Metals 3.4.9 Fish Tissue 3.4.10 Benthics "],["automated-output.html", "3.5 Automated Output", " 3.5 Automated Output This dataset is the end result of the automated assessment process. It is saved as a .csv and used for upload to both the riverine and lacustrine applications. The reason the data are stored in .csv and not on the server is to enable individual assessors to adjust AU information manually whenever needed (e.g. if an AU needs to be split). If this data were stored on the server, then the Assessment Data Analyst would need to be involved any time an assessor needed to adjust which AU(s) a station is connected to. This data is available for download from each of the riverine and lacustrine assessment applications. "],["regional-metadata-validation-tool-how-to.html", "Chapter 4 Regional Metadata Validation Tool How To", " Chapter 4 Regional Metadata Validation Tool How To The Regional Metadata Validation Tool is located on the R server. The purpose of this tool is to streamline the QA of metadata attached to individual stations that was performed by automated scripts. This metadata include Water Quality Standards (WQS) and Assessment Unit (AU) information that are necessary to properly analyze and organize assessment results. A scripted process automates the data organization of stations from disparate data sources, spatially joins these sites to the appropriate WQS and AUs where information is not available from previous cycles, and runs basic QA on the output to ensure all stations that were expected to receive metadata did in fact get attributed. These scripts are detailed in the Metadata Attribution section. It is the responsibility of each regional assessor to review the metadata linked through the automated snapping process to ensure the closest snapped information does in fact apply to the given station. This application offers an opportunity to perform QA on the spatial WQS and AU layers and communicate with Central Office Assessment/WQS staff when mistakes are found. "],["application-orientation.html", "4.1 Application Orientation", " 4.1 Application Orientation This application expedites the review of the spatially joined output by organizing stations into the appropriate waterbody type, Assessment Region, and subbasin. The application is divided into two tabs (corresponding to AU review and WQS review) that can be navigated between using the navigation bar at the top of the page. The application opens to the Assessment Unit Review tab by default, but there is no specified order in which a reviewer must interact with either tab. Data for each tab are saved back to the R server independently. Each of these tabs have a drop down option that is accessible by clicking the triangle adjacent to the name of the tab. These options are identical for both the Assessment Unit QA and Water Quality Standards QA tabs. The top option called Watershed Selection allows users to choose certain types of stations to review and the bottom option called Manual Review allows users to visualize the stations selected in the Watershed Selection tab option in more detail. The final tab called About offers instructions to the user similar to what is detailed below. The following application workflow is applicable to both the Assessment Unit QA and Water Quality Standards QA sections of the application. For brevity, this user guide will only go over instructions for the WQS side of the application, but the application flow applied to the WQS side of the application mimics the application flow applied on the AU side of the application. "],["application-use-water-quality-standards-qa-watershed-selection-tab.html", "4.2 Application Use: Water Quality Standards QA- Watershed Selection Tab", " 4.2 Application Use: Water Quality Standards QA- Watershed Selection Tab On load, the tab will provide three related drop down menus on the left hand side of the page. These interact in a top-down hierarchical system where the selection in the top drop down, Select Waterbody Type, dictates available options in the middle drop down, Select DEQ Assessment Region, which then dictates the available options in the bottom drop down, Select Subbasin. Users navigate through these cascading filters to identify the precise waterbody type (e.g. riverine, lacustrine, or estuarine), assessment region, and subbasin to tackle QA in a given session. Once the user is happy with their selection, they must click the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button. This button uses the user provided information to retrieve all the applicable stations that require QA. This information is sourced from the R server in addition to all the applicable spatial layers necessary to produce meaningful maps in the subsequent Manual Review tab option. Once this information is retrieved from the R server, a basic interactive map of the selected subbasin populates the tab as well as verbal breakdowns of the type of information that needs to be reviewed for the selected subbasin (Preprocessing Data Recap for Selected Region/Subbasin/Type Combination). Based on the summarized information about the stations in the selected subbasin, the user may choose to begin reviewing that subbasin or they might scroll through other waterbody type/region/subbasin combinations until a more desirable to do list is identified. Each time a user changes any of the drop down inputs, they must press the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button to retrieve the associated station breakdown from the chosen subbasin. A small progress bar appears in the lower right corner of the browser window to communicate that large spatial data are being transmitted to the app. The popup disappears when the map re-renders and loads the new spatial information. Once a user is ready to review the stations in the selected subbasin, the user must navigate to the Manual Review tab under the Water Quality Standards QA drop down on the navigation bar. "],["application-use-water-quality-standards-qa-manual-review-tab.html", "4.3 Application Use: Water Quality Standards QA- Manual Review Tab", " 4.3 Application Use: Water Quality Standards QA- Manual Review Tab On load, the Manual Review tab renders three buttons, an interactive map of Virginia, and an area for output information from the map that is blank until user input dictates. 4.3.1 Map Orientation The map allows users to zoom and pan interactively with the mouse. In the top left corner there are a number of tools to aid users. The plus and minus buttons offer fixed zoom in/out. The home button returns the map orientation to the original zoom level in case a user gets lost. The magnifying glass button allows users to search for and zoom to a selected station. To use this feature, click on the magnifying glass, begin typing the StationID of interest in the text box (after each character entered, a dropdown of options based on the characters entered appears), once a station is selected, the map will zoom to that station and highlight it with a red circle. You may notice that if there isnt the All Stations in Basin layer on, then no station point appears inside the red circle. Additional layers available in the map (by hovering over the layers button on the left side) include All Stations in Basin (all other stations identified in the basin with data for the given IR window) and Assessment Regions (regional assessment boundaries). 4.3.2 What do the buttons do? The three buttons correspond to the station snapping summaries presented on the Watershed Selection tab. In the example above, we can see this subbasin has: * There are 8 stations that snapped to 1 WQS segment in preprocessing. * There are 1 stations that snapped to &gt; 1 WQS segment in preprocessing. * There are 1 stations that snapped to 0 WQS segments in preprocessing. By clicking the Plot stations that snapped to 1 WQS Segment, users will see all the stations that only retrieved one WQS segment in the spatial snapping process. They are colored from green to light red based on the buffer distance required to snap to a segment (within the smallest set buffer distance of the sequence dictated to the automated snapping functions). Green stations snapped to segments at closer distances compared to red sites. The legend for the buffer distance stations colors is presented in the right corner of the app. By clicking the Plot stations that snapped to &gt;1 WQS Segment, users will see all the stations that retrieved multiple WQS segments in the spatial snapping process (within the smallest set buffer distance of the sequence dictated to the automated snapping functions). They are colored to red to indicate these stations need more attention. By clicking the Plot stations that snapped to 0 WQS Segment, users will see all the stations that retrieved no WQS segments in the spatial snapping process (within the largest buffer distance of the sequence dictated to the automated snapping functions). They are colored to orange to indicate these stations need the most attention. If any of the categories listed above had 0 stations, a warning message will appear in the lower right corner of the app indicating there is nothing for the button to plot. 4.3.3 Application Operation The goal of this section of the application is to review each station and either accept or adjust the provided WQS information snapped to the site. To review a site, users may zoom to a selected map area and click on a site. By clicking a site, users are selecting a site to QA, as indicated by the green halo around the selected site. The associated metadata is now populated in the Stations Data and Spatially Joined WQS tab below the map. The Selected Station Information table details information about the station including: * the WQS_ID of snapped segment(s) * the Buffer Distance (distance required to snap to the linked segment(s)) * n (the number of segments linked to the station during spatial processing) * a hyperlink to the internal GIS Web Application that pulls up the appropriate WQS layer and station information in a new browser tab * additional station metadata information The Spatially Joined WQS Information table details information about the WQS segment(s) attached to the selected station including: * WQS_ID (unique code attributed to each WQS segment) * GNIS_Name (name of the WQS segment) * SEC (WQS section) * CLASS (WQS class) * SPSTDS (WQS special standards) * SECTION_DESCRIPTION (narrative description of the section location) * PWS (whether or not the section is designated as a Public Water Supply) * Tier_III (whether or not the section is designated as a Tier III water) * additional WQS segment information Users may click on more than one station and subsequently reveal information about more than one station in the Stations Data and Spatially Joined WQS tab below the map. If more than one row is presented in a table, a y scroll bar on the right side of the table will automatically appear such that the user may access all the table information. 4.3.3.1 Application Operation: Clear Selection To deselect a station or stations, click the blue Clear Selection button below the map. ` 4.3.4 Application Operation: Station that Snapped to 1 WQS Segment Should a user select a station for review that snapped to 1 WQS segment, the process for QAing that station is simple. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. Additionally, when the Plot stations that snapped to 1 WQS Segment button was pressed, additional spatial layers became available in the layers drop down of the map. The user may now turn on the layer called WQS Segments of Stations Snapped to 1 Segment to reveal all the segments in that category. These segments may be clicked on in the map to reveal the same information from the Selected Station Information table in a popup on the map. 4.3.4.1 Application Operation: Accept If the user is satisfied with the snapped WQS segment, they may press the blue Accept button to reveal a modal window named Accept Snapped WQS. This modal shows the information from the Selected Station Information table one last time for review. There is a textbox below the table where users may input any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. Once the user presses the Accept button in the Accept Snapped WQS modal, the station and WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.4.2 Application Operation: Manual WQS Adjustment (1 segment) If the user is not satisfied with the snapped WQS segment, they may press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. This modal shows the information from the Selected Station Information table one last time for review. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Pro Tip: By clicking the record in the WQS_ID cell of the table on the modal window three time quickly, the cell contents will be highlighted. Users may use the keyboard shortcuts to copy(ctrl+c)/paste(ctrl+v) this information into the text box below the table. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.4.3 Application Operation: Export Reviews All the work the user has completed so far accepting or manually adjusting WQS are only saved in the local application environment. The only way to preserve the work completed in the application is to press the green Export reviews button. It is highly recommended that users click the Export reviews button frequently (every 3-4 stations or after a particularly challenging review) to ensure their work is in fact saved on the server. Users may undo any accidental work still in their local application environment by either closing the app or returning to the Watershed Selection tab without pressing the Export reviews button. This work flow would look like: User selects station, Accept User selects station, Accept User selects station, Accept User presses Export reviews to save work to the R server User selects station, Accept- user realizes they made a mistake and doesnt want to save that accept to the server User closes the app or navigates back to the Watershed Selection tab without pressing the Export reviews button- all information completed in the app since the last time Export reviews was pressed will not be saved User opens the app or clicks the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button on the Watershed Selection tab to pull the last information saved on the server User proceeds to the Manual Review tab and continues work 4.3.5 Application Operation: Station that Snapped to &gt;1 WQS Segment Should a user select a station for review that snapped to &gt;1 WQS segment, the process for QAing that station is relatively simple. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. Additionally, when the Plot stations that snapped to &gt;1 WQS Segment button was pressed, additional spatial layers became available in the layers drop down of the map. The user may now turn on the layer called WQS Segments of Stations Snapped to &gt;1 Segment to reveal all the segments in that category. These segments are plotted on a color ramp to distinguish them from one another and may be clicked on in the map to reveal the same information from the Selected Station Information table in a popup on the map. The GIS Web App hyperlinks may also be useful to use at this stage. 4.3.5.1 Application Operation: Manual WQS Adjustment (&gt; 1 segment) If the user needs to choose between multiple WQS segments, they must press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. The user cannot accept a single StationID that is linked to multiple WQS_IDs using the Accept button. The Manually Adjust WQS modal shows the information from the Selected Station Information table one last time for review. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Pro Tip: By clicking the record in the WQS_ID cell of the table on the modal window three time quickly, the cell contents will be highlighted. Users may use the keyboard shortcuts to copy(ctrl+c)/paste(ctrl+v) this information into the text box below the table. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.6 Application Operation: Station that Snapped to 0 WQS Segments Should a user select a station for review that snapped to no WQS segments, the process for QAing that station is more involved. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. To preserve application rendering time, additional spatial layers are not available in the layers drop down of the map for this scenario. Instead, the user should click the Open Link In New Tab button in the Selected Station Information table to navigate to the GIS Web App to visualize all WQS segments around the station. 4.3.6.1 Application Operation: Manual WQS Adjustment (0 segments) Once the user has chosen an appropriate WQS segment, they must press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. The user cannot accept a single StationID that is linked to no WQS_IDs using the Accept button. The Manually Adjust WQS modal shows the information from the Selected Station Information table one last time for review. This will show information about the station but no WQS_ID. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.7 Application Operation: There are no more stations to attribute Congratulations! That means you have completed the review of all the stations in this subbasin. Make sure you press the Export reviews button one more time to ensure all your hard work is saved on the server. At this point, the user may navigate back to the Watershed selection tab to choose another subbasin to review or close the application to end their metadata review session. "],["having-problems.html", "4.4 Having problems?", " 4.4 Having problems? Please contact Emma Jones (emma.jones@deq.virginia.gov) should you encounter any problems using the application, wonky situations to review, or long rendering times/app crashing. "],["riverine-application-how-to.html", "Chapter 5 Riverine Application How To", " Chapter 5 Riverine Application How To The Riverine Assessment App is located on the R server. update link to 2024 when pushed "],["lacustrine-application-how-to.html", "Chapter 6 Lacustrine Application How To", " Chapter 6 Lacustrine Application How To The Lakes Assessment App is located on the R server. update link to 2024 when pushed "]]
