[["index.html", "DEQ Water Quality Automated Assessment User Guide Chapter 1 Introduction", " DEQ Water Quality Automated Assessment User Guide DEQ Automated Assessment Team Last Updated: 2023-02-08 Chapter 1 Introduction This manual serves as a companion document to DEQs Water Quality Assessment (WQA) automated assessment methodology for completion of the biennial 305b/303d Integrated Report. Virginia is among a handful of state agencies that have organized concerted efforts to systematically automate and expedite the water quality assessment process. These entities have all taken different approaches to best meet their state-specific analysis and reporting needs. Through the Tools for Automated Data Assessment (TADA) working group, EPA is organizing an effort to share code and create national tools to assist partners with assessment analyses. Considering the technological advances in water monitoring that have increased the quantity and types of data requiring consideration during an assessment period, as well as the static or decreasing staff time allocated to the assessment process, automation appears the only solution to keep up with Clean Water Act requirements and keep the public informed of the status of waterbody health and restoration timelines for those waterbodies not meeting water quality standards. By embracing automated assessment procedures, entities aim to provide constituents with higher quality, standardized, and transparent water quality assessment results while adhering to federally mandated deadlines. Virginia has taken a hybrid approach to the process of automating the water quality assessments. DEQ relies upon an automated process to manipulate hundreds of thousands of water quality data collected throughout the state during a given assessment period (six year window). These scripts evaluate each record for exceedances of appropriate Water Quality Standards. Regional assessment staff may use interactive web-based analytical tools provide more context to tabular results of exceedance analyses to assist with Quality Assurance/Quality Control (QA/QC) prior to submitting data to EPAs Assessment, Total Maximum Daily Load (TMDL) Tracking and Implementation System (ATTAINS) system through DEQs internal Comprehensive Environmental Data System (CEDS). This approach maximizes the benefits of each partner in the assessment process, allowing computers to excel at systematic and expedited data manipulation and analysis at scale and allowing humans to digest multiple lines of evidence to identify any potential data collection or analysis errors or areas of environmental concern. "],["HowToUseDocument.html", "1.1 How to Use Document", " 1.1 How to Use Document The following report is an agency effort to facilitate the further adoption of automated assessment methods statewide. The intention of this project is to provide a non-programmer audience with accessible and understandable narratives describing the automated water quality assessment process. This document is not a comprehensive WQA Guidance Manual, nor is it an introduction to using the R programming language. Here, we document the overall automated assessment process, explaining reasons certain decisions were made, how to unpack analyses, and where to seek further assistance through additional resources or interactive data exploration through web-based application interfaces. 1.1.1 Programming Expectations The Water Quality Assessment process is complicated. Learning a programming language is complicated. Combine these topics, the subject of this book, and we arrive at complicated2. The narrative script descriptions are written for understanding at an elementary programming level; however, the individual scripts utilize advanced programming techniques for parsimony, efficiency, and ease of long term maintenance. While it is not necessary to understand every element of code provided to run the automated assessment methods, certain programming themes listed below can be helpful: Tidyverse Rmarkdown Custom functions Optimizing list objects with purrr Pinned data The internally-published DEQ R Methods Encyclopedia is a good resource when getting started with R. These articles are useful to familiarize yourself: Getting Started With R How to Learn (or Continue Learning) R? Connecting to R Connect (for Pinned Data) The WQA Technical Support Team is available for assistance should you encounter any questions. "],["ProjectHistory.html", "1.2 Project History", " 1.2 Project History The agency began efforts toward automating components of the IR with a dissolved metals assessment written in SAS. These tabular results were provided to regional assessment staff along with raw data queries encompassing data stored in CEDS for each IR data window. The Freshwater Probabilistic Monitoring program began automating analyses and a final report for inclusion in the 2016 IR using the open source R programming language. These practices have been further refined each IR cycle, improving report quality and graphics as well as significantly reducing the amount of time required to generate a report after data collection. These methods were identified as a possible solution to the ongoing challenges in the WQA program to complete vast amounts of data analysis on increasingly short timelines with limited staff. An effort to systematically analyze all water quality data stored in CEDS for assessments began in 2017. As a pilot project, lakes and reservoirs in the Blue Ridge Regional Office (BRRO) assessment region were selected for first waterbody type to undergo automation and receive an interactive web-based tool to assist regional assessment staff for the 2018 IR. These initial automated assessment efforts were completed using the open source R programming language and interactive applications were built using the Shiny package. Rivers and streams followed suit for the 2020 IR, joining lacustrine waterbodies with automated assessment methods and interactive assessment tools. The 2020 IR automated methods were scaled from just the BRRO region to statewide applicability. A pilot project to incorporate citizen monitoring data requiring assessment was undertaken in the BRRO assessment region for the 2020 IR. This pilot proved effective in standardizing and increasing efficiency of incorporating this disparate data and was officially adopted as a process for future IR windows. After thorough QA from regional assessment staff across the state, the riverine and lacustrine automated assessment tools were rebooted for the 2022 IR with increased functionality and the ability to assess more parameters, including those not stored in CEDS. However, due to delays organizing citizen monitoring data statewide, automated results for these data were not provided for the 2022 IR. A new database schema for archiving station-specific water quality standards and assessment unit information was implemented. This system benefited the assessment process as well as numerous DEQ water programs that previously did not have access to WQS information at that spatial scale. Appendices and fact sheets for the IR were generated using R and Rmarkdown for the first time during the 2022 IR. The 2024 IR further refines improvements to the riverine and lacustrine automated assessment methods and interactive tools. By partnering with the Chesapeake Monitoring Cooperative (CMC), DEQ leveraged an existing public-facing citizen monitoring data portal to expand utility outside the Chesapeake Bay watershed and incorporate all of Virginias citizen monitoring data. These data are now automatically cleaned and stored by the CMC and can more easily be incorporated in the automated assessment process with web scraping techniques. To date, no estuarine-specific assessment methods have been completed, but the WQA program is investigating utility and potential adoption among regions with estuarine waters. "],["wqa-technical-support-team.html", "1.3 WQA Technical Support Team", " 1.3 WQA Technical Support Team Should you encounter any techical issues running the included scripts or have questions about the automated assessment methods described, please reach out to these technical staff for assistance: Emma Jones (emma.jones@deq.virginia.gov) Joe Famularo (joseph.famularo@deq.virginia.gov) Kristen Bretz (kristen.bretz@deq.virginia.gov) "],["Acknowledgements.html", "1.4 Acknowledgements", " 1.4 Acknowledgements Many Water Quality Assessment (WQA) and Water Quality Standards (WQA) staff have contributed to this effort: Emma Jones (emma.jones@deq.virginia.gov) Joe Famularo (joseph.famularo@deq.virginia.gov) Reid Downer (horace.downer@deq.virginia.gov) Kristen Bretz (kristen.bretz@deq.virginia.gov) Jason Hill (jason.hill@deq.virginia.gov) Sandy Mueller (sandra.mueller@deq.virginia.gov) Cleo Baker (cleo.baker@deq.virginia.gov) Amanda Shaver (amanda.shaver@deq.virginia.gov) Tish Robertson (tish.robertson@deq.virginia.gov) Paula Main (paula.main@deq.virginia.gov) Mary Dail (mary.dail@deq.virginia.gov) Martha Chapman (martha.chapman@deq.virginia.gov) Sara Jordan (sara.jordan@deq.virginia.gov) Kristie Britt (kristie.britt@deq.virginia.gov) Jennifer Palmore (jennifer.palmore@deq.virginia.gov) Kelley West (kelley.west@deq.virginia.gov) Rebecca Shoemaker (rebecca.shoemaker@deq.virginia.gov) Please direct any project questions to Emma Jones (emma.jones@deq.virginia.gov). "],["data-organization.html", "Chapter 2 Data Organization", " Chapter 2 Data Organization The assessment process requires hundreds of thousands of rows of data collected and stored by DEQ, other state agencies, and citizen partners. These disparate data sources require various levels of data manipulation prior to any analysis. SAS and R are used for the majority of the assessment data cleaning and manipulation processes. 2.0.0.1 Data Location Most data used for assessments are stored in DEQs internal Comprehensive Environmental Data System (CEDS) and made available through a direct connection to the reporting database known as ODS. The agency is working towards storing all data required for assessments in CEDS, but as of the time of writing the following datasets are stored in locations outside of CEDS: Fish Tissue Data PCB Data VDH Data Citizen Monitoring Data Station Metadata It is important to note that although the data that consitute the conventionals dataset are derive from CEDS/ODS, official assessment records of this data are only stored locally in Microsoft Excel outputs. 2.0.0.2 Data Availablility Most of the following data are provided at the beginning of the assessment process (approximately March of an assessment year). Any delays in data availability have ripple effects on the ability of regional assessors to complete their work on time. Should data be provided for assessment after the expected availability date, assessors may not be able to include said data in a given assessment window. Exceptions to the March data availability date usually apply to Citizen Monitoring, bioassessment, and fish tissue data. Citizen Monitoring data have historically been provided to regional assessment staff in April of a given assessment year. Bioassessment data for the most recent two years of an assessment window trickles in to the assessment process through the summer of a given assessment year due to the lengthy identification process associated with benthic macroinvertebrate samples. Fish tissue data requires protracted laboratory analyses before it is provided back to DEQ from contractor labs for assessment purposes. Due to these data delays, regional assessment staff generally have to delay certain assessment steps until all data is available for a given station/Assessment Unit, making automated assessment methods ever more important when data are provided. "],["conventionals-data.html", "2.1 Conventionals Data", " 2.1 Conventionals Data The conventionals dataset contains the bulk of the data analyzed during any given assessment window. Historically, this dataset has been queried using SAS and a direct connection to the raw monitoring data in the ODS reporting database (the database that makes CEDS data accessible to reporting tools). Recently, efforts to streamline the assessment process and standardize data provided across agency programs produced an effort to convert these SAS scripts into R. The conventionals query combines WQM field and analyte data with data handling steps that standardize data discrepancies like multiple lab values returned for identical sample date/times, full parameter speciation but no total value, etc. This R query and data standardization method is considered to be under development as code review is a constant part of data improvement strategies. The current version of the R based conventionals query is available here. In order to access the data the conventionals function calls, users must have a direct connection to ODS from their environment. Please see the DEQ R Methods Encyclopedia article on ODS for more information. Once the official conventionals dataset is provided for a given assessment window, it is stored on the R server as a pinned dataset to preserve a standardized data version (the data of record) in a centralized location accessible by all DEQ staff. This expedites the amount of time it takes to pull the data into an R environment as well as improves assessment application rendering time. An example conventionals dataset is available on the R server and may be retrieved using the script below. Please see the DEQ R Methods Encyclopedia article on pinned data sources for more infomation on how to connect your local system to the R server to query this data. This version of the dataset is used for feature enhancements and application development for the IR2024 cycle. However, this is not the official IR2024 conventionals dataset. conventionals &lt;- pin_get(&#39;ejones/conventionals2024draft&#39;, board = &#39;rsconnect&#39;) conventionals[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["water-column-metals.html", "2.2 Water Column Metals", " 2.2 Water Column Metals query from Roger, for now "],["sediment-metals.html", "2.3 Sediment Metals", " 2.3 Sediment Metals query from Roger, for now "],["fish-tissue-data.html", "2.4 Fish Tissue Data", " 2.4 Fish Tissue Data provided by Rick &amp; Gabe, for now "],["pcb-data.html", "2.5 PCB Data", " 2.5 PCB Data From Mark Richards, not in CEDS "],["vdh-data.html", "2.6 VDH Data", " 2.6 VDH Data Beach closure, HAB event "],["citizen-monitoring-data.html", "2.7 Citizen Monitoring Data", " 2.7 Citizen Monitoring Data Citizen Monitoring data have historically been provided to DEQ in various data formats and schema using numerous digital and analog storage methods. This data system required multiple iterations of lengthy data standardization and QA/QC processes in order to ensure the data were utilized for assessments. A standardized system requiring citizen groups to either upload their data to the Chesapeake Monitoring Cooperative (CMC) Data Explorer and DEQ scraping the CMC API using automated R scripts or provide all data to DEQ in a standardized template has replaced previous data receiving methods. Citizen data not stored in the CMC database live on DEQ staff computers and require further storage solutions. A sample citizen monitoring dataset is available in the exampleData directory for you to download and use when practicing the automated scripts. citmon &lt;- readRDS(&#39;exampleData/citmon.RDS&#39;) citmon[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["station-metadata.html", "2.8 Station Metadata", " 2.8 Station Metadata Station metadata are critical to ensuring that stations are assessed appropriately whether an assessment is conducted by hand or with automation. These metadata include the appropriate Assessment Unit and Water Quality Standard information that apply to the station. After this information is identified for a given station, it may be assessed according the the assessment rules that apply to those standards for the specified waterbody type. Metadata are provided for stations through a combination of automated spatial analyses and human review. No metadata are ever linked to stations without regional assessor review. The Metadata Attribution section covers the details of attributing metadata to each station. "],["low-flow-7q10-data.html", "2.9 Low Flow (7Q10) Data", " 2.9 Low Flow (7Q10) Data The 2024 IR is the first assessment window to use a standardized low flow analysis process. Information presented below is still under review by regional assessment staff. The workflow first analyzes a 7Q10 low flow statistic for all available gages in VA based on the last 50 years of water data. The functions called to analyze the xQy flow statistics are identical to DEQs Water Permitting protocols and were written by Connor Brogan. The water data is provided by the USGS NWIS data repository and is called in the xQy function by the USGS dataRetreival package. Important assumptions of the xQy program are identified below. Once flow statistics are generated for all available gages statewide, this information is compared to available flow data during a given assessment window. Any gages identified below the 7Q10 statistic are flagged for the appropriate time period. This information is spatially joined to a major river basin layer to extrapolate available flow data to areas without gaging stations. This is not an ideal extrapolation of flow data, but it serves as a decent initial flag for assessors to know when/where to investigate further. These temporal low flow flags are joined to individual site monitoring data by major river basin during the automated assessment process. If parameters used to assess aquatic life condition are collected during low flow periods, then the data are flagged inside the assessment applications, indicating further review is necessary prior to accepting the automated assessment exceedance calculations for that site. 2.9.1 7Q10 Method The method for identifying low flow information for the assessment period is detailed below. The DFLOW_CoreFunctions_EVJ.R script is an assessment-specific adaptation of Connor Brogans xQy protocols that allow for minor adjustments to the DFLOW procedure to allow for assessment purposes (for more information on these changes, see the Important 7Q10 Calculation Notes section below. This analysis needs to be performed on or after April 2 of each assessment window cutoff to ensure the entire final water year is included in the analysis. The results are posted on the R server for inclusion in the automated assessment methods. library(tidyverse) library(zoo) library(dataRetrieval) library(e1071) library(sf) library(leaflet) library(inlmisc) library(DT) source(&#39;DFLOW_CoreFunctions_EVJ.R&#39;) 2.9.2 USGS Site Data Gathering All USGS gage information sampled in the last 50 years need to be collected from USGS NWIS. We can use the whatNWISsites() function to identify which sites have daily discharge data (00060) in a designated area (stateCd = VA). sites &lt;- whatNWISsites(stateCd=&quot;VA&quot;, parameterCd=&quot;00060&quot;, hasDataTypeCd=&quot;dv&quot;) %&gt;% filter(site_tp_cd %in% c(&#39;ST&#39;, &#39;SP&#39;)) # only keep ST (stream) and SP (spring) sites sites_sf &lt;- sites %&gt;% st_as_sf(coords = c(&quot;dec_long_va&quot;, &quot;dec_lat_va&quot;), remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng Now we will pull daily flow data for each site identified and calculate 7Q10. This is saved in the local environment as a list object with each gage a unique list element. # store it somewhere flowAnalysis &lt;- list() for(i in unique(sites$site_no)){ print(i) siteFlow &lt;- xQy_EVJ(gageID = i,#USGS Gage ID DS=&quot;1972-03-31&quot;,#Date to limit the lower end of usgs gage data download in yyyy-mm-dd DE=&quot;2023-04-01&quot;,#Date to limit the upper end of USGS gage data download in yyyy-mm-dd WYS=&quot;04-01&quot;,#The start of the analysis season in mm-dd. Defaults to April 1. WYE=&quot;03-31&quot;,#The end of the analysis season in mm-dd. Defaults to March 31. x=7,#If you want to include a different xQY then the defaults, enter x here y=10, onlyUseAcceptedData = F ) flowAnalysis[i] &lt;- list(siteFlow) } Using the purrr library, we can extract just the flow metric information for each gage and store in a tibble for use later. # extract 7Q10 by gageNo x7Q10 &lt;- map_df(flowAnalysis, &quot;Flows&quot;) # EVJ added in gageNo to xQy_EVJ() x7Q10[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) And we need the actual daily flow data to compare to the low flow metrics, so we will extract that next. # now to extract flow data already pulled by function flows &lt;- map_df(flowAnalysis, &quot;outdat&quot;) flows[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Since we only really care about flow data from our assessment window, lets extract just the flow data and filter to our IR window of interest. We can then join the low flow metrics by gage number and flag any daily average flow data that falls below the gages 7Q10 metric. # now just grab flow data in assessment window, join in 7Q10, identify any measures below 7Q10 assessmentFlows &lt;- map_df(flowAnalysis, &quot;outdat&quot;) %&gt;% filter(between(Date, as.Date(&quot;2017-01-01&quot;), as.Date(&quot;2022-12-31&quot;))) %&gt;% left_join(x7Q10, by = c(&#39;Gage ID&#39; = &quot;gageNo&quot;)) %&gt;% mutate(`7Q10 Flag` = case_when(Flow &lt;= n7Q10 ~ &#39;7Q10 Flag&#39;, TRUE ~ as.character(NA))) assessmentFlows[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Here we limit our assessmentFlows object to just the rows where a 7Q10 flag is encountered. We can review these low flow events by organizing them by gage and date. # anything below 7Q10? lowAssessmentFlows &lt;- filter(assessmentFlows, `7Q10 Flag` == &#39;7Q10 Flag&#39;) unique(lowAssessmentFlows$`Gage ID`) # what gages do these occur at? # organize low flow events by Gage ID lowAssessmentFlows %&gt;% arrange(`Gage ID`, Date))[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Next, lets review the low flow gages visually on a map. First, we need to transform this low flow information into a spatial object. # see where spatially lowFlowSites &lt;- lowAssessmentFlows %&gt;% distinct(`Gage ID`) %&gt;% left_join(sites_sf, by = c(&#39;Gage ID&#39; = &#39;site_no&#39;)) %&gt;% st_as_sf() We will bring in major river subbasins to better understand how these low flow events happen across the landscape. basins &lt;- st_as_sf(pin_get(&quot;ejones/DEQ_VAHUSB_subbasins_EVJ&quot;, board = &quot;rsconnect&quot;)) %&gt;% group_by(BASIN_CODE, BASIN_NAME) %&gt;% summarise() And here is a map of the major river subbasins with all Virginia USGS gages (USGS sites) and just USGS gages with low flow events in the IR window (Low Flow USGS sites). CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE, options= leafletOptions(zoomControl = TRUE,minZoom = 5, maxZoom = 20, preferCanvas = TRUE)) %&gt;% setView(-79.1, 37.7, zoom=7) %&gt;% addPolygons(data= basins, color = &#39;black&#39;, weight = 1, fillColor= &#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Major River SubBasins&quot;, label = ~BASIN_NAME) %&gt;% addCircleMarkers(data = sites_sf, color=&#39;gray&#39;, fillColor=&#39;gray&#39;, radius = 4, fillOpacity = 0.8,opacity=0.8,weight = 2,stroke=T, group=&quot;USGS sites&quot;, label = ~site_no) %&gt;% addCircleMarkers(data = lowFlowSites, color=&#39;gray&#39;, fillColor=&#39;red&#39;, radius = 4, fillOpacity = 0.8,opacity=0.8,weight = 2,stroke=T, group=&quot;Low Flow USGS sites&quot;, label = ~`Gage ID`) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&quot;Low Flow USGS sites&quot;,&quot;USGS sites&quot;,&quot;Major River SubBasins&quot;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) Now we need to join the watershed information to the low flow analysis so we can easily incorporate this information to all monitoring sites that fall in the watershed. lowFlowSitesHUC &lt;- st_intersection(lowFlowSites, basins) %&gt;% st_drop_geometry() %&gt;% # for this analysis we don&#39;t actually need the spatial information dplyr::select(`Gage ID` = `Gage.ID`, # spatial joins change tibble names, changing back to name we want agency_cd:dec_long_va, BASIN_CODE, BASIN_NAME) lowAssessmentFlows &lt;- left_join(lowAssessmentFlows, lowFlowSitesHUC, by = &#39;Gage ID&#39;) %&gt;% dplyr::select(Agency, `Gage ID`, `Station Name` = station_nm, `Site Type` = site_tp_cd, Latitude = dec_lat_va, Longitude = dec_long_va, Date:Status, `Water Year` = WY, n7Q10, `7Q10 Flag`,BASIN_CODE, BASIN_NAME) This information is pinned to the R server so we can use it during the automated assessment process. pin_get(&#39;ejones/AssessmentWindowLowFlows&#39;, &#39;rsconnect&#39;)[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) 2.9.3 Important 7Q10 Calculation Notes Information from the chief flow statistic programmer (Connor Brogan) about the assumptions of the 7Q10 function used in this analysis: Desired exceedance probability evaluation in check.fs() must be between 0 and 1 Only complete analysis years are included for calculation of both the Harmonic Mean and all low flows. Years with minimum flow of 0 are removed, but compensated for via USGS probability adjustment Must have at least 2 analysis years of complete data (line 133) to calculate xQy flows (not necessary for Harmonic Mean) Provisional gage flow data is removed Negative flows (e.g. tidally reversed) are treated as NA following the USGS SW Toolbox Function only uses gage data where the water year is within the range of the years of DS and DE The gage data is filtered to only include data after the first date of the first analysis season and before the last date of the last analysis season. For instance, if WYS = 04-01 and WYE = 03-31 and DS and DE were 1972 to 2022, then the gage data would be limited to the dates between and including 1972-04-01 and 2022-03-31 The start and end dates of the data must include one at least one instance of WYE The last few days in the analysis season are removed to ensure statistical independence between years Analysis season must have sufficient days to calculate a minimum flow such that at least 7-days are required to calculate a 7-day flow Based on the changes Emma Jones made to the original function for Water Quality Assessment purposes, here are important assumptions to know (numbered according to system above): Must have at least 10 analysis years of complete data to calculate xQy flows Provisional gage flow data is accepted. This is important so water years can be calculated as soon as possible for assessment purposes. Waiting until all data are approved will result in too little time for assessors to review the data. Storm events usually are corrected in the provisional to accepted stage in the USGS QA process, so since we are interested in low flow events, this is not a major concern when using provisional data. "],["automated-assessment-methods.html", "Chapter 3 Automated Assessment Methods", " Chapter 3 Automated Assessment Methods This chapter details the specific steps required to transform the raw data outlined in the Data Organization chapter into preliminary assessment results. The general process the automated assessment specifies begins with performing data manipulation and organization steps to prepare the data for analysis, then apply specific assessment functions to the appropriate data that specify the assessment guidance and Water Quality Standards (WQS), and finally, output a stations table that uses data from the assessment window to summarize each station for review and bulk upload into CEDS. See the Automated Output section to understand the information provided in the stations table output. The automated assessment scripts are trained to systematically apply assessment guidance and WQS to all data for which the appropriate metadata is provided. This supervised system merely applies the rules that developers have programmed to mimic assessment protocols. These scripts do not provide assessment decisions. It is up to the human reviewer (regional assessor) to either accept the automated result or use best professional judgment to e.g. redact questionable data or interpret complex natural systems. "],["metadata-attribution.html", "3.1 Metadata Attribution", " 3.1 Metadata Attribution The automated assessment process hinges on each station having the appropriate metadata to analyze all raw data. The required metadata for each station include what Water Quality Standards apply to the station and which Assessment Unit(s) describe the station. One or more station can be included in an Assessment Unit. It is ultimately the Assessment Units that are used to determine whether or not designated uses are met, which is where the automation stops and the human analysis component is required. In practice, metadata are spatially joined to stations by a rigorous data organization, spatial joining, and QA process that is detailed below. This automated process runs on the Assessment Data Analysts computer and results are provided to regional assessment staff for individual review through a shared shiny application. This application is known as the Regional Metadata Validation Tool and is hosted on the R server. It is up to each regional assessor to manually review each suggested metadata link prior to the assessment start date. After stations have completed the manual review process, they can be analyzed using the automated assessment scripts. Detailed instructions on how to use the Regional Metadata Validation Tool is available in the Regional Metadata Validation Tool How To section. 3.1.1 Distinct Sites Before station metadata can be linked to stations, a list of unique stations that were sampled in a given assessment window is required. Because multiple data sources are combined for an assessment, each unique station from each data source with data in the given IR window are included in this list. For the purposes of the Automated Assessment User Guide, we will overview the process with a snippet of conventionals and citizen monitoring data types. Please see the official script for more information. library(tidyverse) library(sf) library(pins) library(config) library(lubridate) # Connect to server conn &lt;- config::get(&quot;connectionSettings&quot;) # get configuration settings board_register_rsconnect(key = conn$CONNECT_API_KEY, #Sys.getenv(&quot;CONNECT_API_KEY&quot;), server = conn$CONNECT_SERVER)#Sys.getenv(&quot;CONNECT_SERVER&quot;)) After setting up your local evironment with necessar packages and connecting to the R server, bring conventionals data into your environment. conventionals &lt;- pin_get(&#39;ejones/conventionals2024draft&#39;, board = &#39;rsconnect&#39;) %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% dplyr::select(FDT_STA_ID, Latitude, Longitude) %&gt;% mutate(Data_Source = &#39;DEQ&#39;) conventionals[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Next, bring citizen data into your environment. citmon &lt;- readRDS(&#39;exampleData/citmon.RDS&#39;) %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% dplyr::select(FDT_STA_ID, Latitude, Longitude, Data_Source) citmon[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Create an object of unique sites from these two datasets. distinctSites &lt;- bind_rows(conventionals, citmon) distinctSites[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) This process is repeated for all datasets that have data included in the assessment window. These multiple datasets are combined into a single object named distinctSites that we will then compare to existing WQS and AU information. Where stations in our distinctSites object lack either of these pieces of metadata, we must attribute them. 3.1.2 Join Assessment Region and Subbasin Information For rendering purposes in the metadata review application, it is important to have each station linked to the appropriate Assessment Region and Subbasin to limit the amount of data called into the application just to essential information. This information is also important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing. The next chunk reads in the necessary assessment region and subbasin spatial data and spatially joins all sites to these layers. If the sites are missing from the distinctSites_sf object, that means the point plots outside either the assessment region or subbasin polygon. These missingSites are dealt with individually and forced to join to the nearest assessment region and subbasin before rejoining the distinctSites_sf object. assessmentLayer &lt;- st_read(&#39;GIS/AssessmentRegions_VA84_basins.shp&#39;) %&gt;% st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection subbasinLayer &lt;- st_read(&#39;GIS/DEQ_VAHUSB_subbasins_EVJ.shp&#39;) %&gt;% rename(&#39;SUBBASIN&#39; = &#39;SUBBASIN_1&#39;) distinctSites_sf &lt;- st_as_sf(distinctSites, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) %&gt;% # add coordinate reference system, needs to be geographic for now bc entering lat/lng, st_intersection(assessmentLayer ) %&gt;% st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) # if any joining issues occur, that means that there are stations that fall outside the joined polygon area # we need to go back in and fix them manually if(nrow(distinctSites_sf) &lt; nrow(distinctSites)){ missingSites &lt;- filter(distinctSites, ! FDT_STA_ID %in% distinctSites_sf$FDT_STA_ID) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) closest &lt;- mutate(assessmentLayer[0,], FDT_STA_ID =NA) %&gt;% dplyr::select(FDT_STA_ID, everything()) for(i in seq_len(nrow(missingSites))){ closest[i,] &lt;- assessmentLayer[which.min(st_distance(assessmentLayer, missingSites[i,])),] %&gt;% mutate(FDT_STA_ID = missingSites[i,]$FDT_STA_ID) %&gt;% dplyr::select(FDT_STA_ID, everything()) } missingSites &lt;- left_join(missingSites, closest %&gt;% st_drop_geometry(), by = &#39;FDT_STA_ID&#39;) %&gt;% st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) %&gt;% dplyr::select(-c(geometry), geometry) %&gt;% dplyr::select(names(distinctSites_sf)) distinctSites_sf &lt;- rbind(distinctSites_sf, missingSites) } 3.1.3 Join Stations to WQS All stations are then spatially joined to the current WQS spatial layers to link each unique StationID to a unique WQS_ID. These unique WQS_IDs are a concatination of the waterbody type, basin code (e.g. 2A, 2B, 2C, etc.), and a number associated with each segment in the spatial layer. Waterbody types include: RL = Riverine Line LP = Lacustrine Polygon EL = Estuarine Line EP = Estuarine Polygon Since transitioning the WQS storage from local (individual assessor files) to a centralized system (on the R server for multiple program uses), the number of stations that require WQS attribution decreases significantly each IR cycle. To attribute each station to WQS information, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can send that information to an interactive application for humans to manually verify. Where WQS_ID information is already available for stations, they are first removed from the WQS snapping process as to not repeat efforts unnecessarily. WQS_IDs are maintained across WQS layer updates, so any new WQS information will be updated when the stations are joined to the WQS metadata. You can view the available WQSlookup table stored on the server by using the below script. Please see the DEQ R Methods Encyclopedia for information on how to retrieve pinned data from the server. WQStableExisting &lt;- pin_get(&#39;ejones/WQSlookup&#39;, board = &#39;rsconnect&#39;) distinctSitesToDoWQS &lt;- filter(distinctSites_sf, ! FDT_STA_ID %in% WQStableExisting$StationID) Here is the table used to store link information from stations to appropriate WQS. WQStable &lt;- tibble(StationID = NA, WQS_ID = NA) 3.1.3.1 Spatially Join WQS Polygons Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first. 3.1.3.1.0.1 Estuarine Polygons (WQS) Here we find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas. This process also removes any estuarine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS). We will bring in (source) a custom function that runs the analysis for us, feed it our latest WQS information, and let the function run across all input stations. You can see the latest version of the sourced script in this repository. source(&#39;preprocessingModules/WQS_estuaryPoly.R&#39;) # Bring in estuary layer estuarinePolys &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_transform(4326) WQStable &lt;- estuaryPolygonJoin(estuarinePolys, distinctSitesToDoWQS, WQStable) rm(estuarinePolys) # clean up workspace Remove stations that fell inside estuarine polygons from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) 3.1.3.1.1 Lake Polygons (WQS) Next find any sites that fall into a lake WQS polygon. This method is applied to all subbasins at once as it is a simple spatial join. You can see the latest version of the sourced script in this repository. source(&#39;preprocessingModules/WQS_lakePoly.R&#39;) lakesPoly &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_transform(4326) WQStable &lt;- lakePolygonJoin(lakesPoly, distinctSitesToDoWQS, WQStable) rm(lakesPoly) # clean up workspace Remove stations that fell inside lake polygons from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) 3.1.3.2 Spatially Join WQS Lines Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS and where stations remain we will try the estuarine WQS snap. 3.1.3.2.1 Riverine Lines (WQS) To do this join, we will buffer all sites that dont fall into a polygon layer by a set sequence of distances. The output will add a field called Buffer Distance to the WQStable to indicate distance required for snapping. If more than one segment is found within a set buffer distance, then many rows will be attached to the WQStable with the single identifying station name. It is up to the QA tool to help the user determine which of these UIDs are correct and drop the other records. We then remove any riverine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS). source(&#39;snappingFunctions/snapWQS.R&#39;) riverine &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) WQStable &lt;- snapAndOrganizeWQS(distinctSitesToDoWQS, &#39;FDT_STA_ID&#39;, riverine, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters WQStable) rm(riverine) # clean up workspace Remove stations that attached to riverine segments from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) We can use this one last opportunity to test stations that didnt connect to the riverine WQS against the estuarine lines WQS as a one last hope of attributing some WQS information. We will take all stations from the WQStable that didnt snap to any WQS segments (Buffer Distance ==No connections within 80 m) and add those back in to our distinctSitesToDoWQS list to try to snap them to the estuarine lines spatial data. distinctSitesToDoWQS &lt;- filter(WQStable, `Buffer Distance` ==&#39;No connections within 80 m&#39;) %&gt;% left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %&gt;% rename(&#39;StationID&#39;= &#39;FDT_STA_ID&#39;), by=&#39;StationID&#39;) %&gt;% dplyr::select(-c(geometry)) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = T, # don&#39;t remove these lat/lon cols from df crs = 4326) 3.1.3.2.2 Estuarine Lines (WQS) If a site doesnt attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time. Removes any estuary lines sites from the data frame of unique sites that need WQS information. source(&#39;snappingFunctions/snapWQS.R&#39;) estuarineLines &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp&#39; , fid_column_name = &quot;OBJECTID&quot;) #%&gt;% #st_transform(102003) # forcing to albers from start bc such a huge layer #st_transform(4326) # Only send sites to function that could be in estuarine environment WQStable &lt;- snapAndOrganizeWQS(filter(distinctSitesToDoWQS, SUBBASIN %in% c(&quot;Potomac River&quot;, &quot;Rappahannock River&quot;, &quot;Atlantic Ocean Coastal&quot;, &quot;Chesapeake Bay Tributaries&quot;, &quot;Chesapeake Bay - Mainstem&quot;, &quot;James River - Lower&quot;, &quot;Appomattox River&quot;, &quot;Chowan River&quot;, &quot;Atlantic Ocean - South&quot; , &quot;Dismal Swamp/Albemarle Sound&quot;)), &#39;StationID&#39;, estuarineLines, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters WQStable) rm(estuarineLines) Remove stations that attached to estuarine segments from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! StationID %in% WQStable$StationID) Double check no stations were lost in these processes. #Make sure all stations from original distinct station list have some sort of record (blank or populated) int the WQStable. distinctSitesToDoWQS &lt;- filter(distinctSitesToDo, ! FDT_STA_ID %in% WQStableExisting$StationID) distinctSitesToDoWQS$FDT_STA_ID[!(distinctSitesToDoWQS$FDT_STA_ID %in% unique(WQStable$StationID))] 3.1.3.3 Assign something to WQS_ID so sites will not fall through the cracks when application filtering occurs We dont want to give all the assessors statewide all the stations that didnt snap to something, so we need to at least partially assign a WQS_ID so the stations get into the correct subbasin on initial filter. If a station snapped to nothing, we will assigning it a RL WQS_ID and subbasin it falls into by default. WQStableMissing &lt;- filter(WQStable, is.na(WQS_ID)) %&gt;% # drop from list if actually fixed by snap to another segment filter(! StationID %in% filter(WQStable, str_extract(WQS_ID, &quot;^.{2}&quot;) %in% c(&#39;EL&#39;,&#39;LP&#39;,&#39;EP&#39;))$StationID) %&gt;% left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, BASIN_CODE) %&gt;% st_drop_geometry(), by = c(&#39;StationID&#39; = &#39;FDT_STA_ID&#39;)) %&gt;% # some fixes for missing basin codes so they will match proper naming conventions for filtering mutate(BASIN_CODE1 = case_when(is.na(BASIN_CODE) ~ str_pad( ifelse(grepl(&#39;-&#39;, str_extract(StationID, &quot;^.{2}&quot;)), str_extract(StationID, &quot;^.{1}&quot;), str_extract(StationID, &quot;^.{2}&quot;)), width = 2, side = &#39;left&#39;, pad = &#39;0&#39;), TRUE ~ as.character(BASIN_CODE)), BASIN_CODE2 = str_pad(BASIN_CODE1, width = 2, side = &#39;left&#39;, pad = &#39;0&#39;), WQS_ID = paste0(&#39;RL_&#39;, BASIN_CODE2,&#39;_NA&#39;)) %&gt;% dplyr::select(-c(BASIN_CODE, BASIN_CODE1, BASIN_CODE2)) WQStable &lt;- filter(WQStable, !is.na(WQS_ID)) %&gt;% filter(! StationID %in% WQStableMissing$StationID) %&gt;% bind_rows(WQStableMissing) 3.1.4 Join Stations to AUs Assessment Units (AU) are different from WQS attribution steps because these links can change cycle to cycle as new AUs are broken off from existing AUs to more appropriately represent . Thus, a single record of all AUs linked to StationIDs like the WQSlookup table is not a good solution for this use case. Instead the AU to StationID link is stored in a more temporary format (Station Table Excel file) such that regional assessment staff can easily update the AU information on the fly as they assess stations. Generally, the logic holds that the provided AUs are a starting point for an upcoming assessment cycle, not necessarily the final AU for said assessment cycle. The process to link unique stations in an assessment window (distinctSites_sf) to AU information begins by joining all sites in an upcoming window to the Stations Table from the most recent assessment cycle. If a station does not join to a previous assessment cycle Stations Table that means that the station either has not been sample before (e.g. a new station for Probabilistic Monitoring or a special study) or has not been sampled in a long time such that is outside the last assessment window and has not been carried over from a previous assessment cycle for any reason. It is these sites that did not join to the last cycles Stations Table where we will focus our efforts in the subsequent spatial joining steps. 3.1.4.1 Identify which stations need AU information First, bring in the final Stations Table from the most recently completed IR cycle. For this example, we are sourcing the IR2022 final Stations Table (presented as a spatial object in a file geodatabase but we will strip off the spatial data first thing since it is unnecessary for this purpose). We are also going to rename the friendly publication field names to a more standardized format that the automated assessment functions were built upon (read: we are changing field names to match previous versions of the Stations Table schema since the assessment functions were built on that data schema). final2022 &lt;- st_read(&#39;C:/HardDriveBackup/GIS/Assessment/2022IR_final/2022IR_GISData/va_ir22_wqms.gdb&#39;, layer = &#39;va_ir22_wqms&#39;) %&gt;% st_drop_geometry() %&gt;% # only need tabular data from here out # change names of ID305B columns to format required by automated methods rename(ID305B_1 = Assessment_Unit_ID_1, TYPE_1 = Station_Type_1, ID305B_2 = Assessment_Unit_ID_2, TYPE_2 = Station_Type_2, ID305B_3 = Assessment_Unit_ID_3, TYPE_3 = Station_Type_3, ID305B_4 = Assessment_Unit_ID_4, TYPE_4 = Station_Type_4, ID305B_5 = Assessment_Unit_ID_5, TYPE_5 = Station_Type_5, ID305B_6 = Assessment_Unit_ID_6, TYPE_6 = Station_Type_6, ID305B_7 = Assessment_Unit_ID_7, TYPE_7 = Station_Type_7, ID305B_8 = Assessment_Unit_ID_8, TYPE_8 = Station_Type_8, ID305B_9 = Assessment_Unit_ID_9, TYPE_9 = Station_Type_9, ID305B_10 = Assessment_Unit_ID_10, TYPE_10 = Station_Type_10) Now we will join distinct sites to AU information to get all available data to start the assessment process. Note: Assessors may attribute stations to different VAHU6s compared to strictly where the site is located spatially to communicate that said stations (usually that lie close to VAHU6 border) are used to make assessment decisions about the designated VAHU6. For this reason, we use the VAHU6 designation from the previous assessment cycle over the VAHU6 retrieved from CEDS. If the station does not have a record in the previous assessment cycle Stations Table, the VAHU6 designation stored in CEDS is used. The last rows of the below chunk ensure that each station is only listed once in the resultant table. In previous assessment cycles, stations could be assessed for multiple waterbody types (e.g. riverine and lacustrine assessment uses). Since the assessment database was moved from MS Access to CEDS WQA, this duplication is no longer allowed and thus each station should only have one record. distinctSites_AUall &lt;- distinctSites_sf %&gt;% st_drop_geometry() %&gt;% left_join(final2022 %&gt;% dplyr::select(-c(Latitude, Longitude)), # drop duplicate lat/lng fields to avoid join issues by = c(&#39;FDT_STA_ID&#39; = &#39;Station_ID&#39;)) %&gt;% dplyr::select(FDT_STA_ID : VAHU6.y) %&gt;% # drop the last cycle&#39;s results, not important now mutate(VAHU6 = ifelse(is.na(VAHU6.y), as.character(VAHU6.x), as.character(VAHU6.y))) %&gt;% # use last cycle&#39;s VAHU6 designation over CEDS designation by default if available dplyr::select(-c(VAHU6.x, VAHU6.y)) %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n = n()) %&gt;% ungroup() # Find any duplicates View(filter(distinctSites_AUall, n &gt;1)) # 0 rows, cool # above n&gt; 1 used to be stations that were riverine and lacustrine makes sense, these sites are being used for riverine and lacustrine assessment # for IR2024 this should all be cleaned up bc new WQA CEDS rules, but always good to double check Next we will organize stations by whether or not they have AU data based on the previous join. We will call the stations that need AU information distinctSites_AUtoDo. We only test this using the ID305B_1 column because we only need each station to be attributed to at least one AU. distinctSites_AUtoDo &lt;- filter(distinctSites_AUall, is.na(ID305B_1)) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # These sites are all good for automated assessment (once we join WQS info from WQSlookup table) distinctSites_AU &lt;- filter(distinctSites_AUall, !is.na(ID305B_1)) # Quick QA: double check the math works out nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) As with the WQS spatial attribution process, we will first join these sites to polygon layers and then line features to minimize any unnecessary computational load. 3.1.4.2 Spatially Join AU Polygons Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first. 3.1.4.2.1 Estuarine Polygons (AU) We will now find any sites that fall into an estuary AU polygon. This method is only applied to subbasins that intersect estuarine areas and then removes any estuarine sites from the data frame of unique sites that need AU information (distinctSites_AUtoDo). The chunk below sources a custom function for handling AU polygon information available for download here. source(&#39;preprocessingModules/AU_Poly.R&#39;) # Bring in estuary layer estuaryPolysAU &lt;- st_read(&#39;../va_aus_estuarine.shp&#39;) %&gt;% st_transform( 4326 ) %&gt;% st_cast(&quot;MULTIPOLYGON&quot;) # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427 estuaryPolysAUjoin &lt;- polygonJoinAU(estuaryPolysAU, distinctSites_AUtoDo, estuaryTorF = T) %&gt;% mutate(ID305B_1 = ID305B) %&gt;% dplyr::select(names(distinctSites_AU)) %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n = n(), `Buffer Distance` = &#39;In polygon&#39;) %&gt;% ungroup() rm(estuaryPolysAU) # clean up workspace Now we add the newly identified estuary stations to distinctSites_AU. distinctSites_AU &lt;- bind_rows(distinctSites_AU, estuaryPolysAUjoin %&gt;% st_drop_geometry()) And then remove stations that fell inside estuarine polygons from our to do list. distinctSites_AUtoDo &lt;- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% estuaryPolysAUjoin$FDT_STA_ID) nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) 3.1.4.2.2 Lake Polygons (AU) Next we identify any sites that fall into a lake AU polygon. This method is applied to all subbasins, unlike the previous estuary step. We then remove any lake sites from the data frame of unique sites that need AU information. The chunk below sources the same custom function for handling AU polygon information as above available for download here. source(&#39;preprocessingModules/AU_Poly.R&#39;) # Bring in Lakes layer lakesPolyAU &lt;- st_read(&#39;../va_aus_reservoir.shp&#39;) %&gt;% st_transform( 4326 ) %&gt;% st_cast(&quot;MULTIPOLYGON&quot;) # special step for weird WKB error reading in geodatabase, never encountered before, fix from: https://github.com/r-spatial/sf/issues/427 lakesPolysAUjoin &lt;- polygonJoinAU(lakesPolyAU, distinctSites_AUtoDo, estuaryTorF = F)%&gt;% mutate(ID305B_1 = ID305B, `Buffer Distance` = &#39;In polygon&#39;) %&gt;% dplyr::select(names(distinctSites_AU)) %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n = n()) %&gt;% ungroup() rm(lakesPolyAU) # clean up workspace Now add the lake stations to distinctSites_AU. distinctSites_AU &lt;- bind_rows(distinctSites_AU, lakesPolysAUjoin %&gt;% st_drop_geometry()) And we remove stations that fell inside lake polygons from the to do list. This is also a good time to double check that we havent lost any sites in the above process. distinctSites_AUtoDo &lt;- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% lakesPolysAUjoin$FDT_STA_ID) # Double check all sites are still there nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) rm(lakesPolysAUjoin);rm(estuaryPolysAUjoin) # clean up workspace 3.1.4.3 Spatially Join AU Lines Now on to the more computationally heavy AU line snapping methods. We can only try to attach riverine AUs since there is not a published estuarine lines AU spatial layer (like there is for WQS information). All sites that do not snap to a riverine line within a set buffer distance will be flagged with comment saying so and the regional assessors will sort out the most appropriate AU manually in the Regional Metadata Validation Tool. 3.1.4.3.1 Riverine Lines (AU) Time to run all the sites that didnt fall into an AU polygon through our polyline snapping buffering script. This function is slightly different from the WQS buffering function in that it is flexible enough link any chosen fields from the two input arguments. The output of the function will add a field called Buffer Distance to distinctSites_AU to indicate the distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, that many rows will be attached to the snapTable with the identifying station name. It is up to the Regional Metadata Validation Tool to help the user determine which of these AUs are correct and subsequently drop all other records. source(&#39;snappingFunctions/snapPointToStreamNetwork.R&#39;) riverineAU &lt;- st_read(&#39;../va_aus_riverine.shp&#39;) %&gt;% st_transform(102003) snapTable &lt;- snapAndOrganize(distinctSites_AUtoDo, &#39;FDT_STA_ID&#39;, riverineAU, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters tibble(StationID = character(), ID305B = character(), `Buffer Distance` = character()), &quot;ID305B&quot;) snapTable &lt;- snapTable %&gt;% left_join(distinctSites_AUtoDo, by = c(&#39;StationID&#39; = &#39;FDT_STA_ID&#39;)) %&gt;% # get station information rename(&#39;FDT_STA_ID&#39; = &#39;StationID&#39;) %&gt;% mutate(ID305B_1 = ID305B) %&gt;% dplyr::select(names(distinctSites_AU), `Buffer Distance`) %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n = n()) %&gt;% ungroup() rm(riverineAU) # clean up workspace Now we can add these sites to the sites with AU information. distinctSites_AU &lt;- bind_rows(distinctSites_AU , snapTable ) And then remove stations that attached to riverine segments from the to do list. distinctSites_AUtoDo &lt;- filter(distinctSites_AUtoDo, ! FDT_STA_ID %in% snapTable$FDT_STA_ID) We dont have estuarine lines AU information, so the sites that dont connect to any AUs at the max buffer distance will have to be sorted out by the assessors. distinctSites_AU &lt;- distinctSites_AU %&gt;% group_by(FDT_STA_ID) %&gt;% mutate(n=n()) We then make sure all stations from original distinct station list (distinctSites_AUall) have some sort of record (blank or populated) in the distinctSites_AU dataset. # check everyone dealt with nrow(distinctSites_AU) + nrow(distinctSites_AUtoDo) == nrow(distinctSites_AUall) distinctSites_AU$FDT_STA_ID[!(distinctSites_sf$FDT_STA_ID %in% unique(distinctSites_AU$FDT_STA_ID))] if(nrow(distinctSites_AUtoDo) == 0){rm(distinctSites_AUtoDo)} # clean up workspace if QA check good One last step to make sure buffer distances save correctly for future mapping needs. unique(distinctSites_AU$`Buffer Distance`) distinctSites_AU$`Buffer Distance` &lt;- as.character(distinctSites_AU$`Buffer Distance`) So, what does the end result look like? distinctSites_AU[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) 3.1.5 Where does this information go? The above analyses help populate the Regional Metadata Validation Tool. After assessors manually review each station that requires WQS/AU information, this data is consolidated and stored on the R server as a pinned dataset (see for details on that process). That dataset feeds subsequent tools including the Riverine Assessment App, Lakes Assessment App, and Bioassessment Dashboard in addition to querying tools including the CEDS WQM Data Query Tool and CEDS Benthic Data Query Tool. The data is provided via the internal GIS services in the WQM Stations (All stations with full attributes) layer hosted on the GIS REST service and GIS Staff Application. In addition to the published tools that source this data, the dataset is included in countless data analysis projects and products. The entire dataset may be pulled from the R Connect service using the following script. Please see the DEQ R Methods Encyclopedia for information on how to retrieve pinned data from the server. pin_get(&#39;ejones/WQSlookup&#39;, board = &#39;rsconnect&#39;) # raw lookup table pin_get(&quot;ejones/WQSlookup-withStandards&quot;, board = &quot;rsconnect&quot;) # lookup table with WQS information joined "],["organize-metadata.html", "3.2 Organize Metadata", " 3.2 Organize Metadata After all stations for a given assessment window have the appropriate WQS and AU information attributed, there are a number of data organization steps that still need to happen before the data are ready for the automated assessment scripts. These steps include: Reorganize the conventionals dataset to match schema for automated assessment scripts Add Secchi Depth to conventionals dataset Add Citizen Monitoring Data to conventionals Ensure all stations have necessary WQS and AU information Organize all new WQS/AU metadata from the R server and adding new data to pinned datasets Create a stationsTablebegin dataset Carry forward any stations required from last cycle Clean up PCB dataset Clean up Fish Tissue dataset The sections below are purposefully sparse as the methods are in flux given that new data are available in ODS. Stay tuned to see how these methods flesh out for more streamlined data organization in future cycles. 3.2.1 Reorganize Conventionals Dataset The official (SAS) conventionals data schema tends to vary from IR to IR. It is essential that this data are provided to the R scripts exactly how the scripts expect data, so each cycle the provided conventionals dataset must be meticulously QAed to ensure all data are of expected name/type. Additionally, in order to automate the assessment of citizen monitoring data, we must augment the provided conventionals dataset to accommodate the level schema for each citizen monitoring parameter. The nuanced data manipulation steps required to convert the conventionals data and citizen monitoring data from the provided format to the required format for automated analyses are beyond the scope of this book. Instead, the required data schema for automated assessment is provided below. conventionalsSchema &lt;- readRDS(&#39;exampleData/schemaFin.RDS&#39;) conventionalsSchema %&gt;% # preview data DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) 3.2.2 Adding Secchi Depth to conventionals dataset This data are not included in the official (SAS) conventionals data pull but are required for Trophic State Index (TSI) analyses for some lacustrine stations. The below script will only work if a users credentials have access to production ODS wqm data. The example script below first finds all unique stations in the conventionals dataset before querying secchi depth information for those stations for the given assessment window. The date range requires updating each assessment cycle. # dataset of all the unique stations that we need to organize conventionals_distinct &lt;- conventionals %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% # remove any data to avoid confusion dplyr::select(FDT_STA_ID:FDT_COMMENT, Latitude:Data_Source) %&gt;% filter(!is.na(FDT_STA_ID)) # Query secchi data for unique conventionals stations in the appropriate data window library(pool) library(dbplyr) ### connect to Production Environment pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;,# or whatever ODBC driver version you have installed locally Server= &quot;DEQ-SQLODS-PROD,50000&quot;, databasename = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) stationSecchiDepth &lt;- pool %&gt;% tbl(in_schema(&#39;wqm&#39;, &quot;Wqm_Field_Data_View&quot;)) %&gt;% filter(Fdt_Sta_Id %in% !! conventionals_distinct$FDT_STA_ID &amp; between(as.Date(Fdt_Date_Time), &quot;2014-12-31&quot;, &quot;2020-12-31&quot;) &amp; !is.na(Fdt_Secchi_Depth)) %&gt;% # x &gt;= left &amp; x &lt;= right dplyr::select(Fdt_Sta_Id, Fdt_Date_Time, Fdt_Depth, Fdt_Secchi_Depth) %&gt;% as_tibble() %&gt;% mutate(Date = as.Date(Fdt_Date_Time)) %&gt;% dplyr::select(FDT_STA_ID = Fdt_Sta_Id, Date, FDT_DEPTH = Fdt_Depth, Fdt_Secchi_Depth) Once the secchi data is acquired, it can be attached to the original conventionals dataset carefully as detailed below. conventionalsArchive &lt;- conventionals %&gt;% mutate(Date = as.Date(FDT_DATE_TIME)) conventionals &lt;- left_join(conventionalsArchive, stationSecchiDepth, by = c(&#39;FDT_STA_ID&#39;, &#39;Date&#39;, &#39;FDT_DEPTH&#39;)) %&gt;% mutate(SECCHI_DEPTH_M = Fdt_Secchi_Depth) %&gt;% dplyr::select(-c(Fdt_Secchi_Depth, Date)) 3.2.3 Add Citizen Monitoring Data to conventionals Once the citizen monitoring data is queried, QAed, and matches the required conventionals schema (above), the data can be appended to the conventionals dataset for automated analysis. 3.2.4 Ensure all stations have necessary WQS and AU information 3.2.4.1 Organizing all new WQS/AU metadata from the R server A server administrator must retrieve all the WQS/AU information from the attribution application. This directory is consolidated into a single dataset of all new StationID-WQS/AU records. It is important to verify that all expected StationIDs (all distinct stations from the conventionals and citizen monitoring datasets) have WQS and AU information before proceeding. If stations lack this information they will not be assessed accurately. 3.2.4.1.1 Adding new WQS data to pinned datasets Once WQS_ID/AU information is available for all new stations in an assessment cycle, the data are appended to the master WQS information pin on the R server in the case of WQS information. AU information can change cycle to cycle, so the IR year for the StationID-AU information is added to the dataset before pinning back to the master AU information pin. To expedite the transfer of actual WQS information (not just WQS_IDs), another pin contains the actual WQS metadata as well as relevant StationID. This pin is called WQSlookup-withStandards. All new stations attributed need to be joined to the relevant WQS layer to extract this metadata before it can be appended to the master WQS information pin with standards (WQSlookup-withStandards). The script below details this process by bringing in each necessary WQS layer, joining it to the WQSlookup information, and then pinning it to the R server. # table with StationID and WQS_ID information that needs actual WQS metadata WQSlookupToDo &lt;- tibble(StationID = NA, WQS_ID = NA) # blank for example purposes #bring in Riverine layers, valid for the assessment window riverine &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_drop_geometry() # only care about data not geometry WQSlookupFull &lt;- left_join(WQSlookupToDo, riverine, by = &#39;WQS_ID&#39;) %&gt;% filter(!is.na(CLASS)) # drop sites that didn&#39;t actually join rm(riverine) # clean up workspace lacustrine &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_drop_geometry() # only care about data not geometry WQSlookupFull &lt;- left_join(WQSlookupToDo, lacustrine, by = &#39;WQS_ID&#39;) %&gt;% filter(!is.na(CLASS)) %&gt;% # drop sites that didn&#39;t actually join bind_rows(WQSlookupFull) # add these to the larger dataset rm(lacustrine) # clean up workspace estuarineLines &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp&#39; , fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_drop_geometry() # only care about data not geometry WQSlookupFull &lt;- left_join(WQSlookupToDo, estuarineLines, by = &#39;WQS_ID&#39;) %&gt;% filter(!is.na(CLASS)) %&gt;% # drop sites that didn&#39;t actually join bind_rows(WQSlookupFull) # add these to the larger dataset rm(estuarineLines) # clean up workspace estuarinePolys &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_drop_geometry() # only care about data not geometry WQSlookupFull &lt;- left_join(WQSlookupToDo, estuarinePolys, by = &#39;WQS_ID&#39;) %&gt;% filter(!is.na(CLASS)) %&gt;% # drop sites that didn&#39;t actually join bind_rows(WQSlookupFull) # add these to the larger dataset rm(estuarinePolys) # clean up workspace WQSlookup_withStandards_pin &lt;- bind_rows(WQSlookup_withStandards, WQSlookupFull) # for pin The WQSlookup_withStandards_pin object is then pinned to the R server to be sourced by numerous other data users/products. This pin is available by using the following script. WQSlookup_withStandards &lt;- pin_get(&#39;ejones/WQSlookup-withStandards&#39;, board = &#39;rsconnect&#39;) 3.2.4.1.2 Adding new AU data to pinned datasets The new stations in an assessment cycle that were manually reviewed using the Regional Metadata Validation Tool need this AU information added to the existing AUlookup table pin stored on the R server. After an assessor adds these stations into WQA CEDS with AU information, this table is no longer used to source AU information for a station, but it is required for the start of an assessment cycle for all stations that were not included in any previous assessments. The server administrator must pull the AU attribution information contained in the Metadata App from the R server and append this information to the existing AUlookup table pin. The data may be retrieved by anyone using the following script. AUlookupArchive &lt;- pin_get(&#39;ejones/AUlookup&#39;, board = &#39;rsconnect&#39;) 3.2.4.2 Create a stationsTablebegin dataset The so called stationsTablebegin dataset is a key input to the automated assessment scripts. This dataset tells the scripts all the stations it should assess using the automated assessment functions. It is necessary to use this dataset as the list of stations to assess rather than any other individual dataset (e.g. conventionals, citmon/nonagency, PCB, etc.) because stations that need to be touched during a given assessment period might not have information in any of the individual datasets. This dataset also contains any stations from a previous cycle that must be carried over to the new cycle, i.e. carryover stations. These carryover stations may not have any data in the current window and thus would never appear in any individual dataset organized so far. The stationsTablebegin dataset contains not only all the stations that need to be addressed in an assessment cycle, but also at least one Assessment Unit per station for organizational purposes. Because station WQS information are only used for comparing individual parameters to criteria, we do not actually store WQS metadata in the stationsTablebegin dataset. This information is joined to the conventionals dataset at a later step in the automated assessment process to maintain a dataset most similar to the CEDS bulk upload template. Starting with the conventionals_distinct dataset (which includes citmon/non agency data at this point), we can begin to create our dataset of all the unique stations that we need to organize for the current cycle (stationsTablebegin). # conventionals_distinct &lt;- conventionals %&gt;% # distinct(FDT_STA_ID, .keep_all = T) %&gt;% # # remove any data to avoid confusion # dplyr::select(FDT_STA_ID:FDT_COMMENT, Latitude:Data_Source) %&gt;% # filter(!is.na(FDT_STA_ID)) conventionals_distinct &lt;- pin_get(&#39;ejones/conventionals2024_distinctdraft&#39;, board = &#39;rsconnect&#39;) %&gt;% filter(!is.na(Latitude) | !is.na(Longitude)) # and make a spatial version conventionals_sf &lt;- conventionals_distinct %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng We want the stationsTablebegin dataset to look like our CEDS Bulk Upload template to make future steps easier. The biggest changes from the older stations table format to this new bulk upload template is the addition of the 10 ID305B and station type columns as well as the lacustrine designation. The lacustrine designation field identifies whether or no a lake station is considered to be within the Lacustrine Zone of the lake. This information is used to assess nutrients in Section 187 lakes. stationsTemplate &lt;- read_excel(&#39;data/WQA_CEDS_templates/WQA_Bulk_Station_Upload_Final.xlsx&#39;,#&#39;WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx&#39;, sheet = &#39;Stations&#39;, col_types = &quot;text&quot;)[0,] # just want structure and not the draft data, force everything to character for now We want to populate as much of the information from the bulk upload template for each station to make the regional assessors lives easier. We will start by filling in as much as we can about each station that had an entry in last cycle. We will pull this information from the WQA area of ODS. You need ODS access to retrieve this information for yourself. Later steps will allow you to source the output of these query/manipulation steps if you do not have ODS access. 3.2.4.2.1 Last cycle AU information Lets start by populating this template with information we can grab from the previous assessment cycle stations table information. Connect to the ODS production environment. library(pool) library(dbplyr) ### Production Environment pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;, # or whatever ODBC driver version you have installed locally Server= &quot;DEQ-SQLODS-PROD,50000&quot;, databasename = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) 3.2.4.2.2 Carry forward any stations required from last cycle Find all stations from the last IR cycle that should be carried forward for review this cycle, either impaired last time or with the comment field containing carr% string (e.g. carry over, carried over, etc.). Start by querying stations in IR2022 and join in their AU information and station parameters. The key to these joins is the WXA_STATION_DETAIL_ID/Station Detail Id field. # data prep stations &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &quot;Wqa_Station_Details_View&quot;)) %&gt;% filter(WSD_CYCLE == 2022) %&gt;% as_tibble() %&gt;% distinct(WXA_STATION_DETAIL_ID, .keep_all = T) %&gt;% # distinct on this variable for AU join or duplicated rows for stations filter(WSD_STATION_ID != &#39;4AGSE013.78&#39;) # problem site OIS needs to deal with # WQA geospatial data only seems to have citmon/non agency station locations. All other stations (DEQ) need to be queried from the WQM side of ODS stationsGeospatial_wqa &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;Wqa_Stations_Geospatial_Data_View&#39;)) %&gt;% filter(Station_Id %in% !! stations$STA_NAME) %&gt;% #stations$WSD_STATION_ID) %&gt;% as_tibble() %&gt;% dplyr::select(Station_Id, Latitude, Longitude) # WQM geospatial data for DEQ stations stationsGeospatial_wqm &lt;- pool %&gt;% tbl(in_schema(&#39;wqm&#39;, &#39;WQM_Sta_GIS_View&#39;)) %&gt;% filter(Station_Id %in% !! stations$WSD_STATION_ID) %&gt;% as_tibble() %&gt;% dplyr::select(Station_Id, Latitude, Longitude) # combine geospatial data into one object for easier joining stationsGeospatial &lt;- bind_rows(stationsGeospatial_wqa, stationsGeospatial_wqm) %&gt;% distinct(Station_Id, .keep_all = T) %&gt;% mutate(Station_Id = case_when(Station_Id == &#39;Griggs Pond&#39; ~ toupper(Station_Id), Station_Id == &#39;Sims Metal 003&#39; ~ &#39;SIMS METAL 003&#39;, TRUE ~ as.character(Station_Id))) # Joining problems later if we don&#39;t capitalize the names Griggs Pond and Simms Metal 003 as they are elsewhere in ODS # QUery AU information AU &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA 305b]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationDetails &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA Station Parameters Pivot]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationType &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA Station Detail Types]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationsAU &lt;- left_join(stations, AU, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% left_join(stationType, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% left_join(stationsGeospatial, by = c(&#39;STA_NAME&#39; = &#39;Station_Id&#39;)) %&gt;% # Make sure you join on STA_NAME and not WSD_STATION_ID here!!! left_join(stationDetails, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) # actual analysis, find all stations with impaired parameters impairedStations &lt;- stationsAU %&gt;% filter_at(.vars = vars(contains(&quot;Status Code&quot;)), .vars_predicate = any_vars(str_detect(., &#39;IM&#39;))) # Cast a wide net: string search for any stations that were carried over from more previous cycles by looking for # variations of the phrase &quot;Carry&quot; in the station comment field carryoverStations &lt;- stationsAU %&gt;% filter(str_detect(WSD_COMMENTS, &#39;carr&#39;)) # combine lists and remove duplicates stationsFromLastCycle &lt;- bind_rows(impairedStations, carryoverStations) %&gt;% distinct(WXA_STATION_DETAIL_ID, .keep_all = T) # clean up workspace rm(list = c(&#39;stations&#39;,&#39;stationsAU&#39;, &#39;stationDetails&#39;, &#39;impairedStations&#39;, &#39;carryoverStations&#39;, &#39;AU&#39;, &#39;stationType&#39;, &#39;stationsGeospatial_wqa&#39;, &#39;stationsGeospatial_wqm&#39;)) Now we need to clean up this data to match the bulk upload data template. We will also strip out the data from previous cycles to not confuse anyone from cycle to cycle. stationsTable2024begin &lt;- stationsFromLastCycle %&gt;% dplyr::select(STATION_ID = STA_NAME, #### OR COULD USE WSD_STATION_ID ID305B_1 = `ID305B 1`, ID305B_2 = `ID305B 2`, ID305B_3 = `ID305B 3`, ID305B_4 = `ID305B 4`, ID305B_5 = `ID305B 5`, ID305B_6 = `ID305B 6`, ID305B_7 = `ID305B 7`, ID305B_8 = `ID305B 8`, ID305B_9 = `ID305B 9`, ID305B_10 = `ID305B 10`, WATER_TYPE = WWT_WATER_TYPE_DESC, SALINITY = WSC_DESCRIPTION, LACUSTRINE = WSD_LAC_ZONE_YN, REGION = STA_REGION, TYPE_1 = `Station Type 1`, TYPE_2 = `Station Type 2`, TYPE_3 = `Station Type 3`, TYPE_4 = `Station Type 4`, TYPE_5 = `Station Type 5`, TYPE_6 = `Station Type 6`, TYPE_7 = `Station Type 7`, TYPE_8 = `Station Type 8`, TYPE_9 = `Station Type 9`, TYPE_10 = `Station Type 10`, LATITUDE = Latitude, LONGITUDE = Longitude, WATERSHED = STA_WATERSHED, VAHU6 = STA_VA_HU6) %&gt;% mutate(REGION = case_when(REGION == &#39;NVRO&#39; ~ &#39;NRO&#39;, REGION == &#39;WCRO&#39; ~ &#39;BRRO&#39;, TRUE~ as.character(REGION))) %&gt;% dplyr::select(any_of(names(stationsTemplate))) stationsTable2024begin &lt;- bind_rows(stationsTemplate %&gt;% mutate(LATITUDE = as.numeric(LATITUDE), LONGITUDE = as.numeric(LONGITUDE)), stationsTable2024begin) # add back in missing columns Quick QA check for any missing geospatial data. missingGeospatial &lt;- filter(stationsTable2024begin, is.na(LATITUDE) | is.na(LONGITUDE)) # clean up workspace rm(list = c(&#39;stationsFromLastCycle&#39;,&#39;missingGeospatial&#39;)) 3.2.4.2.3 AU information from Last Cylce Now we need to get the same metadata for all the stations from the conventionals (and citmon/non agency) dataset from the current cycle. We can make our lives easier by only doing this work for new stations from the conventionals dataset (i.e. dropping all stations from our to do list that already have this information from our last step). This chunk will also reformat the queried data into the bulk upload template format so it can be combined with the stationsTable2024begin dataset created above. stationsToDo &lt;- filter(conventionals_distinct, ! FDT_STA_ID %in% stationsTable2024begin$STATION_ID) # use the same method from above with a new station list stations &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &quot;Wqa_Station_Details_View&quot;)) %&gt;% filter(WSD_STATION_ID %in% !! stationsToDo$FDT_STA_ID) %&gt;% # pull all data from stations identified above as_tibble() %&gt;% # get that data local before doing more complicated things than SQL wants to handle # keep only the most recent record for each station by grouping and then filtering group_by(WSD_STATION_ID) %&gt;% filter(WSD_CYCLE == max(WSD_CYCLE )) %&gt;% distinct(WXA_STATION_DETAIL_ID, .keep_all = T) %&gt;% # still need only 1 record per site ungroup() # ungroup so the WSD_STATION_ID column doesn&#39;t come along for the ride to future steps where not necessary # WQA geospatial data only seems to have citmon/non agency station locations. All other stations (DEQ) need to be queried from the WQM side of ODS stationsGeospatial_wqa &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;Wqa_Stations_Geospatial_Data_View&#39;)) %&gt;% filter(Station_Id %in% !! stations$STA_NAME) %&gt;% #stations$WSD_STATION_ID) %&gt;% as_tibble() %&gt;% dplyr::select(Station_Id, Latitude, Longitude) # WQM geospatial data for DEQ stations stationsGeospatial_wqm &lt;- pool %&gt;% tbl(in_schema(&#39;wqm&#39;, &#39;WQM_Sta_GIS_View&#39;)) %&gt;% filter(Station_Id %in% !! stations$WSD_STATION_ID) %&gt;% as_tibble() %&gt;% dplyr::select(Station_Id, Latitude, Longitude) # combine geospatial data into one object for easier joining stationsGeospatial &lt;- bind_rows(stationsGeospatial_wqa, stationsGeospatial_wqm) %&gt;% distinct(Station_Id, .keep_all = T) AU &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA 305b]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationDetails &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA Station Parameters Pivot]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationType &lt;- pool %&gt;% tbl(in_schema(&#39;wqa&#39;, &#39;[WQA Station Detail Types]&#39;)) %&gt;% # **note** need to put views with spaces in name in brackets for SQL to work filter(`Station Detail Id` %in% !! stations$WXA_STATION_DETAIL_ID) %&gt;% as_tibble() stationsAU &lt;- left_join(stations, AU, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% left_join(stationType, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% left_join(stationsGeospatial, by = c(&#39;STA_NAME&#39; = &#39;Station_Id&#39;)) %&gt;% # Make sure you join on STA_NAME and not WSD_STATION_ID here!!! left_join(stationDetails, by = c(&#39;WXA_STATION_DETAIL_ID&#39; = &#39;Station Detail Id&#39;)) %&gt;% # reorganize to fit the data template dplyr::select(STATION_ID = STA_NAME, #### OR COULD USE WSD_STATION_ID ID305B_1 = `ID305B 1`, ID305B_2 = `ID305B 2`, ID305B_3 = `ID305B 3`, ID305B_4 = `ID305B 4`, ID305B_5 = `ID305B 5`, ID305B_6 = `ID305B 6`, ID305B_7 = `ID305B 7`, ID305B_8 = `ID305B 8`, ID305B_9 = `ID305B 9`, ID305B_10 = `ID305B 10`, WATER_TYPE = WWT_WATER_TYPE_DESC, SALINITY = WSC_DESCRIPTION, LACUSTRINE = WSD_LAC_ZONE_YN, REGION = STA_REGION, TYPE_1 = `Station Type 1`, TYPE_2 = `Station Type 2`, TYPE_3 = `Station Type 3`, TYPE_4 = `Station Type 4`, TYPE_5 = `Station Type 5`, TYPE_6 = `Station Type 6`, TYPE_7 = `Station Type 7`, TYPE_8 = `Station Type 8`, TYPE_9 = `Station Type 9`, TYPE_10 = `Station Type 10`, LATITUDE = Latitude, LONGITUDE = Longitude, WATERSHED = STA_WATERSHED, VAHU6 = STA_VA_HU6) %&gt;% mutate(REGION = case_when(REGION == &#39;NVRO&#39; ~ &#39;NRO&#39;, REGION == &#39;WCRO&#39; ~ &#39;BRRO&#39;, TRUE~ as.character(REGION))) %&gt;% dplyr::select(any_of(names(stationsTemplate))) # Smash in with the rest of the sites already organized stationsTable2024begin &lt;- bind_rows(stationsTable2024begin, stationsAU) # clean up workspace rm(list = c(&#39;stations&#39;, &#39;stationDetails&#39;, &#39;stationsAU&#39;, &#39;AU&#39;, &#39;stationType&#39;, &#39;stationsGeospatial_wqa&#39;, &#39;stationsGeospatial_wqm&#39;)) 3.2.4.2.4 AU information for New Stations So what stations do we have left? These are stations that are in the conventionals dataset but dont have any historical records in CEDS WQA. We will reorganize them into the template format that we need and add AU information from the pinned data on the R server. stationsToDo &lt;- filter(conventionals_distinct, ! FDT_STA_ID %in% stationsTable2024begin$STATION_ID) %&gt;% # glean what we can to populate the data template dplyr::select(STATION_ID = FDT_STA_ID, WATER_TYPE = STA_LV1_CODE, TYPE_1 = STA_LV2_CODE, LATITUDE = Latitude, LONGITUDE = Longitude) %&gt;% mutate(# throw in a flag for swamp but still need to force it to a waterbody type the assessment tools understand TYPE_1 = case_when(WATER_TYPE == &#39;SWAMP&#39; ~ paste(TYPE_1, &#39;SWAMP&#39;, sep = &#39;: &#39;), TRUE~ as.character(TYPE_1)), WATER_TYPE = ifelse(WATER_TYPE == &#39;SWAMP&#39;, NA, WATER_TYPE)) %&gt;% dplyr::select(any_of(names(stationsTemplate))) %&gt;% # Spatially join info we don&#39;t trust from CEDS WQM, first turn this into a spatial object st_as_sf(coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # add projection, needs to be geographic for now bc entering lat/lng # get pinned AU data from the R server AUlookup &lt;- pin_get(&#39;ejones/AUlookup&#39;, board = &#39;rsconnect&#39;) %&gt;% filter(CYCLE == 2024) # only get sites attributed by assessors for this cycle # get spatial data from the R server vahu6 &lt;- st_as_sf(pin_get(&quot;ejones/AssessmentRegions_VA84_basins&quot;, board = &quot;rsconnect&quot;)) dcr11 &lt;- st_as_sf(pin_get(&quot;ejones/dcr11&quot;, board = &quot;rsconnect&quot;)) # Spatially join info we don&#39;t trust from CEDS WQM stationsLeft &lt;- st_join(stationsToDo, vahu6) %&gt;% dplyr::select(STATION_ID:LONGITUDE, VAHU6, REGION = ASSESS_REG) %&gt;% st_join(dcr11) %&gt;% dplyr::select(STATION_ID:REGION, WATERSHED = ANCODE) %&gt;% st_drop_geometry() %&gt;% # turn back into tibble left_join(dplyr::select(AUlookup, FDT_STA_ID, ID305B_1 ), by = c(&#39;STATION_ID&#39; = &#39;FDT_STA_ID&#39;)) # join in AU info from the pinned AUlookup dataset (populated by assessors in metadata attribution app) # smash into template stationsTable2024begin &lt;- bind_rows(stationsTable2024begin, stationsLeft) # clean up workspace rm(list = c(&#39;stationsLeft&#39;, &#39;stationsToDo&#39;, &#39;stationsTemplate&#39;, &#39;stationsGeospatial&#39;, &#39;dcr11&#39;, &#39;vahu6&#39;, &#39;AUlookup&#39;)) This dataset is pinned to the R server for anyone to use. You can retreive it using the following script. stationsTablebegin &lt;- pin_get(&#39;ejones/stationsTable2024begin&#39;, board = &#39;rsconnect&#39;) stationsTablebegin %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) ## Warning in instance$preRenderHook(instance): It seems your data is too big ## for client-side DataTables. You may consider server-side processing: https:// ## rstudio.github.io/DT/server.html 3.2.5 Clean up PCB dataset IR 2022 had to use fuzzyjoining to get PCB StationID to a real DEQ StationID for the data to show up in apps. Data TBD so not writing cleanup methods yet. This data is pinned to server when ready to be sourced by assessment apps/analysts. 3.2.6 Clean up Fish Tissue dataset Who knows what this will look like. We really need a better system for updating data. No scripts included on how to do this yet since the product changes cycle to cycle. This data is pinned to server when ready to be sourced by assessment apps/analysts. 3.2.7 Pin Official Clean IR data to R server By cleaning up all provided data and then pinning to the R server, we can easily distribute and archive all data required and sourced by automated scripts in on secure location. The assessment applications source the pinned data to expedite application rendering time. The official assessment data pinned to the R server include: Conventionals dataset (including citizen monitoring data) Water Column Metals Sediment Metals VAHU6 spatial data WQS information for each station "],["automated-assessment.html", "3.3 Automated Assessment", " 3.3 Automated Assessment Now that data are prepped for analysis, the last step is to bring all the data together, complete a few more manipulation steps, and feed the data through a process that assesses the data. To minimize the level of coding experience required to run the automated assessment process, the operation is written as a loop instead of as a function. Loops are inherently slow and sometimes confusing, but for this specific use case housing all operations inside a single loop increases the visibility of the order of operations and decreases the amount of coding experience users need to understand what is happening when. All individual parameter analyses are programmed as efficient functions and are detailed in the Individual Parameter Analyses section. As always, first set up your local environment with all the necessary packages you will use, source any necessary scripts, and connect to the R server to pull pinned data. The custom assessment functions brought in below are stored in separate .R files and sourced into this script. This is a best practice to enable easier maintenance of the custom assessment functions as well as minimize the amount of code in this Rmarkdown document. When you source the scripts, you will see all the functions in your Environment pane. This is different than when we call in a package because one cannot see all the functions in a loaded package in their Environment pane. DEQ is working towards publishing an automated assessment package to neatly distribute all these custom functions. Until then, you can download these sourced scripts here. You can download the latest working copy of the automated assessment workflow in a single Rmarkdown document here. An important feature of this script is in the first line of code: options(digits = 12). This set our local environment options and tells R to use 12 digits when storing numeric information. It is critical to store all potential digits of numeric data to not impose intermediate rounding steps and potentially skew results. Steps like these allow for statewide consistency. Spreadsheet and database software do not easily allow this level of calculation control. It is important to note that some tidyverse default printing behavior might make it appear that numerical data are rounded to only three digits. This is a feature for fast and clear data printing and does not reflect the true nature of the numeric data stored in your environment after establishing the options(digits = 12) environment option. You can always double check the numeric data by printing the full object (e.g. datasetIamWorkingWith$numericalColumnIwantToInvestigateFurther). options(digits = 12) # critical to seeing all the data library(tidyverse) library(sf) library(readxl) library(pins) library(config) library(EnvStats) library(lubridate) library(round) # for correct round to even logic # Bring in custom assessment functions source(&#39;automatedAssessmentFunctions.R&#39;) source(&#39;updatedBacteriaCriteria.R&#39;) # get configuration settings conn &lt;- config::get(&quot;connectionSettings&quot;) # use API key to register board board_register_rsconnect(key = conn$CONNECT_API_KEY, server = conn$CONNECT_SERVER) 3.3.1 Bring in required datasets Bring in pinned data and local data. This section is still being actively written, stay tuned for an update with final IR2024 data. # official March Data releases for IR 2022 conventionals &lt;- pin_get(&#39;ejones/conventionals2024draft&#39;, board = &#39;rsconnect&#39;) conventionals_distinct &lt;- pin_get(&#39;ejones/conventionals2024_distinctdraft&#39;, board = &#39;rsconnect&#39;) #old will update stations2020IR &lt;- pin_get(&quot;stations2020IR-sf-final&quot;, board = &quot;rsconnect&quot;) WQMstationFull &lt;- pin_get(&quot;WQM-Station-Full&quot;, board = &quot;rsconnect&quot;) VSCIresults &lt;- pin_get(&quot;VSCIresults&quot;, board = &quot;rsconnect&quot;) %&gt;% filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) VCPMI63results &lt;- pin_get(&quot;VCPMI63results&quot;, board = &quot;rsconnect&quot;) %&gt;% filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) VCPMI65results &lt;- pin_get(&quot;VCPMI65results&quot;, board = &quot;rsconnect&quot;) %&gt;% filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) # placeholders from IR2022 until draft or final IR2024 data are available WCmetals &lt;- pin_get(&quot;WCmetals-2022IRfinal&quot;, board = &quot;rsconnect&quot;) Smetals &lt;- pin_get(&quot;Smetals-2022IRfinal&quot;, board = &quot;rsconnect&quot;) markPCB &lt;- read_excel(&#39;data/2022 IR PCBDatapull_EVJ.xlsx&#39;, sheet = &#39;2022IR Datapull EVJ&#39;) # will be pinned data for IR2024 fishPCB &lt;- read_excel(&#39;data/FishTissuePCBsMetals_EVJ.xlsx&#39;, sheet= &#39;PCBs&#39;) # will be pinned data for IR2024 fishMetals &lt;- read_excel(&#39;data/FishTissuePCBsMetals_EVJ.xlsx&#39;, sheet= &#39;Metals&#39;) # will be pinned data for IR2024 Bring in the Station Table from the two most recent assessment cycles. The comment field from each of these datasets is joined to the Station Table output to assist regional assessment staff. stationsTable2022 &lt;- readRDS(&#39;../2.organizeMetadata/data/stationsTable2022.RDS&#39;) stationsTable2020 &lt;- readRDS(&#39;../2.organizeMetadata/data/stationsTable2020.RDS&#39;) Bring in lake nutrient standards. The lake-specific nutrient criteria are stored separate from WQS spatial data because these fields do not exist in the WQS spatial layers. Lake names in the WQS spatial layers are notoriously difficult to join against because of all the variation in names. As such, this dataset contains a special joining name field Lake_Name to clean up joining issues. lakeNutStandards &lt;- read_csv(&#39;exampleData/9VAC25-260-187lakeNutrientStandards.csv&#39;) ## Parsed with column specification: ## cols( ## Lake_Name = col_character(), ## `Man-made Lake or Reservoir Name` = col_character(), ## Location = col_character(), ## `Chlorophyll a (ug/L)` = col_double(), ## `Total Phosphorus (ug/L)` = col_double() ## ) lakeNutStandards %&gt;% # preview dataset DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Bring in beginning station table data. This information communicates to the scripts which stations should be assessed and where they should be organized (AUs). It also has data from the last cycle to populate the historical station information table in the application. To Note: The AU assignments in this table are valid as of the start of the assessment process. For any AU splits/reassignments, the assessors control that through local (.csv) copies of the output of this script that is uploaded to the relevant Assessment Application on the Connect Server (and subsequently uploaded to CEDS via the bulk data upload tool). stationTable &lt;- pin_get(&#39;ejones/stationsTable2024begin&#39;, board = &#39;rsconnect&#39;) Bring in Station Table Bulk Upload Template. This is the template format to match such that output from this script can be easily uploaded to CEDS via the Bulk Data Upload tool. Any deviations to this template in the output dataset were specifically requested by regional assessment staff to improve and expedite their review process. They are responsible for removing those added fields prior to bulk data upload. stationsTemplate &lt;- read_excel(&#39;WQA_CEDS_templates/WQA_Bulk_Station_Upload_Final.xlsx&#39;,#&#39;WQA_CEDS_templates/WQA_Bulk_Station_Upload (3).xlsx&#39;, sheet = &#39;Stations&#39;, col_types = &quot;text&quot;)[0,] %&gt;% mutate(LATITUDE = as.numeric(LATITUDE), LONGITUDE = as.numeric(LONGITUDE)) %&gt;% mutate_at(vars(contains(&#39;_EXC&#39;)), as.integer) %&gt;% mutate_at(vars(contains(&#39;_SAMP&#39;)), as.integer) %&gt;% # new addition that breaks station table upload template but very helpful for assessors mutate(BACTERIADECISION = as.character(NA), BACTERIASTATS = as.character(NA), `Date Last Sampled` = as.character(NA)) 3.3.2 Attach WQS information Pull pinned WQS info saved on server. Then perform a series of data manipulation steps that: Join WQS to station table by StationID Create a new variable that will correct for Class II pH differences for Tidal Waters Join actual WQS critera (object name WQSvalues) to each StationID Perform a little data cleanup to lose columns unnecessary for future steps Join station ecoregion information (for benthic analyses) Standardize lake names (to match 187 lake names for future joining of criteria) Some problematic stations need manual steps to attach the correct lake name Join lake nutrient criteria to individual stations Lake Drummond requires some extra manual work per hh special standard https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section310/ Rearrange data schema to more useful format after all previous joins Correct units for nutrient criteria to match conventionals data format (ug/L to mg/L) WQSlookup &lt;- pin_get(&quot;WQSlookup-withStandards&quot;, board = &quot;rsconnect&quot;) citmonWQS &lt;- readRDS(&#39;C:/HardDriveBackup/R/GitHub/IR2024/1.preprocessData/data/citmonStationsWithWQS.RDS&#39;) %&gt;% dplyr::select(StationID = `Station Id`, `WQS Section`, `WQS Class`, `WQS Special Standard`) stationTable &lt;- stationTable %&gt;% # (1) # Special CitMon/Non Agency step until full WQS_ID inplementation in IR2028 left_join(citmonWQS, by = c(&#39;STATION_ID&#39; = &#39;StationID&#39;)) %&gt;% # (1) # Join to real WQS_ID&#39;s (do this second in case citmon station double listed, want proper WQS_ID if available) (1) left_join(WQSlookup, by = c(&#39;STATION_ID&#39; = &#39;StationID&#39;)) %&gt;% # coalesce these similar fields together, taking WQS_ID info before citmon method mutate(CLASS = coalesce(CLASS, `WQS Class`), SEC = coalesce(SEC, `WQS Section`), SPSTDS = coalesce(SPSTDS, `WQS Special Standard`)) %&gt;% dplyr::select(-c(`WQS Section`, `WQS Class`, `WQS Special Standard`)) %&gt;% # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep=&quot;_&quot;)) %&gt;% # (2) mutate(CLASS_BASIN = ifelse(CLASS_BASIN == &#39;II_7&#39;, &quot;II_7&quot;, as.character(CLASS))) %&gt;% # (2) # Join actual WQS criteria to each StationID left_join(WQSvalues, by = &#39;CLASS_BASIN&#39;) %&gt;% # (3) # data cleanup dplyr::select(-c(CLASS.y,CLASS_BASIN)) %&gt;% # (4) rename(&#39;CLASS&#39; = &#39;CLASS.x&#39;) %&gt;% # (4) # As of 1/5/23, confirmed that water temperature criteria for class VII waters is determined by the former class of the water. Also confirmed that all class VII waters in TRO, PRO, and NRO were formerly class III, which means that these waters have a maximum temperature criteria of 32 degrees C. mutate(`Max Temperature (C)` = case_when( CLASS == &quot;VII&quot; &amp; REGION == &quot;TRO&quot; ~ 32, CLASS == &quot;VII&quot; &amp; REGION == &quot;PRO&quot; ~ 32, CLASS == &quot;VII&quot; &amp; REGION == &quot;NRO&quot; ~ 32, TRUE ~ as.numeric(`Max Temperature (C)`) )) %&gt;% # (3) # Join station ecoregion information (for benthic analyses) left_join(dplyr::select(WQMstationFull, WQM_STA_ID, EPA_ECO_US_L3CODE, EPA_ECO_US_L3NAME) %&gt;% #(5) distinct(WQM_STA_ID, .keep_all = TRUE), by = c(&#39;STATION_ID&#39; = &#39;WQM_STA_ID&#39;)) %&gt;% # Standardize lake names (to match 187 lake names for future joining of criteria) lakeNameStandardization() %&gt;% # standardize lake names (6) # extra special step (6) mutate(Lake_Name = case_when(STATION_ID %in% c(&#39;2-TRH000.40&#39;) ~ &#39;Thrashers Creek Reservoir&#39;, STATION_ID %in% c(&#39;2-LSL000.16&#39;) ~ &#39;Lone Star Lake F (Crystal Lake)&#39;, STATION_ID %in% c(&#39;2-LSL000.04&#39;) ~ &#39;Lone Star Lake G (Crane Lake)&#39;, STATION_ID %in% c(&#39;2-LSL000.20&#39;) ~ &#39;Lone Star Lake I (Butler Lake)&#39;, STATION_ID %in% c(&#39;2-NWB002.93&#39;,&#39;2-NWB004.67&#39;, &#39;2-NWB006.06&#39;) ~ &#39;Western Branch Reservoir&#39;, STATION_ID %in% c(&#39;2-LDJ000.60&#39;) ~ &#39;Lake Nottoway (Lee Lake)&#39;, TRUE ~ as.character(Lake_Name))) %&gt;% # Join lake nutrient criteria to individual stations left_join(lakeNutStandards %&gt;% mutate(Lakes_187B = &#39;y&#39;), # special step to make sure the WQS designation for 187 are correct even when not by = c(&#39;Lake_Name&#39;)) %&gt;% # (7) # lake drummond special standards mutate(Lakes_187B = ifelse(is.na(Lakes_187B.y ), Lakes_187B.x, Lakes_187B.y), # dd. For Lake Drummond, located within the boundaries of Chesapeake and Suffolk in the Great Dismal Swamp, chlorophyll a shall not exceed 35 g/L and total phosphorus shall not exceed 40 g/L at a depth of one meter or less. https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section310/ `Chlorophyll a (ug/L)` = case_when(Lake_Name %in% c(&#39;Lake Drummond&#39;) ~ 35, TRUE ~ as.numeric(`Chlorophyll a (ug/L)`)), `Total Phosphorus (ug/L)` = case_when(Lake_Name %in% c(&#39;Lake Drummond&#39;) ~ 40, TRUE ~ as.numeric(`Total Phosphorus (ug/L)`))) %&gt;% # (7) dplyr::select(STATION_ID:StreamType, Lakes_187B, `Description Of Waters`:`Total Phosphorus (ug/L)`) %&gt;% # (8) # match lake limit to TP data unit mutate(`Total Phosphorus (mg/L)` = `Total Phosphorus (ug/L)` / 1000) # (9) # Identify stations that are missing WQS missingWQS &lt;- filter(stationTable, is.na(CLASS)) 3.3.3 Identify Station Type It is important to have a lookup table of all lake stations to efficiently run computationally-intense lake methods only on lake stations. This next step identifies all stations that have lake AU designations. We will use this information later to determine how to analyze certain parameters that have different methods based on waterbody type. lakeStations &lt;- filter_at(stationTable, .vars = vars(contains(&quot;ID305B&quot;)), .vars_predicate = any_vars(str_detect(., &#39;L_&#39;))) 3.3.4 Bring in Low Flow Information We also need to bring in the previously analyzed low flow periods for the assessment window. These USGS stream gages are analyzed using the 7Q10 methodology consistent with the DEQ Water Permitting Program with two assumption changes: Provisional data are accepted (due to the expedited assessment timeline). QA/QC changes from Provision to Accepted data generally involve storm event corrections, so using Provisional data should not affect low flow analyses. Gages analyzed for low flow statistics require 10 or more valid water years for inclusion in analyses. This gage-specific information is spatially joined to major river subbasins to extrapolate low flow conditions to stations that fall into said watersheds. The temporal windows where low flow events were recorded in a watershed are joined to stations that fall into said watershed. Lake stations are removed from this flag. Parameters that should not be analyzed during low flow events are flagged during the automated assessment process and presented to the assessment staff in the assessment applications. A station-level flag is presented in the stationTable output of these scripts for users who prefer to only use the tabular output from the automated assessment tools. Spatially join when doing stuff now with draft data # Simplify the data first by date and BASIN_CODE as to not duplicate data when joined to conventionals raw data assessmentWindowLowFlowsBasinLevelFlag &lt;- dplyr::select(assessmentWindowLowFlows, Date, `Gage ID`, `7Q10 Flag`, BASIN_CODE) %&gt;% group_by(Date, BASIN_CODE) %&gt;% summarise(`7Q10 Flag Gage` = paste(unique(`Gage ID`),collapse = &quot;, &quot;), `7Q10 Flag` = `7Q10 Flag`) %&gt;% # still have duplicate rows with same info, so need to drop those or will duplicate conventionals data when joined in mutate(n = 1:n()) %&gt;% filter(n == 1) %&gt;% dplyr::select(-n) # get rid of count bc no longer needed conventionals &lt;- conventionals %&gt;% left_join(dplyr::select(conventionals_distinct, FDT_STA_ID, BASIN_CODE) %&gt;% st_drop_geometry(), by = &#39;FDT_STA_ID&#39;) %&gt;% mutate(SampleDate = as.Date(FDT_DATE_TIME)) %&gt;% #left_join(dplyr::select(stationTable, STATION_ID, VAHU6), by = c(&#39;FDT_STA_ID&#39; = &#39;STATION_ID&#39;)) %&gt;% # join in VAHU6 information from Assessors #left_join(dplyr::select(subbasinToVAHU6, VAHU6, BASIN_CODE, Basin_Code), by = &#39;VAHU6&#39;) # join in basin information by VAHU6 left_join(assessmentWindowLowFlowsBasinLevelFlag, by = c(&#39;SampleDate&#39; = &#39;Date&#39;, &#39;BASIN_CODE&#39; = &#39;BASIN_CODE&#39;)) %&gt;% dplyr::select(-SampleDate) # rm unnecessary column # Special step to remove low flow information from lake stations conventionals &lt;- conventionals %&gt;% mutate(`7Q10 Flag` = case_when(FDT_STA_ID %in% lakeStations$STATION_ID ~ NA_character_, TRUE ~ `7Q10 Flag`)) #View(filter(conventionals, `7Q10 Flag` == &quot;7Q10 Flag&quot;)) 3.3.5 Metals Reoganization We need to reorganize raw water column metals data to enable better exceedance analyses across automated scripts and applications. # Separate object for analysis, tack on METALS and RMK designation to make the filtering of certain lab comment codes easier WCmetalsForAnalysis &lt;- WCmetals %&gt;% dplyr::select(Station_Id, FDT_DATE_TIME, FDT_DEPTH, # include depth bc a few samples taken same datetime but different depths METAL_Antimony = `STORET_01095_ANTIMONY, DISSOLVED (UG/L AS SB)`, RMK_Antimony = RMK_01097, METAL_Arsenic = `STORET_01000_ARSENIC, DISSOLVED (UG/L AS AS)`, RMK_Arsenic = RMK_01002, METAL_Barium = `STORET_01005_BARIUM, DISSOLVED (UG/L AS BA)`, RMK_Barium = RMK_01005, METAL_Cadmium = `STORET_01025_CADMIUM, DISSOLVED (UG/L AS CD)`, RMK_Cadmium = RMK_01025, METAL_Chromium = `STORET_01030_CHROMIUM, DISSOLVED (UG/L AS CR)`, RMK_Chromium = RMK_01030, # Chromium III and ChromiumVI dealt with inside metalsAnalysis() METAL_Copper = `STORET_01040_COPPER, DISSOLVED (UG/L AS CU)`, RMK_Copper = RMK_01040, METAL_Lead = `STORET_01049_LEAD, DISSOLVED (UG/L AS PB)`, RMK_Lead = RMK_01049, METAL_Mercury = `STORET_50091_MERCURY-TL,FILTERED WATER,ULTRATRACE METHOD UG/L`, RMK_Mercury = RMK_50091, METAL_Nickel = `STORET_01065_NICKEL, DISSOLVED (UG/L AS NI)`, RMK_Nickel = RMK_01067, METAL_Uranium = `URANIUM_TOT`, RMK_Uranium = `RMK_7440-61-1T`, METAL_Selenium = `STORET_01145_SELENIUM, DISSOLVED (UG/L AS SE)`, RMK_Selenium = RMK_01145, METAL_Silver = `STORET_01075_SILVER, DISSOLVED (UG/L AS AG)`, RMK_Silver = RMK_01075, METAL_Thallium = `STORET_01057_THALLIUM, DISSOLVED (UG/L AS TL)`, RMK_Thallium = RMK_01057, METAL_Zinc = `STORET_01090_ZINC, DISSOLVED (UG/L AS ZN)`, RMK_Zinc = RMK_01092, METAL_Hardness = `STORET_DHARD_HARDNESS, CA MG CALCULATED (MG/L AS CACO3) AS DISSOLVED`, RMK_Hardness = RMK_DHARD) %&gt;% group_by(Station_Id, FDT_DATE_TIME, FDT_DEPTH) %&gt;% mutate_if(is.numeric, as.character) %&gt;% # need everyone as character so we can pivot longer in one go pivot_longer(cols = METAL_Antimony:RMK_Hardness, #RMK_Antimony:RMK_Hardness, names_to = c(&#39;Type&#39;, &#39;Metal&#39;), names_sep = &quot;_&quot;, values_to = &#39;Value&#39;) %&gt;% ungroup() %&gt;% group_by(Station_Id, FDT_DATE_TIME, FDT_DEPTH, Metal) %&gt;% pivot_wider(id_cols = c(Station_Id, FDT_DATE_TIME, FDT_DEPTH, Metal), names_from = Type, values_from = Value) %&gt;% # pivot remark wider so the appropriate metal value is dropped when filtering on lab comment codes filter(! RMK %in% c(&#39;IF&#39;, &#39;J&#39;, &#39;O&#39;, &#39;QF&#39;, &#39;V&#39;)) %&gt;% # lab codes dropped from further analysis pivot_longer(cols= METAL:RMK, names_to = &#39;Type&#39;, values_to = &#39;Value&#39;) %&gt;% # get in appropriate format to flip wide again pivot_wider(id_cols = c(Station_Id, FDT_DATE_TIME, FDT_DEPTH), names_from = c(Type, Metal), names_sep = &quot;_&quot;, values_from = Value) %&gt;% mutate_at(vars(contains(&#39;METAL&#39;)), as.numeric) %&gt;%# change metals values back to numeric rename_with(~str_remove(., &#39;METAL_&#39;)) # drop METAL_ prefix for easier analyses "],["automated-assessment-analysis.html", "3.4 Automated Assessment Analysis", " 3.4 Automated Assessment Analysis Finally, we are ready to analyze some data! Go through stationTable one station at a time, join conventionals, analyze all available data by each parameter function (or nested functions), and report out results in the WQA CEDS bulk upload template format. The next chunk is long. We start by setting up placeholder objects to store the data we want to save for later (stationTableResults and ammoniaAnalysis). The ammonia calculation takes a bit of time, so we save all the data, analysis, and results in ammoniaAnalysis so we can easily source this information for faster application rendering. The loop runs through each station, correcting special standards information and running any thermocline analyses, as necessary. This stationData object is fed into all subsequent automated assessment functions that calculate results for individual parameters and output information into the appropriate output format. Certain parameter functions take longer to compute than others, or are sourced multiple times in this chunk, so they are run first, e.g. ammonia, Public Water Supply (PWS) criteria, nutrients, dissolved oxygen daily average criteria, etc. The results object is then constructed parameter by parameter into the Stations Table template format. This dataset is then appended to the overall output dataset, stationTableResults, before moving on to the next station. Should a station in the stationTable object not have any data in conventionals, the appropriate metadata in the Stations Table template format is still sent to the stationTable output object with any previous cycle comments, potentially indicating that the station doesnt have new data. Please see the Individual Parameter Analyses section for detailed descriptions of what each individual parameter functions are doing during the below analysis. # make placeholder objects to store data results stationTableResults &lt;- stationsTemplate # save ammonia results (based on default assessment information) for use in app to speed rendering ammoniaAnalysis &lt;- tibble() # time the operation so we can know how long it takes: startTime &lt;- Sys.time() # loop over all sites, not super efficient but get the job done in easy to follow format for(i in 1:nrow(stationTable)){ print(paste(&#39;Assessing station&#39;, i, &#39;of&#39;, nrow(stationTable), sep=&#39; &#39;)) # print progress information # pull one station data stationData &lt;- filter(conventionals, FDT_STA_ID %in% stationTable$STATION_ID[i]) %&gt;% left_join(stationTable, by = c(&#39;FDT_STA_ID&#39; = &#39;STATION_ID&#39;)) %&gt;% # Special Standards Correction step. This is done on the actual data bc some special standards have temporal components pHSpecialStandardsCorrection() %&gt;% # correct pH to special standards where necessary temperatureSpecialStandardsCorrection() %&gt;% # correct temperature special standards where necessary # special lake steps {if(stationTable$STATION_ID[i] %in% lakeStations$STATION_ID) suppressWarnings(suppressMessages( mutate(., lakeStation = TRUE) %&gt;% thermoclineDepth())) # adds thermocline information and SampleDate else mutate(., lakeStation = FALSE) } # -------------------------------------------------------------------------------------------------------------------------------------- # Add some extra columns that are helpful for assessors but don&#39;t fit the bulk upload template # Previous station table comments comments &lt;- stationTableComments(stationTable$STATION_ID[i], stationsTable2022, &#39;2022&#39;, stationsTable2020, &#39;2020&#39;) # Date last sampled if(nrow(stationData) &gt; 0){ dateLastSampled &lt;- as.character(max(stationData$FDT_DATE_TIME)) } else {dateLastSampled &lt;- &#39;No data in conventionals data pull&#39;} # Low Flow Summary, only gives flag if station has FDT_TEMP_CELCIUS, DO_mg_L, FDT_FIELD_PH collected on day with gage in subbasin # below 7Q10 lowFlowSummary &lt;- lowFlowFlagColumn(stationData) #--------------------------------------------------------------------------------------------------------------------------------------- # Ammonia special section ammoniaAnalysisStation &lt;- freshwaterNH3limit(stationData, trout = ifelse(unique(stationData$CLASS) %in% c(&#39;V&#39;,&#39;VI&#39;), TRUE, FALSE), mussels = TRUE, earlyLife = TRUE) # https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ states the assumption is that # waters are to be assessed with the assumption that mussels and early life stages of fish should be present # trout presence is determined by WQS class, this can be changed in the app but is forced to be what the station # is attributed to in the automated assessment scripts # PWS Criteria if(nrow(stationData) &gt; 0){ if(is.na(unique(stationData$PWS)) ){ PWSconcat &lt;- tibble(PWS= NA) } else { PWSconcat &lt;- cbind(#tibble(STATION_ID = unique(stationData$FDT_STA_ID)), assessPWS(stationData, NITRATE_mg_L, LEVEL_NITRATE, 10, &#39;PWS_Nitrate&#39;), assessPWS(stationData, CHLORIDE_mg_L, LEVEL_CHLORIDE, 250, &#39;PWS_Chloride&#39;), assessPWS(stationData, SULFATE_TOTAL_mg_L, LEVEL_SULFATE_TOTAL, 250, &#39;PWS_Total_Sulfate&#39;)) %&gt;% dplyr::select(-ends_with(&#39;exceedanceRate&#39;)) } # chloride assessment if data exists if(nrow(filter(stationData, !is.na(CHLORIDE_mg_L))) &gt; 0){ chlorideFreshwater &lt;- rollingWindowSummary( annualRollingExceedanceSummary( annualRollingExceedanceAnalysis(chlorideFreshwaterAnalysis(stationData), yearsToRoll = 3, aquaticLifeUse = TRUE) ), &quot;CHL&quot;) } else {chlorideFreshwater &lt;- tibble(CHL_EXC = NA, CHL_STAT= NA)} # Water toxics combination with PWS, Chloride Freshwater, and water column PCB data if(nrow(bind_cols(PWSconcat, chlorideFreshwater, PCBmetalsDataExists(filter(markPCB, str_detect(SampleMedia, &#39;Water&#39;)) %&gt;% filter(StationID %in% stationData$FDT_STA_ID), &#39;WAT_TOX&#39;)) %&gt;% dplyr::select(contains(c(&#39;_EXC&#39;,&#39;_STAT&#39;))) %&gt;% mutate(across( everything(), as.character)) %&gt;% pivot_longer(cols = contains(c(&#39;_EXC&#39;,&#39;_STAT&#39;)), names_to = &#39;parameter&#39;, values_to = &#39;values&#39;, values_drop_na = TRUE) %&gt;% filter(! str_detect(values, &#39;WQS info missing from analysis&#39;)) %&gt;% filter(! values == &quot;S&quot;)) &gt;= 1) { WCtoxics &lt;- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = &#39;Review&#39;) } else { WCtoxics &lt;- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = NA)} } else { WCtoxics &lt;- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = NA) } # If data exists for station, run it, otherwise just output what last cycle said and comment if(nrow(stationData) &gt; 0){ # Nutrients based on station type # Nutrient: TP (lakes have real standards; riverine no longer uses 0.2 mg/L as an observed effect for Aquatic life use) if(unique(stationData$lakeStation) == TRUE){ TP &lt;- TP_Assessment(stationData) } else { TP &lt;- countNutrients(stationData, PHOSPHORUS_mg_L, LEVEL_PHOSPHORUS, NA) %&gt;% quickStats(&#39;NUT_TP&#39;) %&gt;% mutate(NUT_TP_STAT = ifelse(NUT_TP_STAT != &quot;S&quot;, &quot;Review&quot;, NA)) } # flag OE but don&#39;t show a real assessment decision # Nutrients: Chl a (lakes) if(unique(stationData$lakeStation) == TRUE){ chla &lt;- chlA_Assessment(stationData) #tibble(NUT_CHLA_EXC = NA, NUT_CHLA_SAMP = NA, NUT_CHLA_STAT = NA) # placeholder for now } else { chla &lt;- countNutrients(stationData, CHLOROPHYLL_A_ug_L, LEVEL_CHLOROPHYLL_A, NA) %&gt;% quickStats(&#39;NUT_CHLA&#39;) %&gt;% mutate(NUT_CHLA_STAT = NA) } # don&#39;t show a real assessment decision # Run DO Daily Avg for everyone! DO_Daily_Avg_STAT &lt;- paste0(&#39;DO_Daily_Avg_STAT: &#39;, DO_Assessment_DailyAvg(stationData) %&gt;% quickStats(&#39;DO_Daily_Avg&#39;) %&gt;% dplyr::select(DO_Daily_Avg_STAT) %&gt;% pull())#} results &lt;- cbind( StationTableStartingData(stationData), tempExceedances(stationData) %&gt;% quickStats(&#39;TEMP&#39;), DOExceedances_Min(stationData) %&gt;% quickStats(&#39;DO&#39;), pHExceedances(stationData) %&gt;% quickStats(&#39;PH&#39;), bacteriaAssessmentDecisionClass( # NEW for IR2024, bacteria only assessed in two most recent years of assessment period filter(stationData, between(FDT_DATE_TIME, assessmentPeriod[1] + years(4), assessmentPeriod[2])), uniqueStationName = unique(stationData$FDT_STA_ID)), # Ammonia rollingWindowSummary( annualRollingExceedanceSummary( annualRollingExceedanceAnalysis(ammoniaAnalysisStation, yearsToRoll = 3, aquaticLifeUse = FALSE)), parameterAbbreviation = &quot;AMMONIA&quot;), # Flag for whether or not metals data exists metalsData(filter(WCmetals, Station_Id %in% stationData$FDT_STA_ID), &#39;WAT_MET&#39;), # Mark&#39;s water column PCB results, flagged WCtoxics, # from above, adds in PWS and water column PCB information # Roger&#39;s sediment metals analysis, transcribed metalsData(filter(Smetals, Station_Id %in% stationData$FDT_STA_ID), &#39;SED_MET&#39;), # Mark&#39;s sediment PCB results, flagged PCBmetalsDataExists(filter(markPCB, str_detect(SampleMedia, &#39;Sediment&#39;)) %&gt;% filter(StationID %in% stationData$FDT_STA_ID), &#39;SED_TOX&#39;), # Gabe&#39;s fish metals results, flagged PCBmetalsDataExists(filter(fishMetals, Station_ID %in% stationData$FDT_STA_ID), &#39;FISH_MET&#39;), # Gabe&#39;s fish PCB results, flagged PCBmetalsDataExists(filter(fishPCB, `DEQ rivermile` %in% stationData$FDT_STA_ID), &#39;FISH_TOX&#39;), # Benthics benthicAssessment(stationData, VSCIresults), # Nutrient Assessment done above by waterbody type TP, chla) %&gt;% # COMMENTS mutate(COMMENTS = paste0(DO_Daily_Avg_STAT) ) %&gt;% left_join(comments, by = &#39;STATION_ID&#39;) %&gt;% left_join(lowFlowSummary, by = &#39;STATION_ID&#39;) %&gt;% dplyr::select(-ends_with(c(&#39;exceedanceRate&#39;, &#39;VERBOSE&#39;, &#39;Assessment Decision&#39;, &#39;StationID&#39;))) %&gt;% # to match Bulk Upload template but helpful to keep visible til now for testing mutate(`Date Last Sampled` = dateLastSampled) } else {# pull what you can from last cycle and flag as carry over results &lt;- filter(stationTable, STATION_ID == stationTable$STATION_ID[i]) %&gt;% dplyr::select(STATION_ID:VAHU6, COMMENTS) %&gt;% mutate(COMMENTS = &#39;This station has no data in current window but was carried over due to IM in one of the 2020IR status fields or the 2020 stations table reports the station was carried over from a previous cycle.&#39;) %&gt;%#, #BACTERIA_COMMENTS = NA) %&gt;% left_join(comments, by = &#39;STATION_ID&#39;) %&gt;% mutate(`Date Last Sampled` = dateLastSampled) } stationTableResults &lt;- bind_rows(stationTableResults,results) ammoniaAnalysis &lt;- bind_rows(ammoniaAnalysis, tibble(StationID = unique(stationData$FDT_STA_ID), AmmoniaAnalysis = list(ammoniaAnalysisStation))) } stationTableResults &lt;- bind_rows(stationsTemplate, stationTableResults) %&gt;% # for now bc bacteria needs help still dplyr::select(STATION_ID:`7Q10 Flag`) timeDiff = Sys.time()- startTime "],["individual-parameter-analyses.html", "3.5 Individual Parameter Analyses", " 3.5 Individual Parameter Analyses The functions detailed in this section are called in either the automated assessment workflow report or by waterbody-specific shiny applications that unpack the output of the automated assessment workflow report. All of the automated assessment functions are stored in a single .R file that is shared among the waterbody-specific shiny assessment applications and Rmarkdown report to minimize the labor involved in maintaining these critical functions. One day, these functions will be published as an R package, but until then, you can access the latest version of this script here. 3.5.1 How to Test Individual Parameter Functions The following code snippet will allow you to generate the appropriate stationData required for most function arguments. The below snippet will only run if you have run all the required data gathering and manipulation steps identified in the Automated Assessment Analysis section. The chunk below is set up such that you can easily change test stations by updating your station object. # any station included in the stationTable object can be specified here, whether or not there is data for the station station &lt;- &#39;1AACO014.57&#39; # other potential examples: &#39;2-JKS023.61&#39;, &#39;1ABAR037.84&#39; # set up stationData object to run through individual parameter functions stationData &lt;- filter(conventionals, FDT_STA_ID %in% station)%&gt;% left_join(stationTable, by = c(&#39;FDT_STA_ID&#39; = &#39;STATION_ID&#39;)) %&gt;% # Special Standards Correction step. This is done on the actual data bc some special standards have temporal components pHSpecialStandardsCorrection() %&gt;% # correct pH to special standards where necessary temperatureSpecialStandardsCorrection() %&gt;% # correct temperature special standards where necessary # special lake steps {if(station %in% lakeStations$STATION_ID) suppressWarnings(suppressMessages( mutate(., lakeStation = TRUE) %&gt;% thermoclineDepth())) # adds thermocline information and SampleDate else mutate(., lakeStation = FALSE) } 3.5.1.1 Why would you want to test individual functions? New for the IR2024 cycle, all data, methods, and results are available for regional assessment staff (or any DEQ staff) to interrogate, for riverine and lacustrine waterbody types and some estuarine methods (stay tuned on improvements on this waterbody type). The agency has been working through technology hurdles to achieve full transparency for assessment processes for a number of years, and while we arent fully there yet, we are getting closer. This bookdown is written as an accompanying handbook to all automated assessment processes to decipher the process for beginner R users. One of the goals of the WQA program is that anyone with access to the assessment data and analytical scripts can run the automated assessment processes and come to the same results. By consistently applying the same rules and logic to assessment data, we can ensure the assessment decisions are universally applied across the state. Lastly, while we hope the next section containing plain English, narrative descriptions of the automated assessment functions provide sufficient explanation as to what and why analytical steps were taken, should you run into a question, the chunk above and the functions below will allow any user to dig into any function to fully understand each process. We encourage you to investigate these functions in a stepwise manner. Should you run into problems, please reach out to the WQA Technical Support Team. 3.5.2 Temperature A maximum temperature criteria applies to stations depending on their associated WQS. The tempExceedances() function identifies any temperature exceedances of the Max Temperature (C) field for each sample provided. The function removes all Level I and Level II data and missing data before rounding measure to even and comparing against the provided criteria. An internal 7Q10 Flag is passed through this function for use in the quickStats() function should a user adjust the default drop7Q10 argument. Read more about the quickStats() helper function in the Additional Functions section. #Max Temperature Exceedance Function tempExceedances &lt;- function(stationData){ dplyr::select(stationData, FDT_DATE_TIME, FDT_DEPTH, tidyselect::contains(&#39;TEMP_CELCIUS&#39;), `Max Temperature (C)`, `7Q10 Flag`) %&gt;% # Just get relevant columns dplyr::filter(! (LEVEL_FDT_TEMP_CELCIUS %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out dplyr::filter(! is.na(FDT_TEMP_CELCIUS)) %&gt;% # get rid of NA&#39;s # rename columns to make exceedance analyses easier to apply dplyr::rename(parameter = !! names(.[3]), limit = !! names(.[6])) %&gt;% # Apply Round to Even Rule before testing for exceedances dplyr::mutate(parameterRound = signif(parameter, digits = 2), # two significant figures based on WQS https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section50/ exceeds = case_when(parameterRound &gt; limit ~ TRUE, # Identify where above max Temperature, parameterRound &lt;= limit ~ FALSE, # no exceedance TRUE ~ NA)) } # Example Usage: # where stationData is already in your environment, see Automated Assessment for environment set up help # tempExceedances(stationData) # tempExceedances(stationData) %&gt;% # quickStats(&#39;TEMP&#39;) # pass results to quickStats() to consolidate results 3.5.3 Dissolved Oxygen There are two general dissolved oxygen (DO) criteria that may apply to stations, depending on their associated WQS. The minimum DO criteria are evaluated against the provided Dissolved Oxygen Min (mg/L) using the DOExceedances_Min() function. This function handles lake stations differently than other waterbody types. If a station is in a Section 187 lake, then only the epilimnion and unstratified samples are evaluated. Thermocline information is evaluated using the thermoclineDepth() function in a previous step. Regardless of the waterbody type, all Level I and Level II data and missing data are removed before measures are rounded to even and compared against the provided criteria. An internal 7Q10 Flag is passed through this function for use in the quickStats() function should a user adjust the default drop7Q10 argument. Read more about the quickStats() helper function in the Additional Functions section. # Minimum DO Exceedance function DOExceedances_Min &lt;- function(stationData){ # special step for lake stations, remove samples based on lake assessment guidance if(unique(stationData$lakeStation) == TRUE){ if(!is.na(unique(stationData$Lakes_187B)) &amp; unique(stationData$Lakes_187B) == &#39;y&#39;){ DOdata &lt;- dplyr::filter(stationData, LakeStratification %in% c(&quot;Epilimnion&quot;, NA)) %&gt;% # only use epilimnion or unstratified samples for analysis dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, DO_mg_L, RMK_DO, LEVEL_DO, `Dissolved Oxygen Min (mg/L)`, LakeStratification, `7Q10 Flag`) # Just get relevant columns, } else { DOdata &lt;- dplyr::select(stationData, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, DO_mg_L, RMK_DO, LEVEL_DO, `Dissolved Oxygen Min (mg/L)`, LakeStratification, `7Q10 Flag`) }# Just get relevant columns, } else { DOdata &lt;- dplyr::select(stationData, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, DO_mg_L, RMK_DO, LEVEL_DO, `Dissolved Oxygen Min (mg/L)`, `7Q10 Flag`) # Just get relevant columns, } DOdata %&gt;% dplyr::filter(!(LEVEL_DO %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out dplyr::filter(!is.na(DO_mg_L)) %&gt;% dplyr::rename(parameter = !!names(.[4]), limit = !!names(.[7])) %&gt;% # rename columns to make functions easier to apply # Round to Even Rule dplyr::mutate(parameterRound = signif(parameter, digits = 2), # two significant figures based on https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section50/ exceeds = case_when(parameterRound &lt; limit ~ TRUE, # Identify where below min DO parameterRound &gt;= limit ~ FALSE, # no exceedance TRUE ~ NA)) } # Example Usage: # where stationData is already in your environment, see Automated Assessment for environment set up help # DOExceedances_Min(stationData) # DOExceedances_Min(stationData) %&gt;% # quickStats(&#39;DO&#39;) # pass results to quickStats() to consolidate results A separate function called DO_Assessment_DailyAvg() evaluates whether or not a station exceeds the DO daily average criteria, provided in the Dissolved Oxygen Daily Avg (mg/L) field. This analysis is only run if the number of samples for a station in a given date are greater than one. This function handles lake stations differently than other waterbody types. If a station is in a Section 187 lake, then only the epilimnion and unstratified samples are evaluated. Thermocline information is evaluated using the thermoclineDepth() function in a previous step. Regardless of the waterbody type, all Level I and Level II data and missing data are removed before measures are rounded to even and compared against the provided criteria. An internal 7Q10 Flag is passed through this function for use in the quickStats() function should a user adjust the default drop7Q10 argument. Read more about the quickStats() helper function in the Additional Functions section. # Daily Average exceedance function DO_Assessment_DailyAvg &lt;- function(stationData){ # special step for lake stations, remove samples based on lake assessment guidance if(unique(stationData$lakeStation) == TRUE){ if(!is.na(unique(stationData$Lakes_187B)) &amp; unique(stationData$Lakes_187B) == &#39;y&#39;){ DOdata &lt;- dplyr::filter(stationData, LakeStratification %in% c(&quot;Epilimnion&quot;, NA)) %&gt;% # only use epilimnion or unstratified samples for analysis dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, DO_mg_L, RMK_DO, LEVEL_DO, `Dissolved Oxygen Daily Avg (mg/L)`, LakeStratification, `7Q10 Flag`) # Just get relevant columns, } else { DOdata &lt;- dplyr::select(stationData, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, DO_mg_L, RMK_DO, LEVEL_DO, `Dissolved Oxygen Daily Avg (mg/L)`, LakeStratification, `7Q10 Flag`) }# Just get relevant columns, } else { DOdata &lt;- dplyr::select(stationData, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, DO_mg_L, RMK_DO, LEVEL_DO, `Dissolved Oxygen Daily Avg (mg/L)`, `7Q10 Flag`) # Just get relevant columns, } DOdata %&gt;% dplyr::filter(!(LEVEL_DO %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out dplyr::filter(!is.na(DO_mg_L)) %&gt;% #get rid of NA&#39;s dplyr::mutate(date = as.Date(FDT_DATE_TIME, format=&quot;%m/%d/%Y&quot;), limit = `Dissolved Oxygen Daily Avg (mg/L)`) %&gt;% dplyr::group_by(date) %&gt;% dplyr::mutate(n_Samples_Daily = n()) %&gt;% # how many samples per day? dplyr::filter(n_Samples_Daily &gt; 1) %&gt;% # Daily average with average rounded to even dplyr::mutate(DO_DailyAverage = signif(mean(DO_mg_L), digits = 2), # two significant figures based on https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section50/ exceeds = case_when(DO_DailyAverage &lt; limit &amp; is.na(`7Q10 Flag`) ~ TRUE, # exceedance if above limit and no 7Q10 Flag DO_DailyAverage &lt; limit &amp; `7Q10 Flag` == &quot;7Q10 Flag&quot; ~ FALSE, # exceedance if above limit and 7Q10 Flag DO_DailyAverage &gt;= limit ~ FALSE, # no exceedance TRUE ~ NA)) %&gt;% #ifelse(DO_DailyAverage &lt; `Dissolved Oxygen Daily Avg (mg/L)`,T,F)) %&gt;% dplyr::ungroup() %&gt;% distinct(date, .keep_all = T) %&gt;% dplyr::select(-c(FDT_DATE_TIME)) } # Example Usage: # where stationData is already in your environment, see Automated Assessment for environment set up help # DO_Assessment_DailyAvg(stationData) # DO_Assessment_DailyAvg(stationData) %&gt;% # quickStats(&#39;DO_Daily_Avg&#39;) # pass results to quickStats() to consolidate results 3.5.4 pH A pH range criteria applies to stations depending on their associated WQS. The pHExceedances() function identifies any pH values outside of the range specified in the associated pH Min and pH Max fields. This function handles lake stations differently than other waterbody types. If a station is in a Section 187 lake, then only the epilimnion and unstratified samples are evaluated. Regardless of the waterbody type, the function removes all Level I and Level II data and missing data before rounding measures to even and comparing against the provided criteria. An internal 7Q10 Flag is passed through this function for use in the quickStats() function should a user adjust the default drop7Q10 argument. Read more about the quickStats() helper function in the Additional Functions section. # pH range Exceedance Function pHExceedances &lt;- function(stationData){ # special step for lake stations, remove samples based on lake assessment guidance if(unique(stationData$lakeStation) == TRUE){ if(!is.na(unique(stationData$Lakes_187B)) &amp; unique(stationData$Lakes_187B) == &#39;y&#39;){ pHdata &lt;- filter(stationData, LakeStratification %in% c(&quot;Epilimnion&quot;, NA)) %&gt;% # only use epilimnion or unstratified samples for analysis dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, FDT_FIELD_PH, RMK_FDT_FIELD_PH, LEVEL_FDT_FIELD_PH, `pH Min`, `pH Max`, LakeStratification, `7Q10 Flag`) # Just get relevant columns, } else { pHdata &lt;- dplyr::select(stationData, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, FDT_FIELD_PH, RMK_FDT_FIELD_PH, LEVEL_FDT_FIELD_PH, `pH Min`, `pH Max`, LakeStratification, `7Q10 Flag`) }# Just get relevant columns, } else { pHdata &lt;- dplyr::select(stationData, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, FDT_FIELD_PH, RMK_FDT_FIELD_PH, LEVEL_FDT_FIELD_PH, `pH Min`, `pH Max`, `7Q10 Flag`) }# Just get relevant columns, pHdata &lt;- filter(pHdata, !(LEVEL_FDT_FIELD_PH %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out filter(!is.na(FDT_FIELD_PH)) #get rid of NA&#39;s # only run analysis if WQS exist for station if(any(is.na(pHdata$`pH Min`)) | any(is.na(pHdata$`pH Max`))){ pH &lt;- mutate(pHdata, interval = 1, exceeds = FALSE, limit = `pH Min`) # placeholder to run quickStats() without any WQS } else { pH &lt;- pHdata %&gt;% rowwise() %&gt;% # Round to Even Rule mutate(parameterRound = signif(FDT_FIELD_PH, digits = 2)) %&gt;% # two significant figures based on WQS https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section50/ mutate(interval=findInterval(parameterRound,c(`pH Min`,`pH Max`), left.open=TRUE, rightmost.closed = TRUE)) %&gt;% # Identify where pH outside of assessment range with round to even ungroup()%&gt;% mutate(exceeds = case_when(interval != 1 ~ TRUE, # Highlight where pH doesn&#39;t fall into criteria range interval == 1 ~ FALSE, # no exceedance if inside limit TRUE ~ NA), limit = `pH Min`) } return(pH) } # Example Usage: # where stationData is already in your environment, see Automated Assessment for environment set up help # pHExceedances(stationData) # pHExceedances(stationData) %&gt;% # quickStats(&#39;PH&#39;) # pass results to quickStats() to consolidate results 3.5.5 Bacteria Recreational bacteria is a more complicated standard to apply to monitoring data due to the unique rolling windows. To standardize the logic across E.coli and enterococci data, the nested function architecture that evaluates the bacteria data is programmed to handle either E.coli or enterococci data. The three functions required to calculate the bacteria criteria are detailed below. bacteriaLogic 3.5.5.1 bacteriaExceedances_NEW The bacteriaExceedances_NEW() function is the innermost function in the nested bacteria function logic. This is the analytical function that applies the bacteria criteria logic. This function takes in all station data, the designated bacteriaField and bacteriaRemark field arguments to tell the function which type of bacteria to assess (E.coli or enterococci), a sampleRequirement argument (minimum n samples in 90 day window needed to apply geomean), a STV argument (threshold for E.coli or enterococci), and a geomeanCriteria argument (threshold for E.coli or enterococci). The function is applied inside the bacteriaAssessmentDecision() function to calculate unique 90 day data windows, apply STV and geomean criteria where appropriate, and report out the results of this analysis for summarization and language standardization by bacteriaAssessmentDecision(). To ease later data visualization steps, a listcolumn named associatedData is included in the function output that contains the raw data that falls within each 90 day window the user can unpack to see all data associated with each 90 day window. # Function to assess on 90 day windows across input dataset # really just a building block, one probably wouldn&#39;t run this function independently bacteriaExceedances_NEW &lt;- function(stationData, # input dataframe with bacteria data bacteriaField, # name of bacteria field data bacteriaRemark, # name of bacteria comment field sampleRequirement, # minimum n samples in 90 day window needed to apply geomean STV, # unique for ecoli/enter geomeanCriteria # unique for ecoli/enter ){ # Output tibble to organize results, need list object to save associate data out &lt;- tibble(`StationID` = as.character(NA), `Date Window Starts` = as.Date(NA), `Date Window Ends` = as.Date(NA), `Last Sample Date in Window` = as.Date(NA), `Samples in 90 Day Window` = as.numeric(NA), `STV Exceedances In Window` = as.numeric(NA), `STV Exceedance Rate` = as.numeric(NA), `STV Assessment` = as.character(NA), `Geomean In Window` = as.numeric(NA), `Geomean Assessment` = as.character(NA), associatedData = list()) # Data reorg to enable both types of bacteria assessment from a single function stationData2 &lt;- dplyr::select(stationData, FDT_STA_ID, FDT_DATE_TIME, !! bacteriaField, !! bacteriaRemark) %&gt;% rename(Value = bacteriaField, LEVEL_Value = bacteriaRemark) %&gt;% filter(! LEVEL_Value %in% c(&#39;Level II&#39;, &#39;Level I&#39;)) %&gt;% # get lower levels out filter(!is.na(Value)) if(nrow(stationData2) &gt; 0){ # Loop through each row of input df to test 90 day windows against assessment criteria for( i in 1 : nrow(stationData2)){ time1 &lt;- as.Date(stationData2$FDT_DATE_TIME[i]) timePlus89 &lt;- time1 + days(89) # Organize prerequisites to decision process windowData &lt;- filter(stationData2, as.Date(FDT_DATE_TIME) &gt;= time1 &amp; as.Date(FDT_DATE_TIME) &lt;= timePlus89) %&gt;% mutate(nSamples = n(), # count number of samples in 90 day window STVhit = ifelse(Value &gt; STV, TRUE, FALSE), # test values in window against STV geomean = ifelse(nSamples &gt; 1, # calculate geomean of samples if nSamples&gt;1 as.numeric(round::roundAll(EnvStats::geoMean(Value, na.rm = TRUE), digits=0, &quot;r0.C&quot;)), # round to nearest whole number per Memo to Standardize Rounding for Assessment Guidance NA), geomeanCriteriaHit = ifelse(geomean &gt; geomeanCriteria, TRUE, FALSE)) # test round to even geomean against geomean Criteria # First level of testing: any STV hits in dataset? Want this information for all scenarios nSTVhitsInWindow &lt;- nrow(filter(windowData, STVhit == TRUE)) # STV exceedance rate calculation with round to even math STVexceedanceRate &lt;- ifelse(unique(windowData$nSamples) &gt;= 10, as.numeric(round::roundAll((nSTVhitsInWindow / unique(windowData$nSamples)) * 100,digits=0, &quot;r0.C&quot;)), # round to nearest whole number per Memo to Standardize Rounding for Assessment Guidance NA) # no STV exceedance rate if &lt; 10 samples if(nSTVhitsInWindow == 0){ `STV Assessment` &lt;- &#39;No STV violations within 90 day window&#39; } if(nSTVhitsInWindow == 1){ `STV Assessment` &lt;- paste(nSTVhitsInWindow, &#39; STV violation(s) with &#39;, format(STVexceedanceRate, digits = 3), &#39;% exceedance rate in 90 day window | Insufficient Information (Prioritize for follow up monitoring)&#39;,sep=&#39;&#39;)} if(nSTVhitsInWindow &gt;= 2){ `STV Assessment` &lt;- paste(nSTVhitsInWindow, &#39; STV violation(s) with &#39;, format(STVexceedanceRate, digits = 3), &#39;% exceedance rate in 90 day window | Impaired: &#39;, nSTVhitsInWindow,&#39; hits in the same 90-day period&#39;,sep=&#39;&#39;) } # Second level of testing: only if minimum geomean sampling requirements met in 90 day period if(unique(windowData$nSamples) &gt;= sampleRequirement){ # Geomean Hit if(unique(windowData$geomeanCriteriaHit) == TRUE){ `Geomean Assessment` &lt;- paste(&#39;Geomean: &#39;, format(unique(windowData$geomean), digits = 3), &#39; | Impaired: geomean exceeds criteria in the 90-day period&#39;, sep=&#39;&#39;) } else{ `Geomean Assessment` &lt;- paste(&#39;Geomean: &#39;, format(unique(windowData$geomean), digits = 3), &#39; | Geomean criteria met, hold assessment decision for further testing&#39;, sep= &#39;&#39;)} } else { # minimum geomean sampling requirements NOT met in 90 day period `Geomean Assessment` &lt;- &#39;Insufficient Information: geomean sampling criteria not met&#39; } out[i,] &lt;- tibble(`StationID` = unique(stationData2$FDT_STA_ID), `Date Window Starts` = time1, `Date Window Ends` = timePlus89, `Last Sample Date in Window` = max(windowData$FDT_DATE_TIME), `Samples in 90 Day Window` = unique(windowData$nSamples), `STV Exceedances In Window` = nSTVhitsInWindow, `STV Exceedance Rate` = STVexceedanceRate, `STV Assessment` = `STV Assessment`, `Geomean In Window` = ifelse(unique(windowData$nSamples) &gt;= sampleRequirement, unique(windowData$geomean), NA), # avoid excitement, only give geomean result if 10+ samples `Geomean Assessment` = `Geomean Assessment`, associatedData = list(windowData)) } #end for loop } else { out &lt;- tibble(`StationID` = unique(stationData$FDT_STA_ID), `Date Window Starts` = as.Date(NA), `Date Window Ends` = as.Date(NA), `Last Sample Date in Window` = as.Date(NA), `Samples in 90 Day Window` = as.numeric(NA), `STV Exceedances In Window` = as.numeric(NA), `STV Exceedance Rate` = as.numeric(NA), `STV Assessment` = as.character(NA), `Geomean In Window` = as.numeric(NA), `Geomean Assessment` = as.character(NA), associatedData = list(NA)) } # For IR2024, we only want to make assessment decisions on data windows with unique datasets. Running distinct() on `Last Sample Date in Window` # will keep only the rows with the first occurrence of any given sample date, allowing us to remove rolled windows that contain duplicated # datasets. Joining this result back to the out dataset allows us to flag `Valid Assessment Window` appropriately. Since this function # feeds the app, we want to keep all records and only at later functions that make assessment decisions do we want to filter out # non-unique windows. distinctWindows &lt;- distinct(out, `Last Sample Date in Window`, .keep_all = T) %&gt;% mutate(`Valid Assessment Window` = TRUE) out &lt;- left_join(out, distinctWindows, by = c(&quot;StationID&quot;, &quot;Date Window Starts&quot;, &quot;Date Window Ends&quot;, &quot;Last Sample Date in Window&quot;, &quot;Samples in 90 Day Window&quot;, &quot;STV Exceedances In Window&quot;, &quot;STV Exceedance Rate&quot;, &quot;STV Assessment&quot;, &quot;Geomean In Window&quot;, &quot;Geomean Assessment&quot;, &quot;associatedData&quot;)) %&gt;% # join by everything to be safe dplyr::select(StationID:`Last Sample Date in Window`, `Valid Assessment Window`, everything()) return(out) } # Example Usage: # where stationData is already in your environment, see Automated Assessment for environment set up help #bacteriaExceedances_NEW(stationData, &#39;ECOLI&#39;, &#39;LEVEL_ECOLI&#39;, 10, 410, 126) #bacteriaExceedances_NEW(stationData, &#39;ENTEROCOCCI&#39;, &#39;LEVEL_ENTEROCOCCI&#39;, 10, 130, 35) # How to unpack raw data from inside each unqiue 90 day window # y &lt;- bacteriaExceedances_NEW(stationData, &#39;ECOLI&#39;, &#39;LEVEL_ECOLI&#39;, 10, 410, 126) # View(y) # look at this data structure # y$associatedData[1] # look at first 90 day window associated data 3.5.5.2 bacteriaAssessmentDecision The bacteriaAssessmentDecision() function is the workhorse of the bacteria assessment logic, summarizing bacteria assessment results into decisions. This function passes through the provided arguments to the bacteriaExceedances_NEW() function to perform the bacteria criteria analysis. The function is applied inside the bacteriaAssessmentDecisionClass() function to summarize and report out the results of the bacteriaExceedances_NEW() analysis. To ease later data visualization steps, a listcolumn named associatedDecisionData is included in the function output that the user can unpack to see all data associated with each 90 day window. # Function to summarize bacteria assessment results into decisions # This function returns all potential issues with priory on geomean results IF there # are enough samples to run geomean # Round to even rules are applied bacteriaAssessmentDecision &lt;- function(stationData, # input dataframe with bacteria data bacteriaField, # name of bacteria field data bacteriaRemark, # name of bacteria comment field sampleRequirement, # minimum n samples in 90 day window needed to apply geomean STV, # unique for ecoli/enter geomeanCriteria # unique for ecoli/enter ){ # Rename output columns based on station table template stationTableName &lt;- ifelse(bacteriaField == &#39;ECOLI&#39;, &quot;ECOLI&quot;, &quot;ENTER&quot;) nSamples &lt;- select(stationData, Value = {{ bacteriaField }} ) %&gt;% filter(!is.na(Value)) # total n samples taken in assessment window if(nrow(nSamples) &gt; 0){ # only proceed through decisions if there is data to be analyzed # Run assessment function # make two objects here because we want to base all decisions only on valid data, but we also want to output all associated data from # this function, so we create a rawAnalysisForOutput to be saved in a list column for unpacking later rawAnalysisForOutput &lt;- suppressWarnings(bacteriaExceedances_NEW(stationData, bacteriaField, bacteriaRemark, sampleRequirement, STV, geomeanCriteria) ) validForAssessment &lt;- rawAnalysisForOutput %&gt;% filter(`Valid Assessment Window` == TRUE) # bail out if no data to analyze bc all Level II or Level I, OR (new for IR2024) no valid windows for assessment if(nrow(filter(validForAssessment, !is.na(`Date Window Starts`))) == 0 ){ return(tibble(StationID = unique(stationData$FDT_STA_ID), `_EXC` = NA, # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples #`_IMPAIREDWINDOWS` = NA, `_SAMP` = NA, `_GM.EXC` = NA, `_GM.SAMP` = NA, `_STAT` = NA, # is this the right code??? `_STAT_VERBOSE` = NA, `BACTERIADECISION` = NA, `BACTERIASTATS` = NA, associatedDecisionData = list(NA)) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } # number of STV exceedances, reported in bacteria_EXC field in stations table and useful for logic testing # don&#39;t want to do this analysis on validForAssessment bc sum(validForAssessment$`STV Exceedances In Window`) will overshoot real n exceedSTVn &lt;- select(stationData, Value = {{ bacteriaField }} ) %&gt;% filter(Value &gt; STV) # total STV exceedances in dataset # Windows with &gt; 10% STV rate, these can only be calculated on windows with 10 or more samples exceedSTVrate &lt;- filter(validForAssessment, `STV Exceedance Rate` &gt; 10) # windows with geomean exceedances, these can only be calculated on windows with 10 or more samples exceedGeomean &lt;- filter(validForAssessment, `Geomean In Window` &gt; geomeanCriteria) # Decision logic time, work through geomean first and if there is no appropriate geomean data (no windows with 10+ samples) # then go to STV assessment # Were at least 10 samples taken within any 90-day period of the assessment window? if( any(!is.na(validForAssessment$`Geomean In Window`)) ){ # Were at least 10 samples taken within any 90-day period of the assessment window? - Yes # Do the geometric means calculated for the 90-day periods represented by 10+ samples meet the GM criterion? if( nrow(exceedGeomean) == 0){ # Do the geometric means calculated for the 90-day periods represented by 10+ samples meet the GM criterion? - Yes # Do any of the 90-day periods of the assessment window represented in the dataset exceed the 10% STV Exceedance Rate? if( nrow(exceedSTVn) &gt; 0){ # Do any of the 90-day periods of the assessment window represented in the dataset exceed the 10% STV Exceedance Rate? - Yes # Yes, in a 90-day period represented by 10+ samples if(nrow(filter(exceedSTVrate, `Samples in 90 Day Window` &gt;= 10 &amp; `STV Exceedance Rate` &gt; 10)) &gt; 0){ # STV exceedances in a 90-day period represented by &gt;= 10 samples return(tibble(StationID = unique(validForAssessment$StationID), `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = nrow(exceedGeomean), `_GM.SAMP` = nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), `_STAT` = &quot;IM&quot;, `_STAT_VERBOSE` = &quot;Impaired - 2 or more STV exceedances in the same 90-day period represented by 10+ samples, no geomean exceedances.&quot;,#STV exceedances in a 90-day period represented by &gt;= 10 samples after verifying geomean passes where applicable.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } else { # STV exceedances in a 90-day period represented by &lt; 10 samples # 2 or more hits in the same 90-day period? if(any(validForAssessment$`STV Exceedances In Window` &gt;= 2) ){ return(tibble(StationID = unique(validForAssessment$StationID), `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = nrow(exceedGeomean), `_GM.SAMP` = nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), `_STAT` = &quot;IM&quot;, `_STAT_VERBOSE` = &quot;Impaired- 2 or more STV exceedances in the same 90-day period with &lt; 10 samples, no geomean exceedances.&quot;, #2 or more STV hits in the same 90-day period with &lt; 10 samples after verifying geomean passes where applicable.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } else { # did the STV exceedance(s) occur in windows with 10+ samples? if(all(filter(validForAssessment, `STV Exceedances In Window` &gt; 0)$`Samples in 90 Day Window` &gt;= 10)){ return(tibble(StationID = unique(validForAssessment$StationID), `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = nrow(exceedGeomean), `_GM.SAMP` = nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), `_STAT` = &quot;S&quot;, `_STAT_VERBOSE` = &quot;Fully Supporting - No STV exceedance rates &gt;10% or geomean exceedances in any 90-day period represented by 10+ samples.&quot;,# No geomean exceedances and STV exceedance(s) in one or multiple 90-day periods represented by 10+ samples.&quot;, # previous language: 1 STV hit in one or multiple 90-day periods with &lt; 10 samples after verifying geomean passes where applicable.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } else {# STV exceedance(s) occured in windows with &lt; 10 samples # 1 hit in one or multiple 90-day periods after verifying geomean passes where applicable return(tibble(StationID = unique(validForAssessment$StationID), `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = nrow(exceedGeomean), `_GM.SAMP` = nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), `_STAT` = &quot;O&quot;, `_STAT_VERBOSE` = &quot;Fully Supporting - No geomean exceedances and only 1 STV exceedance in one or multiple 90-day periods represented by &lt; 10 samples.&quot;, # previous language: 1 STV hit in one or multiple 90-day periods with &lt; 10 samples after verifying geomean passes where applicable.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } } } } else { # Do any of the 90-day periods of the assessment window represented in the dataset exceed the 10% STV Exceedance Rate? - No return(tibble(StationID = unique(validForAssessment$StationID), `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = nrow(exceedGeomean), `_GM.SAMP` = nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), `_STAT` = &quot;S&quot;, `_STAT_VERBOSE` = &quot;Fully Supporting - No STV exceedance rates &gt;10% or geomean exceedances in any 90-day period represented by 10+ samples.&quot;, #No STV exceedances or geomean exceedances in any 90-day period.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } } else { # Do the geometric means calculated for the 90-day periods represented by 10+ samples meet the GM criterion? - No return(tibble(StationID = unique(validForAssessment$StationID), `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = nrow(exceedGeomean), `_GM.SAMP` = nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), `_STAT` = &quot;IM&quot;, `_STAT_VERBOSE` = &quot;Impaired- geomean exceedance in any 90-day period.&quot;, #geomean exceedance(s) in any 90-day period with &gt;= 10 samples.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } } else { # Were at least 10 samples taken within any 90-day period of the assessment window? - No # Were there any hits of the STV during the dataset? if( nrow(exceedSTVn) == 0){ # Were there any hits of the STV during the dataset? - No return(tibble(StationID = unique(validForAssessment$StationID), `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = as.numeric(NA), #nrow(exceedGeomean), # Data Entry manual updated to require NA instead of 0 if &lt; 10 samples per 90 day window `_GM.SAMP` = as.numeric(NA), #nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), # Data Entry manual updated to require NA instead of 0 if &lt; 10 samples per 90 day window `_STAT` = &quot;IN&quot;, `_STAT_VERBOSE` = &quot;Insufficient Information (Prioritize for follow up monitoring)- No STV exceedances but insufficient data to analyze geomean.&quot;, #0 STV hits but insufficient data to analyze geomean.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE))%&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } else { # Were there any hits of the STV during the dataset? - Yes # 2 or more hits in the same 90-day period if(any(validForAssessment$`STV Exceedances In Window` &gt;= 2) ){ return(tibble(StationID = unique(validForAssessment$StationID), # not quite right yet `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the number of STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = as.numeric(NA), #nrow(exceedGeomean), # Data Entry manual updated to require NA instead of 0 if &lt; 10 samples per 90 day window `_GM.SAMP` = as.numeric(NA), #nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), # Data Entry manual updated to require NA instead of 0 if &lt; 10 samples per 90 day window `_STAT` = &quot;IM&quot;, `_STAT_VERBOSE` = &quot;Impaired - 2 or more STV hits in the same 90-day period with &lt; 10 samples.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE))%&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } else { # 1 hit in one or multiple 90-day periods return(tibble(StationID = unique(validForAssessment$StationID), `_EXC` = nrow(exceedSTVn), # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = nrow(nSamples), `_GM.EXC` = as.numeric(NA), #nrow(exceedGeomean), # Data Entry manual updated to require NA instead of 0 if &lt; 10 samples per 90 day window `_GM.SAMP` = as.numeric(NA), #nrow(filter(validForAssessment, !is.na(`Geomean In Window`))), # Data Entry manual updated to require NA instead of 0 if &lt; 10 samples per 90 day window `_STAT` = &quot;IN&quot;, `_STAT_VERBOSE` = &quot;Insufficient Information (Prioritize for follow up monitoring)- One STV exceedance in one or multiple 90-day periods but insufficient data to analyze geomean.&quot;,#1 STV hit in one or multiple 90-day periods but insufficient data to analyze geomean.&quot;, `BACTERIADECISION` = paste0(stationTableName, &quot;: &quot;,`_STAT_VERBOSE`), `BACTERIASTATS` = paste0(stationTableName, &quot;: Number of 90 day windows with &gt; 10% STV exceedance rate: &quot;, nrow(exceedSTVrate)), associatedDecisionData = list(rawAnalysisForOutput) ) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } } } # No bacteria data to analyze } else { return(tibble(StationID = unique(stationData$FDT_STA_ID), `_EXC` = NA, # right now this is set to # total STV exceedances, not the # STV exceedances in a 90-day period with 10+ samples `_SAMP` = NA, `_GM.EXC` = NA, `_GM.SAMP` = NA, `_STAT` = NA, # is this the right code??? `_STAT_VERBOSE` = NA, `BACTERIADECISION` = NA, `BACTERIASTATS` = NA, associatedDecisionData = list(NA)) %&gt;% rename_with( ~ gsub(&quot;_&quot;, paste0(stationTableName,&quot;_&quot;), .x, fixed = TRUE)) %&gt;% # fix names to match station table format rename_with( ~ gsub(&quot;.&quot;, &quot;_&quot;, .x, fixed = TRUE)) ) # special step to get around accidentally replacing _GM with station table name } } # Example Usage: # where stationData object is already in your environment, see Automated Assessment for environment set up help # bacteriaAssessmentDecision(stationData, &#39;ECOLI&#39;, &#39;LEVEL_ECOLI&#39;, 10, 410, 126) # bacteriaAssessmentDecision(stationData, &#39;ENTEROCOCCI&#39;, &#39;LEVEL_ENTEROCOCCI&#39;, 10, 130, 35) # To get just info for station table #bacteriaAssessmentDecision(stationData, &#39;ECOLI&#39;, &#39;LEVEL_ECOLI&#39;, 10, 410, 126) %&gt;% # dplyr::select(StationID:ECOLI_STAT) #bacteriaAssessmentDecision(stationData, &#39;ENTEROCOCCI&#39;, &#39;LEVEL_ENTEROCOCCI&#39;, 10, 130, 35) %&gt;% # dplyr::select(StationID:ENTER_STAT) # How to unpack analyzed data from a single site # x &lt;- bacteriaAssessmentDecision(stationData, &#39;ENTEROCOCCI&#39;, &#39;LEVEL_ENTEROCOCCI&#39;, 10, 130, 35) # View(x) # look at this data structure # x$associatedDecisionData[1] # look at each 90 day window used to make the assessment decision 3.5.5.3 bacteriaAssessmentDecisionClass The bacteriaAssessmentDecisionClass() is the outermost function to assess bacteria data. The function takes in all data for a single station as well as a StationID (to provide adequate output information in case no bacteria data exist for said station). Based on the WQS class, E.coli is analyzed if the site is freshwater or E.coli and Enterococci are analyzed if the site is saltwater or transitional. It is up to the regional assessor to determine which bacteria analysis best represents these transitional sites. Regardless of the waterbody type, all Level I and Level II data and missing data are removed before measures are rounded to even and compared against the provided criteria. The output of this function includes all the bacteria columns required in the Station Table for CEDS bulk data upload as well as a verbose comment that is provided to the assessor for standardized inclusion in CEDS WQA. ## outermost function to decide which bacteria should be assessed based on WQS Class bacteriaAssessmentDecisionClass &lt;- function(stationData, # input dataframe with bacteria data uniqueStationName # stationID for output in case no data come into function ){ # stop everything if no data to analyze in stationData if(nrow(stationData) == 0){ return( tibble(StationID = uniqueStationName, ECOLI_EXC = as.numeric(NA), ECOLI_SAMP = as.numeric(NA), ECOLI_GM_EXC = as.numeric(NA), ECOLI_GM_SAMP = as.numeric(NA), ECOLI_STAT = as.character(NA), ECOLI_STATECOLI_VERBOSE = as.character(NA), ENTER_EXC = as.numeric(NA), ENTER_SAMP = as.numeric(NA), ENTER_GM_EXC = as.numeric(NA), ENTER_GM_SAMP = as.numeric(NA), ENTER_STAT = as.character(NA), ENTER_STATENTER_VERBOSE = as.character(NA)) )} # lake stations should only be surface sample if(unique(stationData$lakeStation) == TRUE){ stationData &lt;- filter(stationData, FDT_DEPTH &lt;= 0.3) } if(nrow(stationData) &gt; 0){ if(unique(stationData$CLASS) %in% c(&#39;I&#39;, &#39;II&#39;)){ # previously this was programmed to only output enterococci results for these classes, but assessors requested both outputs to be conservative return( left_join(bacteriaAssessmentDecision(stationData, &#39;ECOLI&#39;, &#39;LEVEL_ECOLI&#39;, 10, 410, 126), bacteriaAssessmentDecision(stationData, &#39;ENTEROCOCCI&#39;, &#39;LEVEL_ENTEROCOCCI&#39;, 10, 130, 35), by = &#39;StationID&#39;) %&gt;% mutate(BACTERIADECISION = paste0(BACTERIADECISION.x, &#39; | &#39;, BACTERIADECISION.y), BACTERIASTATS = paste0(BACTERIASTATS.x, &#39; | &#39;, BACTERIASTATS.y)) %&gt;% dplyr::select(-c(BACTERIADECISION.x, BACTERIADECISION.y, BACTERIASTATS.x, BACTERIASTATS.y, associatedDecisionData.x, associatedDecisionData.y)) ) } else { return( bacteriaAssessmentDecision(stationData, &#39;ECOLI&#39;, &#39;LEVEL_ECOLI&#39;, 10, 410, 126) %&gt;% dplyr::select(StationID:BACTERIASTATS) %&gt;% #ECOLI_STATECOLI_VERBOSE) %&gt;% mutate(ENTER_EXC = as.numeric(NA), ENTER_SAMP = as.numeric(NA), ENTER_GM_EXC = as.numeric(NA), ENTER_GM_SAMP = as.numeric(NA), ENTER_STAT = as.character(NA), ENTER_STATENTER_VERBOSE = as.character(NA)) ) } } else { return( tibble(StationID = uniqueStationName, ECOLI_EXC = as.numeric(NA), ECOLI_SAMP = as.numeric(NA), ECOLI_GM_EXC = as.numeric(NA), ECOLI_GM_SAMP = as.numeric(NA), ECOLI_STAT = as.character(NA), ECOLI_STATECOLI_VERBOSE = as.character(NA), ENTER_EXC = as.numeric(NA), ENTER_SAMP = as.numeric(NA), ENTER_GM_EXC = as.numeric(NA), ENTER_GM_SAMP = as.numeric(NA), ENTER_STAT = as.character(NA), ENTER_STATENTER_VERBOSE = as.character(NA)) )} } # Example Usage: # where stationData object is already in your environment, see Automated Assessment for environment set up help # bacteriaAssessmentDecisionClass(stationData) 3.5.6 Nutrients Nutrients are assessed based on waterbody type. Lake stations have a rigorous protocol detailed below, but automated assessment methods for non-lake stations simply sum samples. An example workflow below describes how using the lakeStation field created during prior stationData organization steps can appropriately assess nutrients for different station types. See below for more information on lake vs other waterbody individual nutrient analyses. # Example Usage for the Station Table output: # where stationData object is already in your environment, see Automated Assessment for environment set up help # Nutrients: TP if(unique(stationData$lakeStation) == TRUE){ TP &lt;- TP_Assessment(stationData) } else { TP &lt;- countNutrients(stationData, PHOSPHORUS_mg_L, LEVEL_PHOSPHORUS, NA) %&gt;% quickStats(&#39;NUT_TP&#39;) %&gt;% mutate(NUT_TP_STAT = ifelse(NUT_TP_STAT != &quot;S&quot;, &quot;Review&quot;, NA)) } # if status isn&#39;t S, flag for follow up review # Nutrients: Chl a if(unique(stationData$lakeStation) == TRUE){ chla &lt;- chlA_Assessment(stationData) } else { chla &lt;- countNutrients(stationData, CHLOROPHYLL_A_ug_L, LEVEL_CHLOROPHYLL_A, NA) %&gt;% quickStats(&#39;NUT_CHLA&#39;) %&gt;% mutate(NUT_CHLA_STAT = NA) } # don&#39;t show a real assessment decision 3.5.6.1 Nutrients in Lakes Nutrients are assessed in Virginia lakes and reservoirs based on lake-specific criteria Section 187. Chlorophyll a and Total Phosphorus (TP) data collected in the top meter are assessed for Aquatic Life Use, with TP criteria only applying to stations designated in a lakes lacustrine zone in lakes where algaecides are applied any time during the monitoring period. The lacustrine zone designation is communicated to the automated assessment process in the Station Table LACUSTRINE field. Assessment guidance states: The 90th percentile of chlorophyll data collected at one meter or less within the lacustrine portion of the man-made lake or reservoir between April 1 and October 31 (considered a lake monitoring year) shall not exceed the chlorophyll a criterion for that waterbody in each of the two most recent monitoring years within the assessment window. For a waterbody that received algaecide treatment, the median of the total phosphorus data collected at one meter or less within the lacustrine portion of the man-made lake or reservoir between April 1 and October 31 shall not exceed the total phosphorus criterion in each of the two most recent years that total phosphorus data are available. The aquatic life (fishery) use of any lake assessment unit is considered impaired for nutrients if the criterion for either chlorophyll a or total phosphorus is exceeded at a station or pooled stations in that unit in each of the two most recent monitoring years within the assessment window. The TP and Chlorophyll a functions are designed to operate on individual station data and data from multiple stations (e.g. stations in the same AU). 3.5.6.1.1 Total Phosphorus: Lakes Total Phosphorus (TP) is assessed using two functions, the TP_analysis() and TP_Assessment() functions. The TP_analysis() function calculates and compares annual median TP values to the lake-specific criteria, providing an output for the TP_Assessment() function to apply three year exceedance rules. The TP_analysis() first verifies the station is a lacustrine station in a Section 187 lake. Next, all Level I and Level II data, missing data, data outside the sample season, and data greater than 1 meter depth are removed from further analyses steps. The median TP of each sample month is calculated before taking the median of each sample year, rounding the value to even, and comparing the results to the lake-specific TP criteria. The number of valid samples are totaled and returned with the annual median results and station lacustrine status. TP_analysis &lt;- function(stationData){ if(!is.na(unique(stationData$Lakes_187B))){ if(unique(stationData$Lakes_187B) == &#39;y&#39;){ stationData &lt;- filter(stationData, LACUSTRINE == &#39;Y&#39;) } } TP &lt;- filter(stationData, !is.na(PHOSPHORUS_mg_L)) %&gt;% filter(FDT_DEPTH &lt;= 1) %&gt;% # Guidance calls for top meter only filter(!( LEVEL_PHOSPHORUS %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out dplyr::select(FDT_STA_ID, FDT_DEPTH, FDT_DATE_TIME, SampleDate, PHOSPHORUS_mg_L, `Total Phosphorus (mg/L)`, LACUSTRINE)%&gt;% mutate(Year= year(FDT_DATE_TIME), Month=month(FDT_DATE_TIME)) %&gt;% filter(Month %in% c(4, 5, 6, 7, 8, 9, 10)) # make sure only assess valid sample months if(length(unique(TP$FDT_STA_ID)) &gt; 1){ TPResults &lt;- TP %&gt;% group_by(Month, Year) %&gt;% summarise(samplesPerMonth = n(), medianPHOSPHORUS_mg_L = median(PHOSPHORUS_mg_L, na.rm = T), `Total Phosphorus (mg/L)` = unique(`Total Phosphorus (mg/L)`)) %&gt;% ungroup() %&gt;% group_by(Year) %&gt;% summarise(samplesPerYear = n(), `Annual Median TP` = median(medianPHOSPHORUS_mg_L, na.rm = TRUE), `Total Phosphorus (mg/L)` = unique(`Total Phosphorus (mg/L)`)) %&gt;% mutate(`Annual Median TP Rounded to WQS Format` = signif(`Annual Median TP`, digits = 1), # one significant figures based on WQS https://lis.virginia.gov/cgi-bin/legp604.exe?000+reg+9VAC25-260-187&amp;000+reg+9VAC25-260-187 TP_Exceedance = ifelse(`Annual Median TP Rounded to WQS Format` &gt; `Total Phosphorus (mg/L)`, T, F), ID305B = unique(stationData$ID305B_1)) %&gt;% dplyr::select(ID305B, Year, samplesPerYear, `Annual Median TP`, `Annual Median TP Rounded to WQS Format`, everything()) } else { TPResults &lt;- TP %&gt;% group_by(Year) %&gt;% mutate(samplesPerYear = n(), `Annual Median TP` = median(PHOSPHORUS_mg_L, na.rm = TRUE), `Annual Median TP Rounded to WQS Format` = signif(`Annual Median TP`, digits = 1), # one significant figures based on WQS https://lis.virginia.gov/cgi-bin/legp604.exe?000+reg+9VAC25-260-187&amp;000+reg+9VAC25-260-187 TP_Exceedance = ifelse(`Annual Median TP Rounded to WQS Format` &gt; `Total Phosphorus (mg/L)`, T, F)) %&gt;% dplyr::select(FDT_STA_ID, Year, samplesPerYear, `Annual Median TP`, `Annual Median TP Rounded to WQS Format`,`Total Phosphorus (mg/L)`, TP_Exceedance, LACUSTRINE) %&gt;% distinct(Year, .keep_all=T) } return(TPResults) } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help #TP_analysis(stationData) #TP_analysis(AUdata) # where AUdata is the stationData from all stations in the same AU The TP_Assessment() function analyzes and transforms annual TP results into the Station Table output format. The function first uses the TP_analysis() function to establish annual TP exceedance results. Annual TP results are only analyzed in years with six or greater samples. With nested if/else logic, the function identifies if/when any exceedances exist. If no exceedances occurred in the most recent two years, S is returned in the status field. If both of the most recent two years exceeded median TP criteria, IM is returned in the status field. If one of two years had a median TP exceedance, the third most recent year is brought in as a tiebreak. If two of the most recent three years have a median TP exceedance, IM is returned in the status field, but if one in the most recent three years have a median TP exceedance then Review is returned in the status field. The sample count returned is the number of years analyzed in the analysis and the exceedance field reports the number of years where the median TP exceeds criteria in the most recent two or three years. TP_Assessment &lt;- function(stationData){ TP_Results &lt;- TP_analysis(stationData) %&gt;% ungroup() if(nrow(TP_Results) &gt; 0){ if(is.na(unique(TP_Results$`Total Phosphorus (mg/L)`))){ # bail out if nutrient standards didn&#39;t join properly return(tibble(NUT_TP_EXC= NA, NUT_TP_SAMP = NA, NUT_TP_STAT = NA))} validYears &lt;- filter(TP_Results, samplesPerYear &gt;= 6) # need at least 6 samples per year mostRecent2years &lt;- slice_max(validYears, Year, n = 2) # get most recent two years of results if(nrow(mostRecent2years) == 2){ if(all(unique(mostRecent2years$TP_Exceedance) == FALSE)){ # no exceedances in last two years return(tibble(NUT_TP_EXC= 0, NUT_TP_SAMP = nrow(mostRecent2years), NUT_TP_STAT = &#39;S&#39;) ) } else { # at least one TP_Exceedance exists if(all(unique(mostRecent2years$TP_Exceedance) == TRUE)){ # both years exceed return(tibble(NUT_TP_EXC= nrow(mostRecent2years), NUT_TP_SAMP = nrow(mostRecent2years), NUT_TP_STAT = &#39;IM&#39;)) } else { # run a tiebreak with third most recent year mostRecent3years &lt;- slice_max(validYears, Year, n = 3) # get most recent three years of results mostRecent3yearsExceed &lt;- filter(mostRecent3years, TP_Exceedance == TRUE) if(nrow(mostRecent3yearsExceed) &gt;= 2){ return(tibble(NUT_TP_EXC= nrow(mostRecent3yearsExceed), NUT_TP_SAMP = nrow(mostRecent3years), NUT_TP_STAT = &#39;IM&#39;)) } else { return(tibble(NUT_TP_EXC= nrow(mostRecent3yearsExceed), NUT_TP_SAMP = nrow(mostRecent3years), NUT_TP_STAT = &#39;Review&#39;)) } }}} else {return(tibble(NUT_TP_EXC= NA, NUT_TP_SAMP = nrow(validYears), NUT_TP_STAT = &#39;IN&#39;) ) } } else { return(tibble(NUT_TP_EXC= NA, NUT_TP_SAMP = NA, NUT_TP_STAT = NA) ) } } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help #TP_Assessment(stationData) #TP_Assessment(AUdata) # where AUdata is the stationData from all stations in the same AU 3.5.6.1.2 Chlorophyll a: Lakes Chlorophyll a is assessed using two functions, the chlA_analysis() and chlA_Assessment() functions. The chlA_analysis() function calculates and compares the annual 90th percentile of chlorophyll a values to the lake-specific criteria, providing an output for the chlA_Assessment() function to apply three year exceedance rules. The chlA_analysis() first verifies the station is a lacustrine station in a Section 187 lake. Next, all Level I and Level II data, missing data, data outside the sample season, and data greater than 1 meter depth are removed from further analyses steps. The median chlorophyll a of each sample month is calculated before taking the 90th percentile of chlorophyll a values from each sample year, rounding the value to even, and comparing the results to the lake-specific chlorophyll a criteria. The number of valid samples are totaled and returned with the annual 90th percentile results and station lacustrine status. chlA_analysis &lt;- function(stationData){ if(!is.na(unique(stationData$Lakes_187B))){ if(unique(stationData$Lakes_187B) == &#39;y&#39;){ stationData &lt;- filter(stationData, LACUSTRINE == &#39;Y&#39;) } } chla &lt;- filter(stationData, !is.na(CHLOROPHYLL_A_ug_L)) %&gt;% filter(FDT_DEPTH &lt;= 1) %&gt;% # Guidance calls for top meter only filter(!( LEVEL_CHLOROPHYLL_A %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out dplyr::select(FDT_STA_ID, FDT_DEPTH, FDT_DATE_TIME, SampleDate, CHLOROPHYLL_A_ug_L, `Chlorophyll a (ug/L)`, LACUSTRINE)%&gt;% mutate(Year= year(FDT_DATE_TIME), Month=month(FDT_DATE_TIME)) %&gt;% filter(Month %in% c(4, 5, 6, 7, 8, 9, 10)) # make sure only assess valid sample months if(length(unique(chla$FDT_STA_ID)) &gt; 1){ chlaResults &lt;- chla %&gt;% group_by(Month, Year) %&gt;% summarise(samplesPerMonth = n(), medianCHLOROPHYLL_A_ug_L = median(CHLOROPHYLL_A_ug_L, na.rm = T), `Chlorophyll a (ug/L)` = unique(`Chlorophyll a (ug/L)`)) %&gt;% ungroup() %&gt;% group_by(Year) %&gt;% summarise(samplesPerYear = n(), pct90 = quantile(medianCHLOROPHYLL_A_ug_L, 0.9), `Chlorophyll a (ug/L)` = unique(`Chlorophyll a (ug/L)`)) %&gt;% mutate(`90th Percentile Rounded to WQS Format` = signif(pct90, digits = 2), # two significant figures based on WQS https://lis.virginia.gov/cgi-bin/legp604.exe?000+reg+9VAC25-260-187&amp;000+reg+9VAC25-260-187 chlA_Exceedance = ifelse(`90th Percentile Rounded to WQS Format` &gt; `Chlorophyll a (ug/L)`, T, F), ID305B = unique(stationData$ID305B_1)) %&gt;% dplyr::select(ID305B, Year, samplesPerYear, pct90, `90th Percentile Rounded to WQS Format`, everything()) } else { chlaResults &lt;- chla %&gt;% group_by(Year) %&gt;% mutate(samplesPerYear = n(), pct90 = quantile(CHLOROPHYLL_A_ug_L, 0.9), `90th Percentile Rounded to WQS Format` = signif(pct90, digits = 2), # two significant figures based on WQS https://lis.virginia.gov/cgi-bin/legp604.exe?000+reg+9VAC25-260-187&amp;000+reg+9VAC25-260-187 chlA_Exceedance = ifelse(`90th Percentile Rounded to WQS Format` &gt; `Chlorophyll a (ug/L)`, T, F)) %&gt;% dplyr::select(FDT_STA_ID, Year, samplesPerYear, pct90, `90th Percentile Rounded to WQS Format`,`Chlorophyll a (ug/L)`, chlA_Exceedance, LACUSTRINE) %&gt;% distinct(Year, .keep_all=T) } return(chlaResults) } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help #chlA_analysis(stationData) #chlA_analysis(AUdata) # where AUdata is the stationData from all stations in the same AU The chlA_Assessment() function analyzes and transforms annual chlorophyll a results into the Station Table output format. The function first uses the chlA_analysis() function to establish annual chlorophyll a exceedance results. Annual chlorophyll a results are only analyzed in years with six or greater samples. With nested if/else logic, the function identifies if/when any exceedances exist. If no exceedances occurred in the most recent two years, S is returned in the status field. If both of the most recent two years exceeded the chlorophyll a criteria, IM is returned in the status field. If one of two years had a chlorophyll a exceedance, the third most recent year is brought in as a tiebreak. If two of the most recent three years have a chlorophyll a exceedance, IM is returned in the status field, but if one in the most recent three years have a chlorophyll a exceedance then Review is returned in the status field. The sample count returned is the number of years analyzed in the analysis and the exceedance field reports the number of years where the chlorophyll a exceeds criteria in the most recent two or three years. chlA_Assessment &lt;- function(stationData){ chlA_Results &lt;- chlA_analysis(stationData) %&gt;% ungroup() if(nrow(chlA_Results) &gt; 0){ if(is.na(unique(chlA_Results$`Chlorophyll a (ug/L)`))){ # bail out if nutrient standards didn&#39;t join properly return(tibble(NUT_CHLA_EXC= NA, NUT_CHLA_SAMP = NA, NUT_CHLA_STAT = NA))} validYears &lt;- filter(chlA_Results, samplesPerYear &gt;= 6) # need at least 6 samples per year mostRecent2years &lt;- slice_max(validYears, Year, n = 2) # get most recent two years of results if(nrow(mostRecent2years) == 2){ if(all(unique(mostRecent2years$chlA_Exceedance) == FALSE)){ # no exceedances in last two years return(tibble(NUT_CHLA_EXC= 0, NUT_CHLA_SAMP = nrow(mostRecent2years), NUT_CHLA_STAT = &#39;S&#39;) ) } else { # at least one chlA_Exceedance exists if(all(unique(mostRecent2years$chlA_Exceedance)) == TRUE){ # both years exceed return(tibble(NUT_CHLA_EXC= nrow(mostRecent2years), NUT_CHLA_SAMP = nrow(mostRecent2years), NUT_CHLA_STAT = &#39;IM&#39;)) } else { # run a tiebreak with third most recent year mostRecent3years &lt;- slice_max(validYears, Year, n = 3) # get most recent three years of results mostRecent3yearsExceed &lt;- filter(mostRecent3years, chlA_Exceedance == TRUE) if(nrow(mostRecent3yearsExceed) &gt;= 2){ return(tibble(NUT_CHLA_EXC= nrow(mostRecent3yearsExceed), NUT_CHLA_SAMP = nrow(mostRecent3years), NUT_CHLA_STAT = &#39;IM&#39;)) } else { return(tibble(NUT_CHLA_EXC= nrow(mostRecent3yearsExceed), NUT_CHLA_SAMP = nrow(mostRecent3years), NUT_CHLA_STAT = &#39;Review&#39;)) } }}} else {return(tibble(NUT_CHLA_EXC= NA, NUT_CHLA_SAMP = nrow(validYears), NUT_CHLA_STAT = &#39;IN&#39;) ) } } else { return(tibble(NUT_CHLA_EXC= NA, NUT_CHLA_SAMP = NA, NUT_CHLA_STAT = NA) ) } } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help #chlA_Assessment(stationData) #chlA_analysis(AUdata) # where AUdata is the stationData from all stations in the same AU 3.5.6.2 Nutrients in Other Waterbodies Non-lake station use the countNutrients() function to sum nutrient samples for the Station Table. Using non-standard evaluation techniques in the function arguments allows users to specify which parameter to evaluate and the associated criteria. Criteria or thresholds may be established in the nutrientLimit function arguments for analytical purposes, but NA is used for the Station Table output. All Level I and Level II data and missing data are removed for the selected parameter before the samples compared against the value in the nutrientLimit argument. The quickStats() function can then use that output to efficiently tabulate the number of samples and exceedances. countNutrients &lt;- function(stationData, fieldName, commentName, nutrientLimit){ fieldName_ &lt;- enquo(fieldName) commentName_ &lt;- enquo(commentName) dplyr::select(stationData,FDT_STA_ID,FDT_DATE_TIME, !! fieldName_, !! commentName_)%&gt;% # Just get relevant columns filter(!( !! commentName_ %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out filter(!is.na(!!fieldName_ )) %&gt;% #get rid of NA&#39;s rename(parameter = !!names(.[3])) %&gt;% # rename columns to make functions easier to apply mutate(limit = nutrientLimit, exceeds = ifelse(parameter &gt; limit, T, F)) # Identify where above WQS limit } # Example Usage: # where stationData object is already in your environment, see Automated Assessment for environment set up help # countNutrients(stationData, PHOSPHORUS_mg_L, LEVEL_PHOSPHORUS, 0.2) %&gt;% # quickStats(&#39;NUT_TP&#39;) # countNutrients(stationData, CHLOROPHYLL_A_ug_L, LEVEL_CHLOROPHYLL_A, NA) %&gt;% # quickStats(&#39;NUT_CHLA&#39;) 3.5.6.3 Trophic State Index (TSI): Lakes TSI is calculated for lakes not defined in Section 187 of the Virginia Water Quality Standards (e.g. lakes without nutrient criteria). The Carlson TSI equations use secchi depth (SD), surface chlorophyll a (CA), and surface total phosphorus (TP) to estimate algal biomass to determine if DO problems in lakes and reservoirs are natural. These calculations are applied to stratified lakes using aggregated station data from mid-June through mid-September. The TSI index is calculated with equations for secchi depth, chlorophyll a, and TP, each resulting in a continuous variable from 0-100. If any of the parameters results in index values of 60 or greater, nutrient enrichment could be adversely affecting the waterbodys designated uses. The TSIcalculation() function calculates all three TSI equations for individual stations not designated as Section 187 lakes (in the linked WQS metadata). Depth and temporal filters established by the WQA Guidance are applied to the station dataset and sample secchi depth measures are carried across all relevant measures (because secchi depth measures are only stored at the 0.3 meter depth record for all samples). All Level I and Level II data and missing data are removed from each parameter before measures are run through the appropriate TSI equation. The mean of each parameter TSI index results is calculated and reported back with a listcolumn of associated data. TSIcalculation &lt;- function(stationData){ if(unique(stationData$lakeStation) == TRUE){ if(is.na(unique(stationData$Lakes_187B))){ # first fill down secchi depth in case it isn&#39;t stored exactly at 0.3 meter secchiFix &lt;- stationData %&gt;% group_by(FDT_STA_ID, FDT_DATE_TIME) %&gt;% fill(SECCHI_DEPTH_M, .direction = &quot;downup&quot;) %&gt;% filter(FDT_DEPTH &lt;= 0.3) %&gt;% # remove all data except those from mid June through mid September (going with June 15 and Sept 15 since guidance does not specify) mutate(monthday = as.numeric(paste0(month(FDT_DATE_TIME), day(FDT_DATE_TIME)))) %&gt;% filter(between(monthday, 615, 915 )) %&gt;% dplyr::select(-monthday) # Calculate Secchi depth TSI SDdata &lt;- filter(secchiFix, !is.na(SECCHI_DEPTH_M) ) %&gt;% filter(!( LEVEL_SECCHI_DEPTH %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH,SECCHI_DEPTH_M) %&gt;% mutate(TSI_SD = 10*(6 - (log(SECCHI_DEPTH_M) / log(2))) ) SD &lt;- suppressWarnings(suppressMessages( SDdata %&gt;% group_by(FDT_STA_ID) %&gt;% summarise(meanSD = mean(SECCHI_DEPTH_M, na.rm = T), # take average of all secchi depths first TSI_SD = 10*(6 - (log(meanSD) / log(2))) ) ))# log() is natural log in R # Calculate Chlorophyll a TSI chlaData &lt;- filter(secchiFix, !is.na(CHLOROPHYLL_A_ug_L) ) %&gt;% filter(!( LEVEL_CHLOROPHYLL_A %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, CHLOROPHYLL_A_ug_L) %&gt;% mutate(TSI_chla = 10*(6 - (2.04 - 0.68 * (log( CHLOROPHYLL_A_ug_L))) / log(2))) chla &lt;- suppressWarnings(suppressMessages( chlaData %&gt;% group_by(FDT_STA_ID) %&gt;% summarise(meanchla = mean(CHLOROPHYLL_A_ug_L, na.rm = T), # take average of all chl a first TSI_chla = 10*(6 - (2.04 - 0.68 * (log(meanchla))) / log(2))) ))# log() is natural log in R # Calculate Total Phosphorus TSI TPdata &lt;- filter(secchiFix, !is.na(PHOSPHORUS_mg_L) ) %&gt;% filter(!( LEVEL_PHOSPHORUS %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, PHOSPHORUS_mg_L) %&gt;% mutate(PHOSPHORUS_ug_L = PHOSPHORUS_mg_L * 1000,# first convert mg/L to ug/L TSI_TP = 10*(6 - (log( (48 / PHOSPHORUS_ug_L )) / log(2))) ) TP &lt;- suppressWarnings(suppressMessages( TPdata %&gt;% group_by(FDT_STA_ID) %&gt;% summarise(meanTP = mean(PHOSPHORUS_ug_L, na.rm = T), # take average of all TP first TSI_TP = 10*(6 - (log( (48 / meanTP)) / log(2))) ) ))# log() is natural log in R TSI &lt;- full_join(SD, chla, by = c(&#39;FDT_STA_ID&#39;)) %&gt;% full_join(TP, by = c(&#39;FDT_STA_ID&#39;)) %&gt;% bind_cols(tibble(associatedData = list(full_join(SDdata, chlaData, by = c(&#39;FDT_STA_ID&#39;, &#39;FDT_DATE_TIME&#39;, &#39;FDT_DEPTH&#39;)) %&gt;% full_join(TPdata, by = c(&#39;FDT_STA_ID&#39;, &#39;FDT_DATE_TIME&#39;, &#39;FDT_DEPTH&#39;))) )) return(TSI) } else {return(NULL)} } else {return(NULL)} } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help # TSIcalculation(stationData) 3.5.7 Ammonia Ammonia is evaluated using acute, chronic, and four-day criteria across rolling three year windows that may not exceed more than once every three years on the average in free flowing streams. The presence or absence of trout, freshwater mussel species, and early life stages of fish during most times of the year each affect the calculation of ammonia criteria for a given waterbody. The WQS state that the Department of Environmental Quality, after consultation with the Virginia Department of Wildlife Resources and the U.S. Fish and Wildlife Service, has determined that the majority of Virginia freshwaters are likely to contain, or have contained in the past, freshwater mussel species in the family Unionidae and contain early life stages of fish during most times of the year. Therefore, the ammonia criteria presented in subsections B and C of this section are designed to provide protection to these species and life stages. In practice, this means, unless demonstrated otherwise, mussels and fish early life stages are assumed present for criteria method calculations. Once the method for criteria calculation has been established for a waterbody, individual temperature and pH measurements are required to calculate individual criteria, and averages of these parameters are required when more than one measure is present within a given criteria window (for chronic, 30 day, and four day criteria). Ammonia is a complicated criteria to program and understand. The automated assessment scripts have organized ammonia criteria calculation into nested functions such that individual components of the analysis can be extracted when needed (e.g. during interactive application data visualization). To expedite application rendering time, calculated ammonia results are stored in a R dataset (.RDS) and provided to the application instead of recalculating the criteria on the fly in the application. As such, the WQS (including trout present/absent) attributed to a station in the metadata attribution process as well as mussels and early life fish stages present values are the default values when exploring ammonia criteria analyses in the interactive waterbody-specific shiny assessment applications. Should sufficient justification exist to depart from these default values for individual stations, please contact the WQA Technical Support Team. 3.5.7.1 freshwaterNH3limit The calculation and application of ammonia criteria is performed by the freshwaterNH3limit() function. This function takes in data from one station (stationData argument) as well as three additional boolean (TRUE/FALSE) arguments: trout (trout present/absent), mussels (mussels present/absent), and earlyLife (early life stages of fish present/absent). The trout argument determined by the associated WQS Class where Class V and VI waters indicate trout are present and any other class indicates trout are absent. As stated above, the mussels and earlyLife arguments are default set to TRUE to be most protective, but should justification exist, these options may be toggled to FALSE and result in different criteria methods applied to the data. This function is complicated and will be explained using the order of operations presented internally. Running through each scenario individually could assist understanding of the logic. First, since this is a computationally-intensive function, if no data are available to be analyzed, the function returns NULL. If data are present, the function removes all Level I and Level II data, missing data, and data greater than 1 meter depth before rechecking whether data are there for analysis and returning NULL if not. The function then creates storage objects for acute, chronic, and four day average results and loops through each row of data to correctly calculate acute criteria and find any chronic scenarios (and then run a 4 day analysis if data exist). Each new sample event prompts the creation of acute, chronic, and four day data windows. If sufficient data exist in any of these windows, the window temperature and pH averages are used to calculate ammonia criteria based on the scenario arguments established above for comparison to the window ammonia average, rounded to even. Chronic and four day windows are only calculated if more than one result exists in those data windows. The data analyzed within each window is saved as a listcolumn called associatedData that may be extracted for investigation later. The output of the function is a long dataset where each unique sample event could have a row summarizing the acute, chronic, and four day calculated criteria, sample count in the given window, exceedance result, and associated data (associatedData listcolumn). # Calculate limits and return dataframe with original data and limits 9VAC25-260-155 https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ freshwaterNH3limit &lt;- function(stationData, # dataframe with station data trout, # T/F condition mussels,# T/F condition earlyLife# T/F condition ){ # If no data, return nothing if(nrow(stationData)==0){return(NULL)} # remove any data that shouldn&#39;t be considered stationDataAmmonia &lt;- filter(stationData, !(LEVEL_FDT_TEMP_CELCIUS %in% c(&#39;Level II&#39;, &#39;Level I&#39;)) | !(LEVEL_FDT_FIELD_PH %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out # # lake stations should only be surface sample # {if(unique(stationData$lakeStation) == TRUE) filter(., FDT_DEPTH &lt;= 1) %&gt;% # all samples should be &lt; 1m (this allows for 0.3m surface samples and 0m integrated samples) # else . } %&gt;% filter(!is.na(AMMONIA_mg_L)) %&gt;% #get rid of NA&#39;s dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, FDT_TEMP_CELCIUS, FDT_FIELD_PH, AMMONIA_mg_L) %&gt;% # can only run analysis if all above variable are populated for a given date/time filter(!is.na(FDT_TEMP_CELCIUS) &amp; !is.na(FDT_FIELD_PH)) # If no data, return nothing if(nrow(stationDataAmmonia)==0){return(NULL)} # make a place to store analysis results acuteCriteriaResults &lt;- tibble(FDT_STA_ID = as.character(NA), WindowDateTimeStart = as.POSIXct(NA), FDT_DEPTH = as.character(NA),# character so we can concatenate multiple depth values if needed Value = as.numeric(NA), ValueType = as.character(NA), `Criteria Type` = as.character(NA), CriteriaValue = as.numeric(NA), `Sample Count` = as.numeric(NA), parameterRound = as.numeric(NA), Exceedance = as.numeric(NA), associatedData = list()) chronicCriteriaResults &lt;- acuteCriteriaResults fourDayCriteriaResults &lt;- acuteCriteriaResults # loop through each row of data to correctly calculate acute criteria and find any chronic scenarios (and then # run a 4 day analysis if data exist) for(k in stationDataAmmonia$FDT_DATE_TIME){ #k = stationDataAmmonia$FDT_DATE_TIME[2] acuteDataWindow &lt;- filter(stationDataAmmonia, between(FDT_DATE_TIME, k, k + hours(1))) chronicDataWindow &lt;- filter(stationDataAmmonia, between(FDT_DATE_TIME, k, k + days(30))) fourDayDataWindow &lt;- filter(stationDataAmmonia, between(FDT_DATE_TIME, k, k + days(4))) # Run acute analysis if data exists # Acute is calculated on 1 hour windows, so we need to average temperature and pH within each 1 hour window before we can calculate an # acute criteria. There are no rules on how many samples need to occur in a 1 hour window for the window to be valid, but chronic and # 4 day criteria require &gt; 1 sample to be analyzed. # Acute criteria can combine multiple depths in calculation of criteria, &lt;1m difference (which is taken care of above) if(nrow(acuteDataWindow) &gt; 0){ acuteDataCriteriaAnalysis &lt;- suppressMessages( acuteDataWindow %&gt;% # group_by(FDT_STA_ID) %&gt;% # for some reason if multiple depths but 1 stationID get 2 rows when group_by(FDT_STA_ID) summarise(FDT_STA_ID = paste0(unique(FDT_STA_ID), collapse = &#39; &#39;), ### this is the ugly fix to problem identified above FDT_DEPTH = paste0(sort(unique(FDT_DEPTH)), collapse = &#39; | &#39;), TempValue = mean(FDT_TEMP_CELCIUS, na.rm=T), # get hourly average; don&#39;t round to even bc more calculations to follow with data pHValue = mean(FDT_FIELD_PH, na.rm=T), # get hourly average; don&#39;t round to even bc more calculations to follow with data Value = mean(AMMONIA_mg_L, na.rm=T), # get hourly average; don&#39;t round to even bc more calculations to follow with data `Sample Count` = length(AMMONIA_mg_L)) %&gt;% #count sample that made up average mutate(ValueType = &#39;Hourly Average&#39;, ID = paste( FDT_STA_ID, FDT_DEPTH, sep = &#39;_&#39;), # make a uniqueID in case &gt;1 sample for given datetime `Criteria Type` = &#39;Acute&#39;) %&gt;% {if(trout == TRUE &amp; mussels == TRUE) # Trout &amp; mussels present scenario mutate(., CriteriaValue = as.numeric(signif( min(((0.275 / (1 + 10^(7.204 - pHValue))) + (39.0 / (1 + 10^(pHValue - 7.204)))), (0.7249 * ( (0.0114 / (1 + 10^(7.204 - pHValue))) + (1.6181 / (1 + 10^(pHValue - 7.204)))) * (23.12 * 10^(0.036 * (20 - TempValue))) )), digits = 2))) else . } %&gt;% {if(trout == TRUE &amp; mussels == FALSE) # Trout present &amp; mussels absent scenario mutate(., CriteriaValue = as.numeric(signif( min(((0.275 / (1 + 10^(7.204 - pHValue))) + (39.0 / (1 + 10^(pHValue - 7.204)))), (0.7249 * ( (0.0114 / (1 + 10^(7.204 - pHValue))) + (1.6181 / (1 + 10^(pHValue - 7.204)))) * (62.15 * 10^(0.036 * (20 - TempValue))) )), digits = 2))) else . } %&gt;% {if(trout == FALSE &amp; mussels == TRUE) # Trout absent &amp; mussels present scenario mutate(., CriteriaValue = as.numeric(signif( 0.7249 * ((0.0114 / (1 + 10^(7.204 - pHValue))) + (1.6181 / (1 + 10^(pHValue - 7.204)))) * min(51.93, (23.12 * 10^(0.036 * (20 - TempValue)))), digits = 2))) else .} %&gt;% {if(trout == FALSE &amp; mussels == FALSE) # Trout &amp; mussels absent scenario mutate(., CriteriaValue = as.numeric(signif( 0.7249 * ((0.0114 / (1 + 10^(7.204 - pHValue))) + (1.6181 / (1 + 10^(pHValue - 7.204)))) * min(51.93, (62.15 * 10^(0.036 * (20 - TempValue)))), digits = 2))) else .} %&gt;% mutate(parameterRound = signif(Value, digits = 2), # two significant figures based on 9VAC25-260-155 https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ Exceedance = ifelse(parameterRound &gt; CriteriaValue, 1, 0 ), # use 1/0 to easily summarize multiple results later WindowDateTimeStart = min(acuteDataWindow$FDT_DATE_TIME)) %&gt;% dplyr::select(FDT_STA_ID, WindowDateTimeStart, everything(), -ID) ) # now save data for later, in a format that matches with desired output acuteDataCriteriaAnalysis &lt;- acuteDataCriteriaAnalysis %&gt;% dplyr::select(-c(TempValue, pHValue)) %&gt;% bind_cols(tibble(associatedData = list(left_join(acuteDataWindow, dplyr::select(acuteDataCriteriaAnalysis, FDT_STA_ID, WindowDateTimeStart, TempValue, pHValue, Value), by = c(&#39;FDT_STA_ID&#39;, &#39;FDT_DATE_TIME&#39; = &#39;WindowDateTimeStart&#39;)))) ) # Save the results for viewing later acuteCriteriaResults &lt;- bind_rows(acuteCriteriaResults, acuteDataCriteriaAnalysis) } else {acuteCriteriaResults &lt;- acuteCriteriaResults } # Run chronic analysis if enough data exists, must have &gt; 1 sample in chronic window to run analysis # Chronic is calculated on 30 day windows, so we need to average temperature and pH within each 30 day window before we can calculate a # chronic criteria. The chronic criteria will be associated with each sample date that starts a 30 day period, but it applies to all # samples within the 30 day window. All raw data associated with each window is saved as a listcolumn for later review. # Chronic criteria can combine multiple depths in calculation of criteria, &lt;1m difference (which is taken care of above) if(nrow(chronicDataWindow) &gt; 1){ # need 2 or more data points to run a chronic chronicDataCriteriaAnalysis &lt;- suppressMessages( chronicDataWindow %&gt;% # group_by(FDT_STA_ID) %&gt;% # for some reason if multiple depths but 1 stationID get 2 rows when group_by(FDT_STA_ID) summarise(FDT_STA_ID = paste0(unique(FDT_STA_ID), collapse = &#39; &#39;), ### this is the ugly fix to problem identified above FDT_DEPTH = paste0(sort(unique(FDT_DEPTH)), collapse = &#39; | &#39;), TempValue = mean(FDT_TEMP_CELCIUS, na.rm = T), # get 30 day average; don&#39;t round to even bc more calculations to follow with data pHValue = mean(FDT_FIELD_PH, na.rm = T), # get 30 day average; don&#39;t round to even bc more calculations to follow with data Value = mean(AMMONIA_mg_L, na.rm = T), # get 30 day average; don&#39;t round to even bc more calculations to follow with data `Sample Count` = length(AMMONIA_mg_L)) %&gt;% #count sample that made up average mutate(ValueType = &#39;30 Day Average&#39;, ID = paste( FDT_STA_ID, FDT_DEPTH, sep = &#39;_&#39;), # make a uniqueID in case &gt;1 sample for given datetime `Criteria Type` = &#39;Chronic&#39;) %&gt;% # Trout &amp; mussels present scenario {if(trout == TRUE &amp; mussels == TRUE &amp; earlyLife == TRUE) # Trout &amp; mussels present scenario &amp; earlyLife == TRUE mutate(., CriteriaValue = as.numeric(signif( 0.8876 * ((0.0278 / (1 + 10^(7.688 - pHValue))) + (1.1994 / (1 + 10^(pHValue - 7.688)))) * (2.126 * 10^(0.028 * (20 - max(7, TempValue)))), digits = 2))) else .} %&gt;% {if(trout == TRUE &amp; mussels == TRUE &amp; earlyLife == FALSE) # Trout &amp; mussels present scenario &amp; earlyLife == FALSE mutate(., CriteriaValue = as.numeric(NA)) else .} %&gt;% # Trout present &amp; mussels absent scenario {if(trout == TRUE &amp; mussels == FALSE &amp; earlyLife == TRUE) # Trout present &amp; mussels absent scenario &amp; earlyLife == TRUE mutate(., CriteriaValue = as.numeric(signif( 0.9405 * ((0.0278 / (1 + 10^(7.688 - pHValue))) + (1.1994 / (1 + 10^(pHValue - 7.688)))) * min(6.92, (7.547 * 10^(0.028 * (20 - TempValue)))), digits = 2))) else .} %&gt;% {if(trout == TRUE &amp; mussels == FALSE &amp; earlyLife == FALSE) # Trout present &amp; mussels absent scenario &amp; earlyLife == FALSE mutate(., CriteriaValue = as.numeric(signif( 0.9405 * ((0.0278 / (1 + 10^(7.688 - pHValue))) + (1.1994 / (1 + 10^(pHValue - 7.688)))) * (7.547 * 10^(0.028 * (20 - max(TempValue, 7)))), digits = 2))) else .} %&gt;% # Trout absent &amp; mussels present scenario {if(trout == FALSE &amp; mussels == TRUE &amp; earlyLife == TRUE) # Trout absent &amp; mussels present scenario &amp; earlyLife == TRUE mutate(., CriteriaValue = as.numeric(signif( 0.8876 * ((0.0278 / (1 + 10^(7.688 - pHValue))) + (1.1994 / (1 + 10^(pHValue - 7.688)))) * (2.126 * 10^(0.028 * (20 - max(7, TempValue)))), digits = 2))) else .} %&gt;% {if(trout == FALSE &amp; mussels == TRUE &amp; earlyLife == FALSE) # Trout absent &amp; mussels present scenario &amp; earlyLife == FALSE mutate(., CriteriaValue = as.numeric(NA)) else .} %&gt;% # Trout &amp; mussels absent scenario {if(trout == FALSE &amp; mussels == FALSE &amp; earlyLife == TRUE) # Trout &amp; mussels absent scenario &amp; earlyLife == TRUE mutate(., CriteriaValue = as.numeric(signif( 0.9405 * ((0.0278 / (1 + 10^(7.688 - pHValue))) + (1.1994 / (1 + 10^(pHValue - 7.688)))) * min(6.92, (7.547 * 10^(0.028 * (20 - TempValue)))), digits = 2))) else .} %&gt;% {if(trout == FALSE &amp; mussels == FALSE &amp; earlyLife == FALSE) # Trout &amp; mussels absent scenario &amp; earlyLife == FALSE mutate(., CriteriaValue = as.numeric(signif( 0.9405 * ((0.0278 / (1 + 10^(7.688 - pHValue))) + (1.1994 / (1 + 10^(pHValue - 7.688)))) * (7.547 * 10^(0.028 * (20 - max(TempValue, 7)))), digits = 2))) else .} %&gt;% mutate(parameterRound = signif(Value, digits = 2), # two significant figures based on 9VAC25-260-155 https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ Exceedance = ifelse(parameterRound &gt; CriteriaValue, 1, 0 ), # use 1/0 to easily summarize multiple results later WindowDateTimeStart = min(acuteDataWindow$FDT_DATE_TIME)) %&gt;% dplyr::select(FDT_STA_ID, WindowDateTimeStart, everything(), -ID) ) # now save data for later, in a format that matches with desired output chronicDataCriteriaAnalysis &lt;- chronicDataCriteriaAnalysis %&gt;% dplyr::select(-c(TempValue, pHValue)) %&gt;% bind_cols(tibble(associatedData = list(left_join(chronicDataWindow, dplyr::select(chronicDataCriteriaAnalysis, FDT_STA_ID, WindowDateTimeStart, TempValue, pHValue, Value), by = c(&#39;FDT_STA_ID&#39;, &#39;FDT_DATE_TIME&#39; = &#39;WindowDateTimeStart&#39;)))) ) # Save the results for viewing later chronicCriteriaResults &lt;- bind_rows(chronicCriteriaResults, chronicDataCriteriaAnalysis) } else {chronicCriteriaResults &lt;- chronicCriteriaResults } # Run 4 day analysis if data exists # 4 day analysis is run on 4 day windows, so we need to average temperature and pH within each 4 day window before we can compare to a 2.5x # chronic criteria. The chronic criteria associated with each sample date that starts a 4 day period is used for the 4 day analysis. # All raw data associated with each window is saved as a listcolumn for later review. # 4day criteria can combine multiple depths in calculation of criteria, &lt;1m difference (which is taken care of above) if(nrow(fourDayDataWindow) &gt; 1){ fourDayDataCriteriaAnalysis &lt;- suppressMessages( fourDayDataWindow %&gt;% # group_by(FDT_STA_ID) %&gt;% # for some reason if multiple depths but 1 stationID get 2 rows when group_by(FDT_STA_ID) summarise(FDT_STA_ID = paste0(unique(FDT_STA_ID), collapse = &#39; &#39;), ### this is the ugly fix to problem identified above FDT_DEPTH = paste0(sort(unique(FDT_DEPTH)), collapse = &#39; | &#39;), TempValue = mean(FDT_TEMP_CELCIUS, na.rm=T), # get 4day average; doesn&#39;t do anything bc this doesn&#39;t go into criteria calculation but needed to match other data format pHValue = mean(FDT_FIELD_PH, na.rm=T), # get 4day average; doesn&#39;t do anything bc this doesn&#39;t go into criteria calculation but needed to match other data format Value = mean(AMMONIA_mg_L, na.rm=T), # get 4day average; don&#39;t round to even bc want to see raw and parameterRound columns in output `Sample Count` = length(AMMONIA_mg_L)) %&gt;% #count sample that made up average mutate(ValueType = &#39;Four Day Average&#39;, ID = paste( FDT_STA_ID, FDT_DEPTH, sep = &#39;_&#39;), # make a uniqueID in case &gt;1 sample for given datetime `Criteria Type` = &#39;Four Day&#39;) %&gt;% left_join(dplyr::select(chronicDataCriteriaAnalysis, FDT_STA_ID, `Chronic CriteriaValue` = CriteriaValue), by = c(&#39;FDT_STA_ID&#39;)) %&gt;% mutate(CriteriaValue = as.numeric(signif(`Chronic CriteriaValue` * 2.5, digits = 2)), # 4 day criteria = 2.5x chronic criteria parameterRound = signif(Value, digits = 2), # two significant figures based on 9VAC25-260-155 https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ Exceedance = ifelse(parameterRound &gt; CriteriaValue, 1, 0 ), # use 1/0 to easily summarize multiple results later WindowDateTimeStart = min(fourDayDataWindow$FDT_DATE_TIME)) %&gt;% dplyr::select(FDT_STA_ID, WindowDateTimeStart, everything(), -c(ID, `Chronic CriteriaValue` ) ) ) # now save data for later, in a format that matches with desired output fourDayDataCriteriaAnalysis &lt;- fourDayDataCriteriaAnalysis %&gt;% dplyr::select(-c(TempValue, pHValue)) %&gt;% bind_cols(tibble(associatedData = list(left_join(fourDayDataWindow, dplyr::select(fourDayDataCriteriaAnalysis, FDT_STA_ID, WindowDateTimeStart, TempValue, pHValue, Value), by = c(&#39;FDT_STA_ID&#39;, &#39;FDT_DATE_TIME&#39; = &#39;WindowDateTimeStart&#39;)))) ) # Save the results for viewing later fourDayCriteriaResults &lt;- bind_rows(fourDayCriteriaResults, fourDayDataCriteriaAnalysis) } else {fourDayCriteriaResults &lt;- fourDayCriteriaResults } combinedResults &lt;- bind_rows(acuteCriteriaResults, chronicCriteriaResults) %&gt;% {if(nrow(fourDayCriteriaResults) &gt; 0) bind_rows(., fourDayCriteriaResults) else .} %&gt;% arrange(WindowDateTimeStart, `Criteria Type`) } return( combinedResults)#bind_rows(acuteDataCriteriaAnalysis, chronicDataCriteriaAnalysis)) } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help # ammoniaAnalysisStation &lt;- freshwaterNH3limit(stationData, # trout = ifelse(unique(stationData$CLASS) %in% c(&#39;V&#39;,&#39;VI&#39;), TRUE, FALSE), # mussels = TRUE, # earlyLife = TRUE) # https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section155/ states the assumption is that # waters are to be assessed with the assumption that mussels and early life stages of fish should be present # trout presence is determined by WQS class, this can be changed in the app but is forced to be what the station # is attributed to in the automated assessment scripts 3.5.7.2 annualRollingExceedanceAnalysis Calculating the appropriate ammonia criteria is just the first part of assessing ammonia. To assess ammonia in free-flowing and tidal waters for Wildlife and Aquatic Life Uses, acute, chronic and four day criteria must not exceed respective criteria more than once every three years on the average. Helper functions have been established to assist with the automation of these decisions. The annualRollingExceedanceAnalysis() helper function takes in the output of the freshwaterNH3limit() function and an additional argument (yearsToRoll) that established the number of years to roll analyses over, for our use case we will use three years. The argument aquaticLifeUse allows this function to be used across multiple data input types. The default allows for the logic from the ammonia freshwaterNH3limit() function to be used to flag whether or not a window is valid for later assessment steps. An example of when one would change the default argument for aquaticLifeUse from FALSE to TRUE is if they were assessing freshwater chloride for the Aquatic Life Designated Use. When aquaticLifeUse is set to TRUE, the function uses the Aquatic Life Use definition of valid windows to flag windows for later assessment steps. The Assessment Guidance states: For toxic pollutant assessment of Aquatic Life Designated Use in free-flowing streams, both chronic and acute criteria can be assessed whenever sufficient data are available as applicable. Chronic criteria are to be assessed when multiple grab samples are collected within two separate four-day periods within a three-year period, or when there are two or more separate 30-day SPMD deployments within a three-year period. Two samples (either grab or SPMD) taken within three consecutive years are sufficient to assess acute criteria. The function extracts the year from each data window analyzed and drops any criteria that do not meet sample count rules (e.g. chronic or four day window results that were calculated on 1 or fewer samples). Based on the unique years of the input data windows and the yearsToRoll argument (3 in our case), the function loops over data within each yearsToRoll window range to calculate the number of exceedances in the rolled window (and whether or not the window is valid) by criteria type (e.g. acute, chronic, or four day) and appends all data used for the summary in a listcolumn called associatedData to simplify data review. Unique window ranges are not repeated thanks to the logic outlined in the windowRange range established for the for loop. # New rolling window analysis function. This calculates the number of exceedances in a X year window (not repeating windows) # and determines if the number of windows exceeding criteria are greater than the number of windows not exceeding criteria # using subsequent annualRollingExceedanceSummary() # analyze results by 3 year windows, not a strict rolling window, roll by year annualRollingExceedanceAnalysis &lt;- function(dataToAnalyze, yearsToRoll, # number of years to roll analysis over aquaticLifeUse = FALSE # whether or not this function is being used for Aquatic Life Use criteria # default = FALSE, which uses the Ammonia definition for window validity for chronic and four day sampling criteria # if TRUE, two samples across the rolled window length ensures a valid window ){ if(is.null(dataToAnalyze)){return(NULL)} # place to store results dataWindowResults &lt;- tibble(FDT_STA_ID = as.character(NA), `Window Begin` = as.numeric(NA), FDT_DEPTH = as.character(NA),# character so we can concatenate multiple depth values if needed #FDT_DEPTH = as.numeric(NA), `Criteria Type` = as.character(NA), `Years Analysis Rolled Over`= as.numeric(NA), `Exceedances in Rolled Window` = as.numeric(NA), #`Valid Chronic Window` = NA, associatedData = list()) dataToAnalyze &lt;- dataToAnalyze %&gt;% mutate(Year = year(WindowDateTimeStart), `Valid Window` = case_when(`Criteria Type` %in% c(&quot;Acute&quot;) ~ TRUE, # just to make people feel good later on `Criteria Type` %in% c(&quot;Chronic&quot;, &quot;Four Day&quot;) &amp; `Sample Count` &gt; 1 ~ TRUE, TRUE ~ NA)) # First, identify all window start options windowOptions &lt;- unique(year(dataToAnalyze$WindowDateTimeStart)) # Now, stop loop from repeating windows by capping the top end by yearsToRoll-1 (so don&#39;t have 3 yr windows exceeding assessment period) windowRange &lt;- (min(windowOptions):(max(windowOptions)- (yearsToRoll- 1))) # windowRange check to remove unnecessary windows that occur before the first windowRange year # e.g. if only one year of data is available, window range will return: year, year - 1, and year -2 which isn&#39;t necessary, so stop that behavior if(length(windowRange) &gt; 1 &amp; windowRange[2] &lt; windowRange[1]){ windowRange &lt;- windowRange[1] } for(i in windowRange){ #i = min(min(windowRange):(max(windowRange)- (yearsToRoll- 1))[1]) dataWindow &lt;- filter(dataToAnalyze, Year %in% i:(i + yearsToRoll- 1) ) # minus 1 year for math to work for(k in unique(dataWindow$`Criteria Type`)){ # this logic will skip years with no data dataWindowCriteria &lt;- filter(dataWindow, `Criteria Type` %in% k) dataWindowAnalysis &lt;- suppressMessages( dataWindowCriteria %&gt;% group_by(FDT_STA_ID, #FDT_DEPTH, `Criteria Type`) %&gt;% {if(aquaticLifeUse == FALSE) summarise(., `Criteria Type` = unique(`Criteria Type`), `Window Begin` = i , #year(min(WindowDateTimeStart)), `Years Analysis Rolled Over` = yearsToRoll, `Exceedances in Rolled Window` = sum(Exceedance), `Valid Window` = all(`Valid Window` == T)) else summarise(., `Criteria Type` = unique(`Criteria Type`), `Window Begin` = i, #year(min(WindowDateTimeStart)), `Years Analysis Rolled Over` = yearsToRoll, `Exceedances in Rolled Window` = sum(Exceedance), `Valid Window` = ifelse(nrow(dataWindowCriteria) &gt;= 2, TRUE, NA)) } %&gt;% bind_cols(tibble(associatedData = list(dataWindowCriteria))) ) dataWindowResults &lt;- bind_rows(dataWindowResults, dataWindowAnalysis) } } return(dataWindowResults) } # Example Usage: (demonstrating nested function syntax and pipes, both result in the same output): # where stationData objects are already in your environment, see Automated Assessment for environment set up help # annualRollingExceedanceAnalysis( # freshwaterNH3limit(stationData, # trout = ifelse(unique(stationData$CLASS) %in% c(&#39;V&#39;,&#39;VI&#39;), TRUE, FALSE), # mussels = TRUE, # earlyLife = TRUE), # yearsToRoll = 3, aquaticLifeUse = FALSE) # freshwaterNH3limit(stationData, # trout = ifelse(unique(stationData$CLASS) %in% c(&#39;V&#39;,&#39;VI&#39;), TRUE, FALSE), # mussels = TRUE, # earlyLife = TRUE) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = FALSE) # Freshwater Chloride Example: # annualRollingExceedanceAnalysis( # chlorideFreshwaterAnalysis(stationData), # yearsToRoll = 3, aquaticLifeUse = TRUE) # chlorideFreshwaterAnalysis(stationData) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = FALSE) 3.5.7.3 annualRollingExceedanceSummary The annualRollingExceedanceSummary() helper function summarizes the output of the annualRollingExceedanceAnalysis() function to suggest an ammonia result for Wildlife and Aquatic Life Designated uses. This function determines whether the number of exceeding windows are greater than the number of windows without exceedances and suggests a result based on that analysis. Before suggesting any results, the function ensures all data windows are valid (based upon flags chronic and four day sampling criteria analyzed in previous functions). After summarizing the window results by criteria type, the function suggests a result to the user where a Supporting output is provided only if the number of windows without exceedances are greater than the number of windows with exceedances. All other summaries result in a Review flag. # Flag if more windows exceeding than not annualRollingExceedanceSummary &lt;- function(rolledAnalysis){ if(is.null(rolledAnalysis)){return(NULL)} # make a valid dataset for this analysis validDataForExceedanceSummary &lt;- bind_rows( rolledAnalysis %&gt;% filter(str_detect(`Criteria Type`, &#39;Chronic&#39;) &amp; `Valid Window` == TRUE), # more flexible so metals can pass through this function rolledAnalysis %&gt;% filter(str_detect(`Criteria Type`, &#39;Acute&#39;)) ) %&gt;% {if(&quot;Four Day&quot; %in% unique(rolledAnalysis$`Criteria Type`)) bind_rows(., filter(rolledAnalysis, `Criteria Type` == &quot;Four Day&quot;&amp; `Valid Window` == TRUE) ) else . } %&gt;% {if(any(grep(&quot;All Other Surface Waters&quot;, unique(rolledAnalysis$`Criteria Type`))) ) bind_rows(., filter(rolledAnalysis, str_detect(`Criteria Type`, &quot;All Other Surface Waters&quot;) &amp; `Valid Window` == TRUE) ) else . } suppressMessages( validDataForExceedanceSummary %&gt;% group_by(FDT_STA_ID, FDT_DEPTH, `Criteria Type`) %&gt;% summarise(`n Windows Fine` = sum(`Exceedances in Rolled Window` &lt; 2), `n Windows Exceeding` = sum(`Exceedances in Rolled Window` &gt;= 2)) %&gt;% mutate(`Suggested Result` = case_when(`n Windows Exceeding` &gt;=2 ~ &quot;Impaired&quot;, `n Windows Exceeding` &lt; 2 &amp; `n Windows Fine` &gt; `n Windows Exceeding` ~ &quot;Supporting&quot;, `n Windows Exceeding` &lt; 2 &amp; `n Windows Fine` &lt; `n Windows Exceeding` ~ &quot;Review&quot;, `n Windows Exceeding` &lt; 2 &amp; `n Windows Fine` == `n Windows Exceeding` ~ &quot;Review&quot;, #`n Windows Exceeding` &gt;=2 ~ &quot;Impaired&quot;, TRUE ~ as.character(NA)))) } # Example Usage: (demonstrating nested function syntax and pipes, both result in the same output): # where stationData objects are already in your environment, see Automated Assessment for environment set up help # annualRollingExceedanceSummary( # annualRollingExceedanceAnalysis( # freshwaterNH3limit(stationData, # trout = ifelse(unique(stationData$CLASS) %in% c(&#39;V&#39;,&#39;VI&#39;), TRUE, FALSE), # mussels = TRUE, # earlyLife = TRUE), # yearsToRoll = 3, aquaticLifeUse = FALSE) # ) # freshwaterNH3limit(stationData, # trout = ifelse(unique(stationData$CLASS) %in% c(&#39;V&#39;,&#39;VI&#39;), TRUE, FALSE), # mussels = TRUE, # earlyLife = TRUE) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = FALSE) %&gt;% # annualRollingExceedanceSummary() # Freshwater Chloride Example: # annualRollingExceedanceSummary( # annualRollingExceedanceAnalysis( # chlorideFreshwaterAnalysis(stationData), # yearsToRoll = 3, aquaticLifeUse = TRUE) ) # chlorideFreshwaterAnalysis(stationData) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = FALSE) %&gt;% # annualRollingExceedanceSummary() 3.5.7.4 rollingWindowSummary The rollingWindowSummary() helper function summarizes the output of the annualRollingExceedanceSummary() function for a simplified output for the Station Table. The nested series of functions that analyzed the ammonia data are summarized in an output of two columns, detailing the number of exceedances and the suggested parameter status. As of the writing of this document, the value desired in the number of exceedances column has not been determined (potentially the number of windows with exceedances or the number of exceedances across all windows), so to be conservative, this is left as NA right now and requires input by assessment staff before upload into CEDS. However, the parameter status is summarized as such. Should a Review result be contained in any of the acute, chronic, or four day results, then the parameter status will be set to Review to signify to the assessor that further review is needed for the decision. If all criteria assessed contain Supporting results, then the parameter status will suggest the parameter status is supporting (S). The parameterAbbreviation argument passes a character string of the chosen parameter name to the Station Table output column headers. # Rolling summary for stations table rollingWindowSummary &lt;- function(annualRollingExceedanceSummaryOutput, parameterAbbreviation){ if(&quot;Review&quot; %in% unique(annualRollingExceedanceSummaryOutput$`Suggested Result`)){ z &lt;- tibble(`_EXC` = as.numeric(NA), `_STAT` = &#39;Review&#39;) } else { # first kick out NULL entries (no data to analyze) if(is.null(annualRollingExceedanceSummaryOutput)){ z &lt;- tibble(`_EXC` = as.numeric(NA), `_STAT` = as.character(NA)) } else { z &lt;- tibble(`_EXC` = as.numeric(NA), `_STAT` = &quot;S&quot;)} } names(z) &lt;- paste0(parameterAbbreviation, names(z)) return(z) } # Example Usage: (demonstrating nested function syntax and pipes, both result in the same output): # where stationData objects are already in your environment, see Automated Assessment for environment set up help # rollingWindowSummary( # annualRollingExceedanceSummary( # annualRollingExceedanceAnalysis( # freshwaterNH3limit(stationData, # trout = ifelse(unique(stationData$CLASS) %in% c(&#39;V&#39;,&#39;VI&#39;), TRUE, FALSE), # mussels = TRUE, # earlyLife = TRUE), # yearsToRoll = 3, aquaticLifeUse = FALSE) # ), # parameterAbbreviation = &quot;AMMONIA&quot; # ) # freshwaterNH3limit(stationData, # trout = ifelse(unique(stationData$CLASS) %in% c(&#39;V&#39;,&#39;VI&#39;), TRUE, FALSE), # mussels = TRUE, # earlyLife = TRUE) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = FALSE) %&gt;% # annualRollingExceedanceSummary() %&gt;% # rollingWindowSummary(parameterAbbreviation = &quot;AMMONIA&quot;) # Freshwater Chloride Example: # rollingWindowSummary( # annualRollingExceedanceSummary( # annualRollingExceedanceAnalysis( # chlorideFreshwaterAnalysis(stationData), # yearsToRoll = 3, aquaticLifeUse = TRUE) ), # parameterAbbreviation = &quot;CHL&quot;) # chlorideFreshwaterAnalysis(stationData) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = FALSE) %&gt;% # annualRollingExceedanceSummary() %&gt;% # rollingWindowSummary(parameterAbbreviation = &quot;CHL&quot;) 3.5.8 Water Column Toxics Since there is no place in the Station Table for individual Public Water Supply, freshwater chloride, or water column PCB criteria results, the WAT_TOX fields summarize exceedances for PWS criteria, freshwater chloride aquatic life use, and water column PCB status. The code snippet below (see Automated Assessment Analysis) outlines how these criteria results are combined into a single field in the Station Table. It is highly advised that assessment staff use this field as a flag when reviewing a station summary from the Station Table using the appropriate waterbody-specific shiny assessment application to fully understand which criteria may be providing the WAT_TOX flag. See the Public Water Supply Criteria, Freshwater Chloride, and PCB sections below for details on individual parameter functions demonstrated in the below chunk. # code snippet from Automated Assessment Analysis # where stationData objects are already in your environment, see Automated Assessment for environment set up help # PWS Human Health Criteria if(nrow(stationData) &gt; 0){ if(is.na(unique(stationData$PWS)) ){ PWSconcat &lt;- tibble(PWS= NA) } else { PWSconcat &lt;- cbind(assessPWSsummary(assessPWS(stationData, NITRATE_mg_L, LEVEL_NITRATE, 10), &#39;PWS_Nitrate&#39;), assessPWSsummary(assessPWS(stationData, CHLORIDE_mg_L, LEVEL_CHLORIDE, 250), &#39;PWS_Chloride&#39;), assessPWSsummary(assessPWS(stationData, SULFATE_TOTAL_mg_L, LEVEL_SULFATE_TOTAL, 250), &#39;PWS_Total_Sulfate&#39;)) %&gt;% dplyr::select(-ends_with(&#39;exceedanceRate&#39;)) } # Freshwater Chloride Aquatic Life assessment, if data exists if(nrow(filter(stationData, !is.na(CHLORIDE_mg_L))) &gt; 0){ chlorideFreshwater &lt;- rollingWindowSummary( annualRollingExceedanceSummary( annualRollingExceedanceAnalysis(chlorideFreshwaterAnalysis(stationData), yearsToRoll = 3, aquaticLifeUse = TRUE) ), &quot;CHL&quot;) } else {chlorideFreshwater &lt;- tibble(CHL_EXC = NA, CHL_STAT= NA)} # Water toxics combination with PWS, Chloride Freshwater, and water column PCB data if(nrow(bind_cols(PWSconcat, chlorideFreshwater, PCBmetalsDataExists(filter(markPCB, str_detect(SampleMedia, &#39;Water&#39;)) %&gt;% filter(StationID %in% stationData$FDT_STA_ID), &#39;WAT_TOX&#39;)) %&gt;% dplyr::select(contains(c(&#39;_EXC&#39;,&#39;_STAT&#39;))) %&gt;% mutate(across( everything(), as.character)) %&gt;% pivot_longer(cols = contains(c(&#39;_EXC&#39;,&#39;_STAT&#39;)), names_to = &#39;parameter&#39;, values_to = &#39;values&#39;, values_drop_na = TRUE) %&gt;% filter(! str_detect(values, &#39;WQS info missing from analysis&#39;)) %&gt;% filter(! values == &quot;S&quot;)) &gt;= 1) { WCtoxics &lt;- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = &#39;Review&#39;) } else { WCtoxics &lt;- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = NA)} } else { WCtoxics &lt;- tibble(WAT_TOX_EXC = NA, WAT_TOX_STAT = NA) } 3.5.9 Public Water Supply Criteria Public Water Supply (PWS) human health criteria are assessed using the assessPWS() function and summarized for the Station Table by assessPWSsummary() function. These functions operate across PWS parameters, using non-standard evaluation techniques to allow users to specify which parameter to evaluate and the associated criteria using function arguments. If the WQS metadata linked to the station is designated as a PWS segment, the assessPWS() function first removes all Level I and Level II data and missing data before the median of measures collected over a six year assessment period is calculated. The parameter six year median is rounded to even, compared against the provided criteria, and flagged if the median exceeds the provided criteria limit. Chloride, Sulfate, Total Dissolved Solids, Iron, and Foaming Agents are secondary criteria and are only applicable to data collected at the drinking water intake. To be most protective, the automated assessment process evaluates all PWS designated segments for each of these criteria if data exists, regardless of the proximity of the station to the drinking water supply intake. It is the responsibility of the assessment staff to remove these decisions from the Station Table when they are not applicable. The waterbody-specific shiny assessment applications flag stations within 100 meters from a drinking water intake. assessPWS &lt;- function(stationData, fieldName, commentName, PWSlimit){ if(unique(stationData$PWS) %in% c(&quot;Yes&quot;)){ fieldName_ &lt;- enquo(fieldName) commentName_ &lt;- enquo(commentName) parameterData &lt;- dplyr::select(stationData, FDT_DATE_TIME, !! fieldName_, !! commentName_) %&gt;% filter(!( !! commentName_ %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out filter(!is.na(!!fieldName_ )) %&gt;% #get rid of NA&#39;s mutate(`Parameter Median` = median(!! fieldName_), `Parameter Rounded to WQS Format` = signif(`Parameter Median`, digits = 2), # two significant figures based on WQS https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section140/ limit = PWSlimit) %&gt;% rename(parameter = !!names(.[5])) %&gt;% # rename columns to make functions easier to apply mutate(exceeds = ifelse(parameter &gt; limit, T, F)) # Identify where above WQS limit return(parameterData) } return(NULL) } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help #assessPWS(stationData, NITRATE_mg_L, LEVEL_NITRATE, 10) #assessPWS(stationData, CHLORIDE_mg_L, LEVEL_CHLORIDE, 250) #assessPWS(stationData, SULFATE_TOTAL_mg_L, LEVEL_SULFATE_TOTAL, 250) The output of the assessPWS() function becomes the input for the assessPWSsummary() function when the user desires the results to be summarized for the Station Table. assessPWSsummary &lt;- function(assessPWSresults, outputName){ if(!is.null(assessPWSresults)){ return(quickStats(assessPWSresults, outputName)) } return(quickStats(tibble(limit = NA), outputName)) } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help #assessPWSsummary(assessPWS(stationData, NITRATE_mg_L, LEVEL_NITRATE, 10), &#39;PWS_Nitrate&#39;) #assessPWSsummary(assessPWS(stationData, CHLORIDE_mg_L, LEVEL_CHLORIDE, 250), &#39;PWS_Chloride&#39;) #assessPWSsummary(assessPWS(stationData, SULFATE_TOTAL_mg_L, LEVEL_SULFATE_TOTAL, 250), &#39;PWS_Total_Sulfate&#39;) 3.5.10 Freshwater Chloride Chloride is assessed for Aquatic Life Uses using acute and chronic criteria only in freshwater environments. The evaluation of chloride data against acute and chronic criteria is performed by the chlorideFreshwaterAnalysis() function while the summarization of results to conduct an assessment decision is performed by the nested annualRollingExceedanceAnalysis(), annualRollingExceedanceSummary(), and rollingWindowSummary() functions. These functions are detailed for ammonia, but the same logic rules apply to any parameter dataset provided to the function logic. An example of the freshwater chloride analysis and summarization for the Station Table are provided in the chunk below. The chlorideFreshwaterAnalysis() function first ensures the station provided to the function is designated a freshwater or transitional before removing all Level I and Level II data and missing data. If data exist after those initial filters are applied, subsequent logic may be applied, if not, the function returns NULL. The function then creates storage objects for acute and chronic average results and loops through each row of data to correctly calculate acute criteria and find any chronic scenarios (if data exist). Each new sample event prompts the creation of acute and chronic data windows. If sufficient data exist in any of these windows, the appropriate criteria are applied to the window chloride average, rounded to even. To be most protective, chronic windows are calculated if one or more measure exists in any data window; this information is useful for assessor visualization and understanding in the application environment, but these results based on only one measure are culled from assessment decisions in later functions. The data analyzed within each window is saved as a listcolumn called associatedData that may be extracted for investigation later. The output of the function is a long dataset where each unique sample event could have a row summarizing the acute and chronic criteria, sample count in the given window, exceedance result, and associated data (associatedData listcolumn). chlorideFreshwaterAnalysis &lt;- function(stationData){ # doesn&#39;t apply in class II transition zone stationDataCHL &lt;- filter(stationData, CLASS %in% c(&#39;III&#39;, &quot;IV&quot;,&quot;V&quot;,&quot;VI&quot;,&quot;VII&quot;) | CLASS == &quot;II&quot; &amp; ZONE == &#39;Tidal Fresh&#39;) %&gt;% filter(!(LEVEL_CHLORIDE %in% c(&#39;Level II&#39;, &#39;Level I&#39;))) %&gt;% # get lower levels out filter(!is.na(CHLORIDE_mg_L)) #get rid of NA&#39;s if(nrow(stationDataCHL) &gt; 0){ # make a place to store analysis results acuteCriteriaResults &lt;- tibble(FDT_STA_ID = as.character(NA), WindowDateTimeStart = as.POSIXct(NA), FDT_DEPTH = as.numeric(NA), Value = as.numeric(NA), ValueType = as.character(NA), `Criteria Type` = as.character(NA), CriteriaValue = as.numeric(NA), `Sample Count` = as.numeric(NA), parameterRound = as.numeric(NA), Exceedance = as.numeric(NA), associatedData = list()) chronicCriteriaResults &lt;- acuteCriteriaResults # loop through each row of data to correctly calculate criteria and find any chronic scenarios for(k in stationDataCHL$FDT_DATE_TIME){ #k = stationDataCHL$FDT_DATE_TIME[1] acuteDataWindow &lt;- filter(stationDataCHL, between(FDT_DATE_TIME, k, k + hours(1))) chronicDataWindow &lt;- filter(stationDataCHL, between(FDT_DATE_TIME, k, k + days(4))) # Run acute analysis if data exists if(nrow(acuteDataWindow) &gt; 0){ acuteDataCriteriaAnalysis &lt;- suppressMessages( dplyr::select(acuteDataWindow, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, CHLORIDE_mg_L) %&gt;% group_by(FDT_STA_ID, FDT_DEPTH) %&gt;% # can&#39;t group by datetime or summary can&#39;t happen summarise(Value = mean(CHLORIDE_mg_L, na.rm=T), # get hourly average `Sample Count` = length(CHLORIDE_mg_L)) %&gt;% #count sample that made up average mutate(ValueType = &#39;Hourly Average&#39;, ID = paste( FDT_STA_ID, FDT_DEPTH, sep = &#39;_&#39;), # make a uniqueID in case &gt;1 sample for given datetime `Criteria Type` = &#39;Acute&#39;, CriteriaValue = 860) %&gt;% # 860,000ug/L criteria to mg/L mutate(parameterRound = signif(Value, digits = 2), # two significant figures based on WQS https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section140/ Exceedance = ifelse(parameterRound &gt; CriteriaValue, 1, 0 ), # use 1/0 to easily summarize multiple results later WindowDateTimeStart = min(acuteDataWindow$FDT_DATE_TIME)) %&gt;% dplyr::select(FDT_STA_ID, WindowDateTimeStart, everything(), -ID) ) %&gt;% bind_cols(tibble(associatedData = list(dplyr::select(acuteDataWindow, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, CHLORIDE_mg_L) ) )) # Save the results for viewing later acuteCriteriaResults &lt;- bind_rows(acuteCriteriaResults, acuteDataCriteriaAnalysis) } else {acuteCriteriaResults &lt;- acuteCriteriaResults } # Run chronic analysis if data exists if(nrow(chronicDataWindow) &gt; 0){ chronicDataCriteriaAnalysis &lt;- suppressMessages( dplyr::select(chronicDataWindow, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, CHLORIDE_mg_L) %&gt;% group_by(FDT_STA_ID, FDT_DEPTH) %&gt;% # can&#39;t group by datetime or summary can&#39;t happen summarise(Value = mean(CHLORIDE_mg_L, na.rm=T), # get hourly average `Sample Count` = length(CHLORIDE_mg_L)) %&gt;% #count sample that made up average mutate(ValueType = &#39;Four Day Average&#39;, ID = paste( FDT_STA_ID, FDT_DEPTH, sep = &#39;_&#39;), # make a uniqueID in case &gt;1 sample for given datetime `Criteria Type` = &#39;Chronic&#39;, CriteriaValue = 230) %&gt;% # 230,000ug/L criteria to mg/L mutate(parameterRound = signif(Value, digits = 2), # two significant figures based on WQS https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section140/ Exceedance = ifelse(parameterRound &gt; CriteriaValue, 1, 0 ), # use 1/0 to easily summarize multiple results later WindowDateTimeStart = min(chronicDataWindow$FDT_DATE_TIME)) %&gt;% dplyr::select(FDT_STA_ID, WindowDateTimeStart, everything(), -ID) ) %&gt;% bind_cols(tibble(associatedData = list(dplyr::select(chronicDataWindow, FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH, CHLORIDE_mg_L) ) )) # Save the results for viewing later chronicCriteriaResults &lt;- bind_rows(chronicCriteriaResults, chronicDataCriteriaAnalysis) } else {chronicCriteriaResults &lt;- chronicCriteriaResults } } # summarize results stationCriteriaResults &lt;- bind_rows( acuteCriteriaResults, chronicCriteriaResults) %&gt;% filter(!is.na(FDT_STA_ID)) %&gt;% # drop placeholder rows distinct(FDT_STA_ID, WindowDateTimeStart, FDT_DEPTH, `Criteria Type`, .keep_all = T) %&gt;% # remove duplicates in case &gt; 1 depth per datetime arrange(FDT_STA_ID, WindowDateTimeStart, FDT_DEPTH, `Criteria Type`) return(stationCriteriaResults) } else { return(NULL)} } # Example Usage: # where stationData objects are already in your environment, see Automated Assessment for environment set up help # chlorideFreshwaterAnalysis(stationData) # Example Usage for Station Table (demonstrating nested function syntax and pipes, both result in the same output): # rollingWindowSummary( # annualRollingExceedanceSummary( # annualRollingExceedanceAnalysis( # chlorideFreshwaterAnalysis(stationData), # yearsToRoll = 3, aquaticLifeUse = TRUE) ), # parameterAbbreviation = &quot;CHL&quot;) # chlorideFreshwaterAnalysis(stationData) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = FALSE) %&gt;% # annualRollingExceedanceSummary() %&gt;% # rollingWindowSummary(parameterAbbreviation = &quot;CHL&quot;) 3.5.11 PCB The PCBmetalsDataExists() function is used primarily for flagging whether or not certain data exist when building the Station Table. Most stations evaluated during an assessment period do not have metals or toxics data, so it is important to flag when data require review. This particular function can be used to flag when data exist across water column PCB, sediment PCB, fish tissue metals, and fish tissue PCB data. When data exists for these toxics parameters, regional assessment staff may investigate them against applicable thresholds inside the waterbody-specific shiny assessment application. ## Identify if further review needs to happen for PCB or metals data PCBmetalsDataExists &lt;- function(datasetType, # any of the preorganized PCB or fish tissue datasets parameterType # field name to pass through to Station Table output ){ # if any data given to function if(nrow(datasetType) &gt; 0){ x &lt;- data.frame(EXC = NA, STAT = &#39;Review&#39;) }else { x &lt;- data.frame(EXC = NA, STAT = NA) } names(x) &lt;- paste(parameterType, names(x), sep=&#39;_&#39;) return(x) } # Example Usage: # where markPCB, fishMetals, fishPCB objects are already in your environment, see Automated Assessment for environment set up help # PCBmetalsDataExists(filter(markPCB, str_detect(SampleMedia, &#39;Water&#39;)) %&gt;% # filter(StationID %in% &#39;2-JKS023.61&#39;), &#39;WAT_TOX&#39;) # PCBmetalsDataExists(filter(markPCB, str_detect(SampleMedia, &#39;Sediment&#39;)) %&gt;% # filter(StationID %in% &#39;2-JKS023.61&#39;), &#39;SED_TOX&#39;) # PCBmetalsDataExists(filter(fishMetals, Station_ID %in% &#39;2-JKS023.61&#39;), &#39;FISH_MET&#39;) # PCBmetalsDataExists(filter(fishPCB, `DEQ rivermile` %in% &#39;2-JKS023.61&#39;), &#39;FISH_TOX&#39;) 3.5.12 Water Column Metals There are five functions that support the surface water metals assessment. These functions are used to determine criteria, identify exceedances of those criteria, apply rolling windows to these exceedances, and report these results in the stations table format. Surface water metals criteria are dependent on the designated use (aquatic life or human health). The aquatic life use is subdivided into freshwater and saltwater, which are further subdivided into acute and chronic criteria. The human health designated use is subdivided into public water supply and all other surface waters. Some criteria are specified outright, but a subset of criteria require calculation, which can involve a combination of variables (i.e., hardness, water effect ratio, and correction factor). For more information on surface water metals criteria see 9VAC25-260-140. The functions that are used to assess surface water metals are further described below. Note that the metals assessed for Aquatic Life Use designations use the rolling window analysis procedure detailed in the Ammonia section. 3.5.12.1 metalsCriteriaFunction This function is used to determine the criteria for a given sample, which are then applied against the metal concentrations of that sample. The function takes inputs of ID, Hardness, and WER. The ID field is a unique identifier for a sampling event. Hardness represents the hardness of the water associated with the sampling event in question. WER is the water effects ratio, which is determined by measuring the effect of receiving water (as it is or will be affected by any discharges) on the bioavailability or toxicity of a metal by using standard test organisms and a metal to conduct toxicity tests simultaneously in receiving water and laboratory water. The WER is set to 1 unless an applicant or permittee demonstrates to the departments satisfaction in a permit proceeding that another value is appropriate, or unless available data allow the department to compute a WER for the receiving waters. The formulae used in this function to produce the criteria were extracted from 9VAC25-260-140. Output criteria are set to 2 significant figures. This function is used as part of metalsAnalysis() , a separate function that is described below. # Metals criteria analysis metalsCriteriaFunction &lt;- function(ID, Hardness, WER){ # Remember: ln is really log() in R; exp() is natural antilog in R # Per 9VAC25-260-140, criteria to 2 sig figs #https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section140/ # If no WER supplied, use 1 WER &lt;- ifelse(is.na(WER), 1, WER) # Establish Hardness Criteria criteriaHardness &lt;- ifelse(Hardness &lt; 25, 25, ifelse(Hardness &gt; 400, 400, Hardness)) metalsCriteria &lt;- suppressWarnings( tibble(ID= ID, `Antimony PWS` = 5.6, `Antimony All Other Surface Waters` = 640, `Arsenic Acute Freshwater` = 340, `Arsenic Chronic Freshwater` = 150, `Arsenic PWS` = 10, `Arsenic Acute Saltwater` = 69, `Arsenic Chronic Saltwater` = 36, `Barium PWS` = 2000, `Cadmium Acute Freshwater` = signif(WER * exp(0.9789 * (log(criteriaHardness))-3.866) * (1.136672 - (log(criteriaHardness) * 0.041838)), digits = 2), `Cadmium Chronic Freshwater` = signif(WER * exp(0.7977 * log(criteriaHardness) - 3.909) * (1.101672 - (log(criteriaHardness) * (0.041838))), digits = 2), `Cadmium Acute Saltwater` = signif(33 * WER, digits = 2), `Cadmium Chronic Saltwater` = signif(7.9 * WER, digits = 2), `Cadmium PWS` = 5, `ChromiumIII Acute Freshwater` = signif(WER * (exp(0.8190 * (log(criteriaHardness)) + 3.7256)) * 0.316, digits = 2), `ChromiumIII Chronic Freshwater` = signif(WER * (exp(0.8190 * (log(criteriaHardness)) +0.6848)) * 0.860, digits = 2), `ChromiumIII PWS` = 100, `ChromiumVI Acute Freshwater` = 16, `ChromiumVI Chronic Freshwater` = 11, `ChromiumVI Acute Saltwater` = 1100, `ChromiumVI Chronic Saltwater` = 50, `Copper Acute Freshwater` = signif(WER * (exp(0.9422 * log(criteriaHardness) - 1.700)) * 0.960, digits = 2), `Copper Chronic Freshwater` = signif(WER * (exp(0.8545 * log(criteriaHardness) - 1.702)) * 0.960, digits = 2), `Copper Acute Saltwater` = signif(9.3 * WER, digits = 2), `Copper Chronic Saltwater` = signif(6.0 * WER, digits = 2), `Copper PWS` = 1300, `Lead Acute Freshwater` = signif(WER * (exp(1.273 * log(criteriaHardness) - 1.084)) * (1.46203 - (log(criteriaHardness) * 0.145712)), digits = 2), `Lead Chronic Freshwater` = signif(WER * (exp(1.273 * log(criteriaHardness) - 3.259)) * (1.46203 - (log(criteriaHardness) * 0.145712)), digits = 2), `Lead Acute Saltwater` = signif(230 * WER, digits = 2), `Lead Chronic Saltwater` = signif(8.8 * WER, digits = 2), `Lead PWS` = 15, `Mercury Acute Freshwater` = 1.4, `Mercury Chronic Freshwater` = 0.77, `Mercury Acute Saltwater` = 1.8, `Mercury Chronic Saltwater` = 0.94, `Nickel Acute Freshwater` = signif(WER * (exp (0.8460 * log(criteriaHardness) + 1.312)) * 0.998, digits = 2), `Nickel Chronic Freshwater` = signif(WER * (exp(0.8460 * log(criteriaHardness) - 0.8840)) * 0.997, digits = 2), `Nickel Acute Saltwater` = signif(74 * WER, digits = 2), `Nickel Chronic Saltwater` = signif(8.2 * WER, digits = 2), `Nickel PWS` = 610, `Nickel All Other Surface Waters` = 4600, `Uranium PWS` = 30, `Selenium Acute Freshwater` = 20, `Selenium Chronic Freshwater` = 5.0, `Selenium Acute Saltwater` = signif(290 * WER, digits = 2), `Selenium Chronic Saltwater` = signif(71 * WER, digits = 2), `Selenium PWS` = 170, `Selenium All Other Surface Waters` = 4200, `Silver Acute Freshwater` = signif(WER * (exp(1.72 * log(criteriaHardness) - 6.52)) * 0.85, digits = 2), `Silver Acute Saltwater` = signif(1.9 * WER, digits = 2), `Thallium PWS` = 0.24, `Thallium All Other Surface Waters` = 0.47, `Zinc Acute Freshwater` = signif(WER * (exp(0.8473 * log(criteriaHardness) + 0.884)) * 0.978, digits = 2), `Zinc Chronic Freshwater` = signif(WER * (exp(0.8473 * log(criteriaHardness) + 0.884)) * 0.986, digits = 2), `Zinc Acute Saltwater` = signif(90 * WER, digits = 2), `Zinc Chronic Saltwater` = signif(81 * WER, digits = 2), `Zinc PWS` = 7400, `Zinc All Other Surface Waters` = 26000) %&gt;% pivot_longer(!ID, names_to = &#39;Criteria&#39;, values_to = &#39;CriteriaValue&#39;) %&gt;% mutate(Criteria2 = Criteria) %&gt;% #duplicate column to split separate(Criteria2, c(&quot;Metal&quot;, &quot;Criteria Type&quot;, &quot;Waterbody&quot;), sep = &quot; &quot;) %&gt;% mutate(`Criteria Type` = ifelse(`Criteria Type` == &#39;All&#39;, &#39;All Other Waters&#39;, `Criteria Type`), Waterbody = ifelse(Waterbody == &#39;Other&#39;, NA, Waterbody)) %&gt;% dplyr::select(ID, Metal, Criteria, `Criteria Type`, Waterbody, CriteriaValue)) return(metalsCriteria) } 3.5.12.2 metalsAnalysis This function identifies samples within relevant data windows, calculates averages of the data within those windows where necessary (acute and chronic criteria), determines the appropriate criteria to apply using metalsCriteriaFunction(), and tallies exceedances based on those criteria. metalsAnalysis() approaches metals assessment by separating the analysis into 3 bins: raw, acute, and chronic. The function loops across individual samples for the station under consideration and creates a dataframe based on the data windows for raw, acute, and chronic criteria. The raw data window simply uses the time that the current sample in the loop was collected. The acute data window will include all site-specific samples taken within an hour of the sample under consideration, whereas the chronic data window will include all samples taken within four days of the sample under consideration. After these data windows are created for an individual sample, the process described below occurs in succession: first for the raw data, then for acute data, and ending with chronic data. The loop then repeats this process for the next sample at the station under consideration, starting with the creation of a new set of data windows specific to that sample. The process is as follows: The data are reshaped to facilitate evaluation against criteria. This involves pivoting the data into long format, dropping unnecessary fields, and adding fields used for identification. A mean value is calculated (only true for acute and chronic assessment). The criteria are determined using metalsCriteriaFunction(). The ID, hardness, and WER for the sample under consideration are used as inputs to this function. The sample values are rounded to 2 significant figures and compared to the criteria (also set to 2 significant figures). Exceedances for specific metals are flagged with a value of 1 in the Exceedance column and a 0 is returned to this column if there is no exceedance for a given metal. After this process is completed for all samples at a station, the results from raw, acute, and chronic metals assessment are combined into a single dataframe (stationCriteriaResults), which is the output of the function. Output from the metalsAnalysis() function are manipulated to utilize the annualRollingExceedanceAnalysis() and annualRollingExceedanceSummary() functionality. An example of this workflow is included at the end of the code chunk below. The result of metals analyses analyzed by the annualRollingExceedanceSummary() function is then sent to the metalsAssessmentFunction() function. # For use with rolling aquatic life use method metalsAnalysis &lt;- function(stationMetalsData, stationData, WER){ # If no WER supplied, use 1 WER &lt;- ifelse(is.na(WER), 1, WER) # Get WQS from stationData so correct criteria can be applied stationMetalsData &lt;- left_join(stationMetalsData, dplyr::select(stationData, FDT_STA_ID, CLASS, PWS, ZONE) %&gt;% distinct(FDT_STA_ID, .keep_all = TRUE), by = c(&#39;Station_Id&#39; = &#39;FDT_STA_ID&#39;)) %&gt;% mutate(`Assess As` = case_when(CLASS == &quot;I&quot; ~ &#39;Saltwater&#39;, CLASS == &quot;II&quot; &amp; ZONE == &#39;Estuarine&#39; ~ &#39;Saltwater&#39;, CLASS == &quot;II&quot; &amp; ZONE == &#39;Transition&#39; ~ &#39;More Stringent&#39;, CLASS == &quot;II&quot; &amp; ZONE == &#39;Tidal Fresh&#39; ~ &#39;Freshwater&#39;, CLASS %in% c(&#39;III&#39;, &quot;IV&quot;,&quot;V&quot;,&quot;VI&quot;,&quot;VII&quot;) ~ &#39;Freshwater&#39;, TRUE ~ as.character(NA)), ChromiumIII= Chromium, RMK_ChromiumIII = RMK_Chromium, ChromiumVI= Chromium, RMK_ChromiumVI = RMK_Chromium ) %&gt;% # add individual Chromium variables to make joining to assessment criteria easier # Roger uses ChromiumIII and VI to flag any potential chromium issues, likely further lab analyses needed if either chromium criteria blown dplyr::select(Station_Id:RMK_Cadmium, ChromiumIII:RMK_ChromiumVI, Cadmium:`Assess As`) # make a place to store raw analysis results rawCriteriaResults &lt;- tibble(Station_Id = as.character(NA), WindowDateTimeStart = as.POSIXct(NA), FDT_DEPTH = as.numeric(NA), CLASS = as.factor(NA), PWS = as.factor(NA), ZONE = as.factor(NA), `Assess As` = as.character(NA), Metal = as.character(NA), Value = as.numeric(NA), ValueType = as.character(NA), Criteria = as.character(NA), `Criteria Type` = as.character(NA), Waterbody = as.character(NA), CriteriaValue = as.numeric(NA), `Sample Count` = as.numeric(NA), parameterRound = as.numeric(NA), Exceedance = as.numeric(NA)) acuteCriteriaResults &lt;- rawCriteriaResults chronicCriteriaResults &lt;- acuteCriteriaResults # loop through each row of data to correctly calculate criteria and find any chronic scenarios for(k in stationMetalsData$FDT_DATE_TIME){ rawDataWindow &lt;- filter(stationMetalsData, FDT_DATE_TIME == k) acuteDataWindow &lt;- filter(stationMetalsData, between(FDT_DATE_TIME, k, k + hours(1))) chronicDataWindow &lt;- filter(stationMetalsData, between(FDT_DATE_TIME, k, k + days(4))) # Run any analyses requiring raw data if data exists if(nrow(rawDataWindow) &gt; 0){ rawData &lt;- rawDataWindow %&gt;% group_by(Station_Id, FDT_DATE_TIME, FDT_DEPTH, CLASS, PWS, ZONE, `Assess As`) %&gt;% dplyr::select(-c(contains(&#39;RMK_&#39;))) %&gt;% pivot_longer(cols = Antimony:Hardness, names_to = &quot;Metal&quot;, values_to = &quot;Value&quot;) %&gt;% mutate(ValueType = &#39;Raw Result&#39;, ID = paste(Station_Id, FDT_DATE_TIME, FDT_DEPTH, sep = &#39;_&#39;)) %&gt;% # make a uniqueID in case &gt;1 sample for given datetime ungroup() # Calculate criteria based on raw data rawDataCriteria &lt;- metalsCriteriaFunction(filter(rawData, Metal == &quot;Hardness&quot;)$ID, filter(rawData, Metal == &quot;Hardness&quot;)$Value, WER = 1) %&gt;% filter(`Criteria Type` %in% c(&#39;All Other Waters&#39;, &#39;PWS&#39;)) %&gt;% # don&#39;t need other criteria for acute window {if(is.na(unique(rawData$PWS))) filter(., `Criteria Type` != &#39;PWS&#39;) else .} # Join appropriate criteria to rawData for comparison to averaged data rawDataCriteriaAnalysis &lt;- left_join(rawData, rawDataCriteria, by = c(&#39;ID&#39;, &#39;Metal&#39;)) %&gt;% mutate(`Sample Count` = 1, # will be 1 here parameterRound = signif(Value, digits = 2), # two significant figures based on WQS https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section140/ Exceedance = ifelse(parameterRound &gt; CriteriaValue, 1, 0 ), WindowDateTimeStart = min(rawDataWindow$FDT_DATE_TIME)) %&gt;% # use 1/0 to easily summarize multiple results later filter(!is.na(Criteria)) %&gt;% # filter out metals that don&#39;t have chronic criteria dplyr::select(Station_Id, WindowDateTimeStart, everything()) %&gt;% dplyr::select(-c(FDT_DATE_TIME, ID)) # Save the results for viewing later rawCriteriaResults &lt;- bind_rows(rawCriteriaResults, rawDataCriteriaAnalysis) } else {rawCriteriaResults &lt;- rawCriteriaResults } # Run acute analysis if data exists if(nrow(acuteDataWindow) &gt; 0){ acuteData &lt;- suppressMessages(acuteDataWindow %&gt;% group_by(Station_Id, FDT_DEPTH, CLASS, PWS, ZONE, `Assess As`) %&gt;% # can&#39;t group by datetime or summary can&#39;t happen dplyr::select(-c(contains(&#39;RMK_&#39;))) %&gt;% pivot_longer(cols = Antimony:Hardness, names_to = &quot;Metal&quot;, values_to = &quot;CriteriaValue&quot;) %&gt;% ungroup() %&gt;% group_by(Station_Id, FDT_DEPTH, CLASS, PWS, ZONE, `Assess As`, Metal) %&gt;% summarise(`Sample Count` = length(CriteriaValue), Value = mean(CriteriaValue, na.rm=T)) %&gt;% # get hourly average mutate(ValueType = &#39;Hourly Average&#39;, ID = paste(Station_Id, FDT_DEPTH, sep = &#39;_&#39;)) ) # make a uniqueID in case &gt;1 sample for given datetime # Calculate criteria based on hourly averaged data acuteDataCriteria &lt;- metalsCriteriaFunction(filter(acuteData, Metal == &quot;Hardness&quot;)$ID, filter(acuteData, Metal == &quot;Hardness&quot;)$Value, WER = 1) %&gt;% filter(`Criteria Type` == &#39;Acute&#39;) %&gt;% # don&#39;t need other criteria for acute window # Keep only the criteria needed {if(unique(acuteData$`Assess As`) %in% c(&#39;Freshwater&#39;, &#39;Saltwater&#39;)) filter(., Waterbody %in% c(NA, !!unique(acuteData$`Assess As`))) # if in Transition Zone then use the more stringent standard else group_by(., Metal) %&gt;% mutate(MoreStringent = min(CriteriaValue)) %&gt;% filter(CriteriaValue == MoreStringent) %&gt;% dplyr::select(-MoreStringent)} # Join appropriate criteria to acuteData for comparison to averaged data acuteDataCriteriaAnalysis &lt;- left_join(acuteData, acuteDataCriteria, by = c(&#39;ID&#39;, &#39;Metal&#39;)) %&gt;% mutate(parameterRound = signif(Value, digits = 2), # two significant figures based on WQS https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section140/ Exceedance = ifelse(parameterRound &gt; CriteriaValue, 1, 0 ), # use 1/0 to easily summarize multiple results later WindowDateTimeStart = min(acuteDataWindow$FDT_DATE_TIME)) %&gt;% filter(!is.na(Criteria)) %&gt;% # filter out metals that don&#39;t have chronic criteria dplyr::select(names(rawDataCriteriaAnalysis)) # dplyr::select(Station_Id, WindowDateTimeStart, everything(), -ID) # Save the results for viewing later acuteCriteriaResults &lt;- bind_rows(acuteCriteriaResults, acuteDataCriteriaAnalysis) } else {acuteCriteriaResults &lt;- acuteCriteriaResults } # Run chronic analysis if data exists if(nrow(chronicDataWindow) &gt; 0){ chronicData &lt;- suppressMessages(chronicDataWindow %&gt;% group_by(Station_Id, FDT_DEPTH, CLASS, PWS, ZONE, `Assess As`) %&gt;% # can&#39;t group by datetime or summary can&#39;t happen dplyr::select(-c(contains(&#39;RMK_&#39;))) %&gt;% pivot_longer(cols = Antimony:Hardness, names_to = &quot;Metal&quot;, values_to = &quot;CriteriaValue&quot;) %&gt;% ungroup() %&gt;% group_by(Station_Id, FDT_DEPTH, CLASS, PWS, ZONE, `Assess As`, Metal) %&gt;% summarise(`Sample Count` = length(CriteriaValue), Value = mean(CriteriaValue, na.rm=T)) %&gt;% # get four day average mutate(ValueType = &#39;Four Day Average&#39;, ID = paste(Station_Id, FDT_DEPTH, sep = &#39;_&#39;)) ) # make a uniqueID in case &gt;1 sample for given datetime # Calculate criteria based on hourly averaged data chronicDataCriteria &lt;- metalsCriteriaFunction(filter(chronicData, Metal == &quot;Hardness&quot;)$ID, filter(chronicData, Metal == &quot;Hardness&quot;)$Value, WER = 1) %&gt;% filter(`Criteria Type` == &#39;Chronic&#39;) %&gt;% # don&#39;t need other criteria for chronic window analysis # Keep only the criteria needed {if(unique(chronicData$`Assess As`) %in% c(&#39;Freshwater&#39;, &#39;Saltwater&#39;)) filter(., Waterbody %in% c(NA, !!unique(chronicData$`Assess As`))) # if in Transition Zone then use the more stringent standard else group_by(., Metal) %&gt;% mutate(MoreStringent = min(CriteriaValue)) %&gt;% filter(CriteriaValue == MoreStringent) %&gt;% dplyr::select(-MoreStringent)} # Join appropriate criteria to chronicData for comparison to averaged data chronicDataCriteriaAnalysis &lt;- left_join(chronicData, chronicDataCriteria, by = c(&#39;ID&#39;, &#39;Metal&#39;)) %&gt;% mutate(parameterRound = signif(Value, digits = 2), # two significant figures based on WQS https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section140/ Exceedance = ifelse(parameterRound &gt; CriteriaValue, 1, 0 ), # use 1/0 to easily summarize multiple results later WindowDateTimeStart = min(chronicDataWindow$FDT_DATE_TIME)) %&gt;% filter(!is.na(Criteria)) %&gt;% # filter out metals that don&#39;t have chronic criteria dplyr::select(names(rawDataCriteriaAnalysis)) # dplyr::select(Station_Id, WindowDateTimeStart, everything(), -ID) # Save the results for viewing later chronicCriteriaResults &lt;- bind_rows(chronicCriteriaResults, chronicDataCriteriaAnalysis) } else {chronicCriteriaResults &lt;- chronicCriteriaResults } } stationCriteriaResults &lt;- bind_rows(rawCriteriaResults, acuteCriteriaResults, chronicCriteriaResults) %&gt;% filter(!is.na(Station_Id)) %&gt;% # drop placeholder rows distinct(Station_Id, WindowDateTimeStart, FDT_DEPTH, Criteria, .keep_all = T) %&gt;% # remove duplicates in case &gt; 1 depth per datetime arrange(Station_Id, WindowDateTimeStart, FDT_DEPTH, Metal) return(stationCriteriaResults) } # Example Usage: (demonstrating nested function syntax and pipes, both result in the same output): # where stationData and WCmetalsForAnalysis objects are already in your environment, see Automated Assessment for environment set up help # metalsAnalysis(filter(WCmetalsForAnalysis, Station_Id %in% stationData$FDT_STA_ID), stationData, WER= 1) # filter(WCmetalsForAnalysis, Station_Id %in% stationData$FDT_STA_ID) %&gt;% # metalsAnalysis(stationData, WER= 1) # Example usage with rolling analyses # filter(WCmetalsForAnalysis, Station_Id %in% stationData$FDT_STA_ID) %&gt;% # metalsAnalysis( stationData, WER = 1) %&gt;% # rename(FDT_STA_ID = Station_Id) %&gt;% # mutate(`Criteria Type` = Criteria) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = TRUE) %&gt;% # annualRollingExceedanceSummary() 3.5.12.3 metalsAssessmentFunction This final function analyzes the output of metalsAnalysis() function that was run through the rolling window summary functions, and converts it into the stations table format. However, because there are multiple metals and only 3 columns in the stations table available to describe these results, the total number of windows with exceedances across all criteria for a station are reported in the WAT_MET_EXC field. The WAT_MET_STAT field returns S if no exceedances exist and Review if any exceedances occurred during any data window for any metal criteria. In the WAT_MET_COMMENT field, this function returns a string that includes the criteria that were exceeded with the number of windows with exceedances in parentheses (e.g., Cadmium Chronic Freshwater (2), Barium PWS (1), Copper Acute Freshwater (4), etc.). The WAT_MET_COMMENT is not reported directly in the Station Table but is instead combined into the overall station COMMENT field to comply with standardized assessment application features. # Metals Assessment function that makes sense of output from metalsAnalysis() metalsAssessmentFunction &lt;- function(metalsAnalysisResults){ #Check to make sure that metals data exist if(nrow(metalsAnalysisResults) &gt; 0){ # Organize results to include Staion_Id, Criteria, and the # of Exceedances metalsExceedances &lt;- metalsAnalysisResults %&gt;% rename(&quot;Exceedances&quot; = `n Windows Exceeding`)%&gt;% arrange(`Criteria Type`) %&gt;% # arrange on just Criteria to make column order make more sense filter(Exceedances &gt; 0 &amp; `Suggested Result` != &quot;Supporting&quot;) # If there are any exceedances, create a string that includes the metal name(s) and the number of exceedances in parentheses if(nrow(metalsExceedances)&gt;0){ WAT_MET_STATstring&lt;-sapply(seq_len(nrow(metalsExceedances)), function(i) paste0(metalsExceedances$`Criteria Type`[i],&quot; (&quot;, metalsExceedances$Exceedances[i], &quot;)&quot;)) %&gt;% toString() # Create tibble in the format of the stations table that takes the sum of all exceedances for the WAT_MET_EXC field and returns the string created above for the WAT_MET_STAT field metalsResults &lt;- tibble(&quot;WAT_MET_EXC&quot; = sum(metalsExceedances$Exceedances), &quot;WAT_MET_STAT&quot; = &quot;Review&quot;, &quot;WAT_MET_COMMENT&quot; = WAT_MET_STATstring) }else{ # If metals data do exist, but there are no exceedances, return 0 and &quot;S&quot; for supporting metalsResults &lt;- tibble(&quot;WAT_MET_EXC&quot; = 0, &quot;WAT_MET_STAT&quot; = &quot;S&quot;, &quot;WAT_MET_COMMENT&quot; = NA) }}else{ # If no metals data exist, return a tibble with NA for both fields metalsResults &lt;- tibble(&quot;WAT_MET_EXC&quot; = NA, &quot;WAT_MET_STAT&quot; = NA, &quot;WAT_MET_COMMENT&quot; = NA) } return(metalsResults) } # Example Usage for Station Table (demonstrating nested function syntax and pipes, both result in the same output): # where stationData and WCmetalsForAnalysis objects are already in your environment, see Automated Assessment for environment set up help # filter(WCmetalsForAnalysis, Station_Id %in% stationData$FDT_STA_ID) %&gt;% # metalsAnalysis( stationData, WER = 1) %&gt;% # rename(FDT_STA_ID = Station_Id) %&gt;% # mutate(`Criteria Type` = Criteria) %&gt;% # annualRollingExceedanceAnalysis(yearsToRoll = 3, aquaticLifeUse = TRUE) %&gt;% # annualRollingExceedanceSummary() %&gt;% # metalsAssessmentFunction() 3.5.13 Fish Tissue For a general flag provided to the Station Table to determine whether fish tissue data do/do not exist, please see PCB above. When data exists for these fish tissue metals or PCB data, regional assessment staff may investigate them against applicable thresholds inside the waterbody-specific shiny assessment application. 3.5.14 Benthics The benthicAssessment() function is used primarily for flagging whether or benthic data exist when building the Station Table. Most stations evaluated during an assessment period do not have benthic macroinvertebrate data, so it is important to flag when data require review. Benthic assessments are completed by regional biologists to ensure the appropriate SCI and professional judgment are used for assessments. The biological staff review all samples within a given assessment period using the Bioassessment Dashboard and complete a bioassessment for submission via the Bioassessment Fact Sheet Generator. This information is archived for future biological, assessment, and TMDL staff as well as provided seamlessly within the Riverine Assessment Application. The regional assessment staff know to look for bioassessment information based on the Review flag that the benthicAssessment() function provides to the Station Table output. # Benthic Data flag benthicAssessment &lt;- function(stationData, VSCIresults # pinned dataset with all BenSamps run against just VSCI ){ # this works because the all SCI options are run on all data, so if there is a VSCI result # (even if in real life that is not the correct SCI to use), then benthic data exists for # a given station benthicDataExist &lt;- filter(VSCIresults, StationID %in% unique(stationData$FDT_STA_ID)) if(nrow(benthicDataExist) &gt; 0){tibble(BENTHIC_STAT = &#39;Review&#39;) } else { tibble(BENTHIC_STAT = NA)} } # Example Usage: # where stationData and VSCIresults objects are already in your environment, see Automated Assessment for environment set up help # benthicAssessment(stationData, VSCIresults) 3.5.15 Additional Functions The functions below are helper functions but do not take a starring role in the automated assessment process; however, without these functions, one could not perform an automated assessment. Each function is detailed below. 3.5.15.1 assessmentPeriod The assessmentPeriod and assessmentCycle objects are not functions but incredibly useful vectors. These objects hold assessment window and cycle name information that allow for significantly easier updates from cycle to cycle. By sourcing this information throughout multiple dependent arguments and functions, one can easily make cycle changes when a new cycle is ready for assessment. # Establish Assessment Period, update each cycle assessmentPeriod &lt;- as.POSIXct( c(&quot;2017-01-01 00:00:00 UTC&quot;, &quot;2022-12-31 23:59:59 UTC&quot;), tz=&#39;UTC&#39;) assessmentCycle &lt;- &#39;2024&#39; # Example Usage: # pin_get(&quot;ejones/VSCIresults&quot;, board = &quot;rsconnect&quot;) %&gt;% # filter( between(`Collection Date`, assessmentPeriod[1], assessmentPeriod[2]) ) 3.5.15.2 WQSvalues The WQSvalues object is a tibble that allows for easy WQS updates should these values change over time. This data is joined to individual station data by WQS Class information to easily link measured parameters to appropriate criteria. Follow up functions refine the criteria information should a stations WQS specify any special standards. # WQS information for functions # From: 9VAC25-260-50. Numerical Criteria for Dissolved Oxygen, Ph, and Maximum Temperature # https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section50/ WQSvalues &lt;- tibble(CLASS_BASIN = c(&#39;I&#39;,&quot;II&quot;,&quot;II_7&quot;,&quot;III&quot;,&quot;IV&quot;,&quot;V&quot;,&quot;VI&quot;,&quot;VII&quot;), CLASS = c(&#39;I&#39;,&quot;II&quot;,&quot;II&quot;,&quot;III&quot;,&quot;IV&quot;,&quot;V&quot;,&quot;VI&quot;,&quot;VII&quot;), `Description Of Waters` = c(&#39;Open Ocean&#39;, &#39;Tidal Waters in the Chowan Basin and the Atlantic Ocean Basin&#39;, &#39;Tidal Waters in the Chesapeake Bay and its tidal tributaries&#39;, &#39;Nontidal Waters (Coastal and Piedmont Zone)&#39;,&#39;Mountainous Zone Waters&#39;, &#39;Stockable Trout Waters&#39;,&#39;Natural Trout Waters&#39;,&#39;Swamp Waters&#39;), `Dissolved Oxygen Min (mg/L)` = c(5,4,NA,4,4,5,6,NA), `Dissolved Oxygen Daily Avg (mg/L)` = c(NA,5,NA,5,5,6,7,NA), `pH Min` = c(6,6,6.0,6.0,6.0,6.0,6.0,3.7), `pH Max` = c(9.0,9.0,9.0,9.0,9.0,9.0,9.0,8.0), `Max Temperature (C)` = c(NA, NA, NA, 32, 31, 21, 20, NA)) %&gt;% mutate(CLASS_DESCRIPTION = paste0(CLASS, &quot; | &quot;, `Description Of Waters`)) # Example Usage: # where stationTable is already joined to citmonWQS and WQSlookup, see Automated Assessment for environment set up help # stationTable %&gt;% # # Fix for Class II Tidal Waters in Chesapeake (bc complicated DO/temp/etc standard) # mutate(CLASS_BASIN = paste(CLASS,substr(BASIN, 1,1), sep=&quot;_&quot;)) %&gt;% # (2) # mutate(CLASS_BASIN = ifelse(CLASS_BASIN == &#39;II_7&#39;, &quot;II_7&quot;, as.character(CLASS))) %&gt;% # (2) # # Join actual WQS criteria to each StationID # left_join(WQSvalues, by = &#39;CLASS_BASIN&#39;) 3.5.15.3 lakeNameStandardization This helper function allows for easier lake name cleanup to allow for joins on lake name fields between disparate data types. By storing this crosswalk information in a single function, it can be easily updated if lake names change and applied to multiple scripts easily. # Lake name standardization lakeNameStandardization &lt;- function(x){ # flexibility to handle new and old naming conventions x %&gt;% {if(&quot;WaterName&quot; %in% names(x)) rename(x, WATER_NAME = WaterName) else .} %&gt;% mutate(Lake_Name = case_when(WATER_NAME %in% c(&#39;Abel Lake Reservoir (Long Branch)&#39;) ~ &#39;Abel Lake&#39;, WATER_NAME %in% c(&#39;Big Cherry Reservior&#39;) ~ &#39;Big Cherry Lake&#39;, WATER_NAME %in% c(&#39;Dan River&#39;,&#39;Buffalo Creek&#39;,&#39;Bluestone Creek&#39;) ~ &#39;Kerr Reservoir&#39;, WATER_NAME %in% c(&#39;Smith Lake (Aquia Reservoir)&#39;) ~ &#39;Aquia Reservoir (Smith Lake)&#39;, WATER_NAME %in% c(&#39;Claytor Lake (New River)&#39;, &#39;Claytor Lake (Peak Creek)&#39;, &#39;Claytor Lake Lower (New River)&#39;) ~ &#39;Claytor Lake&#39;, WATER_NAME %in% c(&#39;Fairystone Lake (Goblin Town Creek)&#39;) ~ &#39;Fairystone Lake&#39;, WATER_NAME %in% c(&#39;Harwood Mill Reservoir (PWS)&#39;) ~ &#39;Harwood Mills Reservoir&#39;, WATER_NAME %in% c(&#39;Goose Creek&#39;) ~ &#39;Goose Creek Reservoir&#39;, WATER_NAME %in% c(&#39;Lake Anna&#39;, &#39;Lake Anna/Contrary Creek&#39;, &#39;Lake Anna/Freshwater Creek&#39;, &#39;Lake Anna/Gold Mine Creek&#39;, &#39;Lake Anna/Pamunkey Creek&#39;, &#39;Lake Anna/Plentiful Creek&#39;, &#39;Terrys Run/Lake Anna&#39;) ~ &#39;Lake Anna&#39;, WATER_NAME %in% c(&#39;Lake Cohoon (PWS)&#39;) ~ &#39;Lake Cohoon&#39;, WATER_NAME %in% c(&#39;Conner Lake&#39;) ~ &#39;Lake Conner&#39;, WATER_NAME %in% c(&#39;Lake Kilby (PWS)&#39;) ~ &#39;Lake Kilby&#39;, WATER_NAME %in% c(&#39;Lake Meade (PWS)&#39;,&#39;Pitch Kettle Creek - Lake (PWS)&#39;) ~ &#39;Lake Meade&#39;, WATER_NAME %in% c(&#39;Lake Moomaw (Jackson River)&#39;,&#39;Lake Moomaw Middle (Jackson River)&#39;) ~ &#39;Lake Moomaw&#39;, WATER_NAME %in% c(&#39;Lake Prince - Reservoir (PWS)&#39;) ~ &#39;Lake Prince&#39;, WATER_NAME %in% c(&#39;Lake Shenandoah&#39;) ~ &#39;Shenandoah Lake&#39;, WATER_NAME %in% c(&#39;Lake Smith (PWS)&#39;) ~ &#39;Lake Smith&#39;, WATER_NAME %in% c(&#39;Lake Whitehurst (PWS)&#39;) ~ &#39;Lake Whitehurst&#39;, WATER_NAME %in% c(&#39;Leesville Lake&#39;, &#39;Leesville Lake (Pigg R.)&#39;, &#39;Leesville Lake Middle (Roanoke R.)&#39;) ~ &#39;Leesville Lake&#39;, WATER_NAME %in% c(&#39;Lee Hall Reservoir- Upper, Middle&#39;,&#39;Lee Hall Reservoir-Lower&#39;) ~ &#39;Lee Hall Reservoir&#39;, WATER_NAME %in% c(&#39;Little Creek Reservoir - (PWS)&#39;) ~ &#39;Little Creek Reservoir (VBC)&#39;, WATER_NAME %in% c(&#39;Little Creek Reservoir (PWS)&#39;) ~ &#39;Little Creek Reservoir (JCC)&#39;, WATER_NAME %in% c(&#39;Lonestar Lake F (PWS)&#39;) ~ &#39;Lone Star Lake F (Crystal Lake)&#39;, WATER_NAME %in% c(&#39;Lone Star Lake G (PWS)&#39;) ~ &#39;Lone Star Lake G (Crane Lake)&#39;, WATER_NAME %in% c(&#39;Lone Star Lake I (PWS)&#39;) ~ &#39;Lone Star Lake I (Butler Lake)&#39;, WATER_NAME %in% c(&#39;Martinsville (Beaver Creek) Reservoir&#39;, &#39;Martinsville Reservoir&#39;) ~ &#39;Martinsville Reservoir (Beaver Creek Reservoir)&#39;, WATER_NAME %in% c(&#39;Moormans River&#39;) ~ &#39;Sugar Hollow Reservoir&#39;, WATER_NAME %in% c(&#39;North River&#39;) ~ &#39;Staunton Dam Lake&#39;, WATER_NAME %in% c(&#39;Philpott Reservoir (Goblin Town Creek)&#39;, &#39;Philpott Reservoir Lower (Smith River)&#39;, &#39;Philpott Reservoir (Smith River)&#39;) ~ &quot;Philpott Reservoir&quot;, WATER_NAME %in% c(&#39;Roanoke River&#39;,&#39;Lake Gaston&#39;) ~ &#39;Lake Gaston&#39;, WATER_NAME %in% c(&#39;Roaring Fork Reservoir&#39;) ~ &#39;Roaring Fork&#39;, WATER_NAME %in% c(&#39;S F Rivanna River Reservoir&#39;) ~ &#39;Rivanna Reservoir (South Fork Rivanna Reservoir)&#39;, str_detect(WATER_NAME, &#39;Smith Mtn. Lake&#39;) ~ &#39;Smith Mountain Lake&#39;, WATER_NAME %in% c(&#39;Speights Run - Lake (PWS)&#39;) ~ &#39;Speights Run Lake&#39;, WATER_NAME %in% c(&#39;Troublesome Reservoir&#39;) ~ &#39;Troublesome Creek Reservoir&#39;, WATER_NAME %in% c(&#39;Waller Mill Reservoir [PWS]&#39;) ~ &#39;Waller Mill Reservoir&#39;, WATER_NAME %in% c(&#39;Unnamed pond near Tanyard Swamp&#39;) ~ &#39;Tanyard Swamp&#39;, WATER_NAME %in% c(&#39;Unsegmented lakes in G03&#39;) ~ &#39;West Run&#39;, TRUE ~ as.character(WATER_NAME))) } # Example Usage: # where stationTable is already joined to citmonWQS, WQSlookup, WQSvalues, and WQMstationFull, see Automated Assessment for environment set up help # stationTable %&gt;% # lakeNameStandardization() 3.5.15.4 quickStats This helper function is essential to the automated assessment process. The function takes in parameter data with exceedances identified and sample counts and simplifies the data into a suggested assessment decision, where appropriate. The parameter argument allows users to pass through a character string they want to be included in the Stations Table field naming format. The drop7Q10 argument is default set to false for automated assessment purposes, but the shiny applications use this feature to quickly rerun the exceedance math should a user decide that the 7Q10 flag should be used to remove data for a station and parameter combination. quickStats &lt;- function(parameterDataset, # dataset from one station that has been run through a parameter exceedance analysis function # (e.g. tempExceedances(), DOExceedances_Min(), pHexceedances()) parameter, # the name of the parameter as you want it to appear in final stations table output drop7Q10 = FALSE # drop any records from this analysis with a 7Q10 flag, default is false but can be overridden in apps ){ # Drop any 7Q10 flagged data from exceedance analyses for appropriate parameters if(drop7Q10 == TRUE &amp; parameter %in% c(&quot;TEMP&quot;, &quot;DO&quot;, &quot;DO_Daily_Avg&quot;, &quot;PH&quot;)){ parameterDataset &lt;- dplyr::filter(parameterDataset, is.na(`7Q10 Flag`)) # only assess on assessable dataset } if(nrow(parameterDataset) &gt; 0 &amp; any(!is.na(parameterDataset$limit))){ results &lt;- data.frame(EXC = nrow(dplyr::filter(parameterDataset, exceeds == TRUE)), SAMP = nrow(parameterDataset)) %&gt;% # Implement Round to Even on Exceedance Frequency dplyr::mutate(exceedanceRate = as.numeric(round::roundAll((EXC/SAMP)*100,digits=0, &quot;r0.C&quot;))) # round to nearest whole number per Memo to Standardize Rounding for Assessment Guidance if(results$EXC &gt;= 1){outcome &lt;- &#39;Review&#39;} # for Mary if(results$EXC &gt;= 1 &amp; results$exceedanceRate &lt; 10.5){outcome &lt;- &#39;Review&#39;} if(results$exceedanceRate &gt; 10.5 &amp; results$EXC &gt;= 2 &amp; results$SAMP &gt; 10){outcome &lt;- &#39;10.5% Exceedance&#39;} if(results$EXC &lt; 1 &amp;results$exceedanceRate &lt; 10.5 &amp; results$SAMP &gt; 10){outcome &lt;- &#39;S&#39;} if(results$EXC &gt;= 1 &amp; results$SAMP &lt;= 10){outcome &lt;- &#39;Review&#39;} if(results$EXC &lt; 1 &amp; results$SAMP &lt;= 10 &amp; results$SAMP &gt; 1){outcome &lt;- &#39;S&#39;} # &amp; results$SAMP &gt;1 new 12/21/2020 can&#39;t say supporting on 1 sample if(results$EXC &lt; 1 &amp; results$SAMP &lt;= 10 &amp; results$SAMP == 1){outcome &lt;- &#39;Review&#39;} # &amp; results$SAMP &gt;1 new 12/21/2020 can&#39;t say supporting on 1 sample results &lt;- dplyr::mutate(results, STAT = outcome) names(results) &lt;- c(paste(parameter,names(results)[1], sep = &#39;_&#39;), paste(parameter,names(results)[2], sep = &#39;_&#39;), paste(parameter,names(results)[3], sep = &#39;_&#39;), paste(parameter,names(results)[4], sep = &#39;_&#39;)) #rename based on parameter entered return(results) } else { if(nrow(parameterDataset) == 0){ z &lt;- data.frame(EXC = NA, SAMP= nrow(parameterDataset), exceedanceRate= NA, STAT= NA) names(z) &lt;- paste(parameter,names(z), sep=&#39;_&#39;) return(z) } else { z &lt;- data.frame(EXC = NA, SAMP= nrow(parameterDataset), exceedanceRate= NA, STAT= paste(parameter, &#39;WQS info missing from analysis&#39;)) names(z) &lt;- paste(parameter,names(z), sep=&#39;_&#39;) return(z) } } } # Example Usage: # where stationData is already in your environment, see Automated Assessment for environment set up help # tempExceedances(stationData) %&gt;% # quickStats(&#39;TEMP&#39;) 3.5.15.5 StationTableStartingData This helper function organizes existing site metadata into a format that matches the desired Station Table output format. StationTableStartingData &lt;- function(stationData){ stationData %&gt;% dplyr::select(FDT_STA_ID, ID305B_1:VAHU6) %&gt;% dplyr::rename(&#39;STATION_ID&#39; = &#39;FDT_STA_ID&#39;) %&gt;% dplyr::distinct(STATION_ID, .keep_all = T) } # Example Usage: # where stationData is already in your environment, see Automated Assessment for environment set up help # StationTableStartingData(stationData) 3.5.15.6 temperatureSpecialStandardsCorrection This helper function is applied after WQSvalues() is run on a tibble of station data. The function corrects the coarser WQS value joined by Class by identifying if any temperature special standards are attributed to a station. If so, the function then adjusts each Max Temperature (C) record based on temporal criteria accordingly. # temperature special standards adjustment function # this is applied to the actual monitoring data because there is a temporal component to these criteria temperatureSpecialStandardsCorrection &lt;- function(x){ # ee. Maximum temperature for these seasonally stockable trout waters is 26C and applies May 1 through October 31. https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section310/ # ff. Maximum temperature for these seasonally stockable trout waters is 28C and applies May 1 through October 31. https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section310/ # hh. Maximum temperature for these seasonally stockable trout waters is 31C and applies May 1 through October 31. https://law.lis.virginia.gov/admincode/title9/agency25/chapter260/section310/ mutate(x, `Max Temperature (C)` = case_when(str_detect(as.character(SPSTDS), &#39;ee&#39;) &amp; month(FDT_DATE_TIME) %in% 5:10 ~ 26, str_detect(as.character(SPSTDS), &#39;ff&#39;) &amp; month(FDT_DATE_TIME) %in% 5:10 ~ 28, str_detect(as.character(SPSTDS), &#39;hh&#39;) &amp; month(FDT_DATE_TIME) %in% 5:10 ~ 31, TRUE ~ `Max Temperature (C)`)) } # Example Usage: # where conventionals and stationTable objects are already in your environment, see Automated Assessment for environment set up help # stationData &lt;- filter(conventionals, FDT_STA_ID %in% &#39;2-JKS023.61&#39;) %&gt;% # left_join(stationTable, by = c(&#39;FDT_STA_ID&#39; = &#39;STATION_ID&#39;)) %&gt;% # temperatureSpecialStandardsCorrection() 3.5.15.7 pHSpecialStandardsCorrection This helper function is applied after WQSvalues() is run on a tibble of station data. The function corrects the coarser WQS value joined by Class by identifying if any pH special standards are attributed to a station. If so, the function then adjusts each pH Min and pH Max record based on applicable criteria accordingly. pHSpecialStandardsCorrection &lt;- function(stationData){ mutate(stationData, `pH Min` = case_when(str_detect(as.character(SPSTDS), &#39;6.5-9.5&#39;) ~ 6.5, TRUE ~ `pH Min`), `pH Max` = case_when(str_detect(as.character(SPSTDS), &#39;6.5-9.5&#39;) ~ 9.5, TRUE ~ `pH Max`)) } # Example Usage: # where conventionals and stationTable objects are already in your environment, see Automated Assessment for environment set up help # stationData &lt;- filter(conventionals, FDT_STA_ID %in% &#39;2-JKS023.61&#39;) %&gt;% # left_join(stationTable, by = c(&#39;FDT_STA_ID&#39; = &#39;STATION_ID&#39;)) %&gt;% # pHSpecialStandardsCorrection() 3.5.15.8 stationTableComments This helper function parses station level comments from the two most recent IR Station Table outputs. This information is joined into the current IR Station Table, despite not fitting with the CEDS Bulk Data Upload template, in order to provide regional assessment staff with historical station information adjacent to current station information. stationTableComments &lt;- function(stations, previousStationTable, previousStationTableCycle, previousStationTable2, previousStationTable2Cycle){ lastComment &lt;- filter(previousStationTable, `Station Id` %in% stations) %&gt;% dplyr::select(`Station Id`, Comments) names(lastComment) &lt;- c(&#39;STATION_ID&#39;, paste(previousStationTableCycle, &#39;IR COMMENTS&#39;)) lastComment2 &lt;- filter(previousStationTable2, `Station Id` %in% stations) %&gt;% dplyr::select(`Station Id`, Comments) names(lastComment2) &lt;- c(&#39;STATION_ID&#39;, paste(previousStationTable2Cycle, &#39;IR COMMENTS&#39;)) return(left_join(lastComment, lastComment2, by= &#39;STATION_ID&#39;)) } # Example Usage: # where stationsTable2022 and stationTable2020 are objects containing previous stations table spreadsheets read into your R environment, see Automated Assessment for environment set up help # stationTableComments(&#39;2-JKS023.61&#39;, stationsTable2022, &#39;2022&#39;, stationsTable2020, &#39;2020&#39;) 3.5.15.9 thermoclineDepth This helper function is essential for any lake station. The function takes in all conventionals data for a particular station and calculates a thermocline depth for each sample date. This depth is then used to attribute every sample as either Epilimnion, Hypolimnion, or unstratified (NA). # Calculate daily thermocline depth and designate Epilimnion vs Hypolimnion thermoclineDepth &lt;- function(stationData){ stationData &lt;- stationData %&gt;% mutate(SampleDate = as.Date(FDT_DATE_TIME)) %&gt;% group_by(FDT_STA_ID, SampleDate) dailyThermDepth &lt;- dplyr::select(stationData, FDT_STA_ID, SampleDate, FDT_DEPTH, FDT_TEMP_CELCIUS) %&gt;% mutate(DepthDiff = c(NA, diff(FDT_DEPTH)), TempDiff = c(NA, diff(FDT_TEMP_CELCIUS))) %&gt;% filter(DepthDiff == 1) # get rid of changes less than 1 meter depth # Alt route in case shallow lake if(nrow(dailyThermDepth) &gt; 0){ dailyThermDepth &lt;- filter(dailyThermDepth, TempDiff &lt;= -1) # one more catch if no thermocline established if(nrow(dailyThermDepth) &gt; 0){ dailyThermDepth &lt;- summarise(dailyThermDepth, ThermoclineDepth = min(FDT_DEPTH) - 0.5) %&gt;% ungroup() } else { dailyThermDepth &lt;- summarise(stationData, ThermoclineDepth = NA) %&gt;% ungroup() } } else { dailyThermDepth &lt;- summarise(stationData, ThermoclineDepth = NA) %&gt;% ungroup() } full_join(stationData, dailyThermDepth, by = c(&#39;FDT_STA_ID&#39;, &#39;SampleDate&#39;)) %&gt;% mutate(LakeStratification= ifelse(FDT_DEPTH &lt; ThermoclineDepth,&quot;Epilimnion&quot;,&quot;Hypolimnion&quot;))%&gt;% ungroup() } # Example usage: # where stationData is already in your environment, see Automated Assessment for environment set up help # stationData %&gt;% thermoclineDepth() 3.5.15.10 lowFlowFlagColumn This helper function is new for IR2024. The function provides a flag field to the Station Table output only if a station has temperature, DO, or pH data collected on day with gage in subbasin is below 7Q10. # Function to add a column to stations table output that summarizes 7Q10 information for relevant parameters lowFlowFlagColumn &lt;- function(stationData){ lowFlowFlag &lt;- filter(stationData, !is.na(`7Q10 Flag`)) if(nrow(lowFlowFlag) &gt; 0){ withParameterData &lt;- dplyr::select(lowFlowFlag, FDT_STA_ID, FDT_DATE_TIME, FDT_TEMP_CELCIUS, DO_mg_L, FDT_FIELD_PH, `7Q10 Flag Gage`) %&gt;% pivot_longer(-c(FDT_STA_ID, FDT_DATE_TIME, `7Q10 Flag Gage`), names_to = &#39;parameter&#39;, values_to = &#39;value&#39;) %&gt;% filter(!is.na(value)) # drop rows with no data for specific parameters if(nrow(withParameterData) &gt; 0){ gageFlags &lt;- dplyr::select(withParameterData, FDT_STA_ID, FDT_DATE_TIME, `7Q10 Flag Gage`) %&gt;% distinct(FDT_DATE_TIME, .keep_all = T) %&gt;% summarise(STATION_ID = FDT_STA_ID, `7Q10 Flag` = paste(as.Date(FDT_DATE_TIME), `7Q10 Flag Gage`, collapse = &#39;; &#39;)) %&gt;% mutate(`7Q10 Flag` = paste(&#39;USGS Gages in subbasin below 7Q10: &#39;, `7Q10 Flag`)) } else { gageFlags &lt;- tibble(STATION_ID = unique(lowFlowFlag$FDT_STA_ID), `7Q10 Flag` = NA_character_) } } else { gageFlags &lt;- tibble(STATION_ID = unique(lowFlowFlag$FDT_STA_ID), `7Q10 Flag` = NA_character_) } return(gageFlags) } # Example usage: # where stationData is already in your environment, see Automated Assessment for environment set up help #lowFlowFlagColumn(stationData) "],["automated-output.html", "3.6 Automated Output", " 3.6 Automated Output This dataset is the end result of the automated assessment process. It is saved as a .csv and used for upload to both the riverine and lacustrine applications. The reason the data are stored in .csv and not on the server is to enable individual assessors to adjust AU information manually whenever needed (e.g. if an AU needs to be split). If this data were stored on the server, then the Assessment Data Analyst would need to be involved any time an assessor needed to adjust which AU(s) a station is connected to. This data is available for download from each of the riverine and lacustrine assessment applications. "],["regional-metadata-validation-tool-how-to.html", "Chapter 4 Regional Metadata Validation Tool How To", " Chapter 4 Regional Metadata Validation Tool How To The Regional Metadata Validation Tool is located on the R server. The purpose of this tool is to streamline the QA of metadata attached to individual stations that was performed by automated scripts. This metadata include Water Quality Standards (WQS) and Assessment Unit (AU) information that are necessary to properly analyze and organize assessment results. A scripted process automates the data organization of stations from disparate data sources, spatially joins these sites to the appropriate WQS and AUs where information is not available from previous cycles, and runs basic QA on the output to ensure all stations that were expected to receive metadata did in fact get attributed. These scripts are detailed in the Metadata Attribution section. It is the responsibility of each regional assessor to review the metadata linked through the automated snapping process to ensure the closest snapped information does in fact apply to the given station. This application offers an opportunity to perform QA on the spatial WQS and AU layers and communicate with Central Office Assessment/WQS staff when mistakes are found. "],["application-orientation.html", "4.1 Application Orientation", " 4.1 Application Orientation This application expedites the review of the spatially joined output by organizing stations into the appropriate waterbody type, Assessment Region, and subbasin. The application is divided into two tabs (corresponding to AU review and WQS review) that can be navigated between using the navigation bar at the top of the page. The application opens to the Assessment Unit Review tab by default, but there is no specified order in which a reviewer must interact with either tab. Data for each tab are saved back to the R server independently. Each of these tabs have a drop down option that is accessible by clicking the triangle adjacent to the name of the tab. These options are identical for both the Assessment Unit QA and Water Quality Standards QA tabs. The top option called Watershed Selection allows users to choose certain types of stations to review and the bottom option called Manual Review allows users to visualize the stations selected in the Watershed Selection tab option in more detail. The final tab called About offers instructions to the user similar to what is detailed below. The following application workflow is applicable to both the Assessment Unit QA and Water Quality Standards QA sections of the application. For brevity, this user guide will only go over instructions for the WQS side of the application, but the application flow applied to the WQS side of the application mimics the application flow applied on the AU side of the application. "],["application-use-water-quality-standards-qa-watershed-selection-tab.html", "4.2 Application Use: Water Quality Standards QA- Watershed Selection Tab", " 4.2 Application Use: Water Quality Standards QA- Watershed Selection Tab On load, the tab will provide three related drop down menus on the left hand side of the page. These interact in a top-down hierarchical system where the selection in the top drop down, Select Waterbody Type, dictates available options in the middle drop down, Select DEQ Assessment Region, which then dictates the available options in the bottom drop down, Select Subbasin. Users navigate through these cascading filters to identify the precise waterbody type (e.g. riverine, lacustrine, or estuarine), assessment region, and subbasin to tackle QA in a given session. Once the user is happy with their selection, they must click the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button. This button uses the user provided information to retrieve all the applicable stations that require QA. This information is sourced from the R server in addition to all the applicable spatial layers necessary to produce meaningful maps in the subsequent Manual Review tab option. Once this information is retrieved from the R server, a basic interactive map of the selected subbasin populates the tab as well as verbal breakdowns of the type of information that needs to be reviewed for the selected subbasin (Preprocessing Data Recap for Selected Region/Subbasin/Type Combination). Based on the summarized information about the stations in the selected subbasin, the user may choose to begin reviewing that subbasin or they might scroll through other waterbody type/region/subbasin combinations until a more desirable to do list is identified. Each time a user changes any of the drop down inputs, they must press the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button to retrieve the associated station breakdown from the chosen subbasin. A small progress bar appears in the lower right corner of the browser window to communicate that large spatial data are being transmitted to the app. The popup disappears when the map re-renders and loads the new spatial information. Once a user is ready to review the stations in the selected subbasin, the user must navigate to the Manual Review tab under the Water Quality Standards QA drop down on the navigation bar. "],["application-use-water-quality-standards-qa-manual-review-tab.html", "4.3 Application Use: Water Quality Standards QA- Manual Review Tab", " 4.3 Application Use: Water Quality Standards QA- Manual Review Tab On load, the Manual Review tab renders three buttons, an interactive map of Virginia, and an area for output information from the map that is blank until user input dictates. 4.3.1 Map Orientation The map allows users to zoom and pan interactively with the mouse. In the top left corner there are a number of tools to aid users. The plus and minus buttons offer fixed zoom in/out. The home button returns the map orientation to the original zoom level in case a user gets lost. The magnifying glass button allows users to search for and zoom to a selected station. To use this feature, click on the magnifying glass, begin typing the StationID of interest in the text box (after each character entered, a dropdown of options based on the characters entered appears), once a station is selected, the map will zoom to that station and highlight it with a red circle. You may notice that if there isnt the All Stations in Basin layer on, then no station point appears inside the red circle. Additional layers available in the map (by hovering over the layers button on the left side) include All Stations in Basin (all other stations identified in the basin with data for the given IR window) and Assessment Regions (regional assessment boundaries). 4.3.2 What do the buttons do? The three buttons correspond to the station snapping summaries presented on the Watershed Selection tab. In the example above, we can see this subbasin has: * There are 8 stations that snapped to 1 WQS segment in preprocessing. * There are 1 stations that snapped to &gt; 1 WQS segment in preprocessing. * There are 1 stations that snapped to 0 WQS segments in preprocessing. By clicking the Plot stations that snapped to 1 WQS Segment, users will see all the stations that only retrieved one WQS segment in the spatial snapping process. They are colored from green to light red based on the buffer distance required to snap to a segment (within the smallest set buffer distance of the sequence dictated to the automated snapping functions). Green stations snapped to segments at closer distances compared to red sites. The legend for the buffer distance stations colors is presented in the right corner of the app. By clicking the Plot stations that snapped to &gt;1 WQS Segment, users will see all the stations that retrieved multiple WQS segments in the spatial snapping process (within the smallest set buffer distance of the sequence dictated to the automated snapping functions). They are colored to red to indicate these stations need more attention. By clicking the Plot stations that snapped to 0 WQS Segment, users will see all the stations that retrieved no WQS segments in the spatial snapping process (within the largest buffer distance of the sequence dictated to the automated snapping functions). They are colored to orange to indicate these stations need the most attention. If any of the categories listed above had 0 stations, a warning message will appear in the lower right corner of the app indicating there is nothing for the button to plot. 4.3.3 Application Operation The goal of this section of the application is to review each station and either accept or adjust the provided WQS information snapped to the site. To review a site, users may zoom to a selected map area and click on a site. By clicking a site, users are selecting a site to QA, as indicated by the green halo around the selected site. The associated metadata is now populated in the Stations Data and Spatially Joined WQS tab below the map. The Selected Station Information table details information about the station including: * the WQS_ID of snapped segment(s) * the Buffer Distance (distance required to snap to the linked segment(s)) * n (the number of segments linked to the station during spatial processing) * a hyperlink to the internal GIS Web Application that pulls up the appropriate WQS layer and station information in a new browser tab * additional station metadata information The Spatially Joined WQS Information table details information about the WQS segment(s) attached to the selected station including: * WQS_ID (unique code attributed to each WQS segment) * GNIS_Name (name of the WQS segment) * SEC (WQS section) * CLASS (WQS class) * SPSTDS (WQS special standards) * SECTION_DESCRIPTION (narrative description of the section location) * PWS (whether or not the section is designated as a Public Water Supply) * Tier_III (whether or not the section is designated as a Tier III water) * additional WQS segment information Users may click on more than one station and subsequently reveal information about more than one station in the Stations Data and Spatially Joined WQS tab below the map. If more than one row is presented in a table, a y scroll bar on the right side of the table will automatically appear such that the user may access all the table information. 4.3.3.1 Application Operation: Clear Selection To deselect a station or stations, click the blue Clear Selection button below the map. ` 4.3.4 Application Operation: Station that Snapped to 1 WQS Segment Should a user select a station for review that snapped to 1 WQS segment, the process for QAing that station is simple. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. Additionally, when the Plot stations that snapped to 1 WQS Segment button was pressed, additional spatial layers became available in the layers drop down of the map. The user may now turn on the layer called WQS Segments of Stations Snapped to 1 Segment to reveal all the segments in that category. These segments may be clicked on in the map to reveal the same information from the Selected Station Information table in a popup on the map. 4.3.4.1 Application Operation: Accept If the user is satisfied with the snapped WQS segment, they may press the blue Accept button to reveal a modal window named Accept Snapped WQS. This modal shows the information from the Selected Station Information table one last time for review. There is a textbox below the table where users may input any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. Once the user presses the Accept button in the Accept Snapped WQS modal, the station and WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.4.2 Application Operation: Manual WQS Adjustment (1 segment) If the user is not satisfied with the snapped WQS segment, they may press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. This modal shows the information from the Selected Station Information table one last time for review. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Pro Tip: By clicking the record in the WQS_ID cell of the table on the modal window three time quickly, the cell contents will be highlighted. Users may use the keyboard shortcuts to copy(ctrl+c)/paste(ctrl+v) this information into the text box below the table. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.4.3 Application Operation: Export Reviews All the work the user has completed so far accepting or manually adjusting WQS are only saved in the local application environment. The only way to preserve the work completed in the application is to press the green Export reviews button. It is highly recommended that users click the Export reviews button frequently (every 3-4 stations or after a particularly challenging review) to ensure their work is in fact saved on the server. Users may undo any accidental work still in their local application environment by either closing the app or returning to the Watershed Selection tab without pressing the Export reviews button. This work flow would look like: User selects station, Accept User selects station, Accept User selects station, Accept User presses Export reviews to save work to the R server User selects station, Accept- user realizes they made a mistake and doesnt want to save that accept to the server User closes the app or navigates back to the Watershed Selection tab without pressing the Export reviews button- all information completed in the app since the last time Export reviews was pressed will not be saved User opens the app or clicks the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button on the Watershed Selection tab to pull the last information saved on the server User proceeds to the Manual Review tab and continues work 4.3.5 Application Operation: Station that Snapped to &gt;1 WQS Segment Should a user select a station for review that snapped to &gt;1 WQS segment, the process for QAing that station is relatively simple. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. Additionally, when the Plot stations that snapped to &gt;1 WQS Segment button was pressed, additional spatial layers became available in the layers drop down of the map. The user may now turn on the layer called WQS Segments of Stations Snapped to &gt;1 Segment to reveal all the segments in that category. These segments are plotted on a color ramp to distinguish them from one another and may be clicked on in the map to reveal the same information from the Selected Station Information table in a popup on the map. The GIS Web App hyperlinks may also be useful to use at this stage. 4.3.5.1 Application Operation: Manual WQS Adjustment (&gt; 1 segment) If the user needs to choose between multiple WQS segments, they must press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. The user cannot accept a single StationID that is linked to multiple WQS_IDs using the Accept button. The Manually Adjust WQS modal shows the information from the Selected Station Information table one last time for review. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Pro Tip: By clicking the record in the WQS_ID cell of the table on the modal window three time quickly, the cell contents will be highlighted. Users may use the keyboard shortcuts to copy(ctrl+c)/paste(ctrl+v) this information into the text box below the table. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.6 Application Operation: Station that Snapped to 0 WQS Segments Should a user select a station for review that snapped to no WQS segments, the process for QAing that station is more involved. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. To preserve application rendering time, additional spatial layers are not available in the layers drop down of the map for this scenario. Instead, the user should click the Open Link In New Tab button in the Selected Station Information table to navigate to the GIS Web App to visualize all WQS segments around the station. 4.3.6.1 Application Operation: Manual WQS Adjustment (0 segments) Once the user has chosen an appropriate WQS segment, they must press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. The user cannot accept a single StationID that is linked to no WQS_IDs using the Accept button. The Manually Adjust WQS modal shows the information from the Selected Station Information table one last time for review. This will show information about the station but no WQS_ID. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.7 Application Operation: There are no more stations to attribute Congratulations! That means you have completed the review of all the stations in this subbasin. Make sure you press the Export reviews button one more time to ensure all your hard work is saved on the server. At this point, the user may navigate back to the Watershed selection tab to choose another subbasin to review or close the application to end their metadata review session. "],["having-problems.html", "4.4 Having problems?", " 4.4 Having problems? Please contact Emma Jones (emma.jones@deq.virginia.gov) should you encounter any problems using the application, wonky situations to review, or long rendering times/app crashing. "],["riverine-application-how-to.html", "Chapter 5 Riverine Application How To", " Chapter 5 Riverine Application How To The Riverine Assessment App is located on the R server. The linked version of the application is in draft development stage with DRAFT DATA until after the data solicitation deadline and final product release. Expect this release in late April 2023. The purpose of this tool is to expand upon the Station Table summary provided by the automated assessment methodology. A single parameter summary (or in the case of metals, toxics, etc. a single summary of multiple parameters) does not do justice to the data analyzed by the automated assessment process. The Riverine Assessment App provides an interactive data analysis experience to improve users understanding of automated assessment analyses and decisions. The intended audience is any DEQ employee from regional assessors and field staff, to managers, providing better data understanding through interactive data visualizations, descriptive standard application and WQA guidance explanations, and statewide applicability. A notable benefit of these standardized tools is the increased accessibility of assessment decisions. No longer are assessment tools limited to regional assessment staff, resulting in bottlenecks when assessment questions arise. These self-service tools articulate assessment procedures to guide data and decision discussions across multiple end users. This tool is a companion to the Station Table throughout the decision-making process the regional assessment staff sustain as data is entered into WQA CEDS. The next sections will detail typical workflow procedures to using this tool for data exploration and/or assessment. "],["application-orientation-1.html", "5.1 Application Orientation", " 5.1 Application Orientation This application brings station-level automated analyses provided in the Station Table to life through interactive data visualizations, maps, and tables. The tool is divided into three tabs that must be visited sequentially for proper application functionality. The Data Upload tab provides a basic overview of application data sources, provides download of the statewide Station Table resultant from previous automated assessment procedures, and requires upload of either the default Station Table or a custom Station Table (e.g. the Station Table filtered to a specific region). The Watershed Selection tab allows further investigation of particular assessment watersheds (VAHU6) by assessment region and major basin. This tab offers high level summaries of stations within a chosen assessment watershed as well as detailed exceedance summaries across assessment units through tools embedded in modal windows. The Assessment Unit Review tab dives into all assessment units and stations contained within a selected assessment watershed. By drilling down from watershed to assessment unit to station, users may explore assessment data against applicable Water Quality Standards and criteria to better understand how assessment guidance leads assessment staff to appropriate decisions across designated uses. Previous assessment decisions are provided along with all centrally stored assessment data to provide a full picture of stations that require addressing during a given assessment cycle. "],["application-use-data-upload-tab.html", "5.2 Application Use: Data Upload Tab", " 5.2 Application Use: Data Upload Tab The Data Upload tab is the first screen to appear on application load. The purpose of this screen is to orient users to basic application usage and data sources and to provide a location to upload the Station Table. The Station Table is key to running any subsequent parts of this application. Users may download the latest copy of the statewide Station Table using the Download statewide example dataset to upload to the tool. button at the bottom of the screen. Helptext below the download button indicates the last time the Station Table template was last updated. The Station Table must be uploaded to the application each time the tool is used. The reason the tool requires an upload of the Station Table each use is to allow flexibility for regional assessment staff to change station assessment unit (AU) designations throughout the assessment process. If this dataset were sourced from the R server like all other datasets used by the Riverine App, then assessors would need to notify the Assessment Data Analyst each time a station AU changes, is split, or requires additional AUs. By allowing the regional assessment staff to edit this information in a .csv, the application allows self-service changes to station organization. Users may upload the entire statewide Station Table to the Browse button in the tool or a version filtered ans saved locally with just the region of interest. "],["application-use-watershed-selection-tab.html", "5.3 Application Use: Watershed Selection Tab", " 5.3 Application Use: Watershed Selection Tab The Watershed Selection tab allows users to choose a particular region to investigate further. Using the drop down menu in the sidebar panel, users must choose one assessment region and then click Retrieve Assessment Units From Server in order to proceed with any analyses. Based on the assessment region selected, the application then extracts the appropriate spatial data from standardized datasets saved on the R server and provides that information to the application. A progress message is visible in the lower right of the screen while this data transaction occurs. Once this data is available to the application, the user may further refine data by dynamic filtering drop downs on the sidebar panel. Users may select from only the subbasins within the selected assessment region and then only the assessment watersheds (VAHU6) within the selected subbasin. An interactive map dynamically updates to display the user selection on the main panel of the application. Additional information about the each VAHU6 may be found in the popup table revealed upon clicking on a VAHU6 in the map. Proceeding down the sidebar panel, two buttons reveal modal dialog boxes with additional information about the selected VAHU6. The Spatially Preview Stations and Assessment Units button reveals a modal dialog box with an interactive map of all stations and AUs that fall within the chosen VAHU6. This box provides a preview summarizing the amount of information that must be addressed for the current IR cycle in the chosen VAHU6. AUs are colored by their desingated name and hovering over the AU reveals the name. Clicking the AU segment brings up a popup table with additional AU information. All stations with information related to the current IR cycle are displayed based on whether or not they have data within the cycle. Hovering and clicking on a given station reveals additional station information with dialog boxes and popup tables, respectively. The dialog box may be closed by either clicking outside the box in the grayed out area of the application or by using the Dismiss button. The Watershed Status Overview button reveals a modal dialog box that quickly summarizes station parameter statuses throughout the selected VAHU6. For application rendering speed, the statuses displayed are from the uploaded Station Table. Should a user change a parameter status in the Station Table uploaded to the application, then the resultant status map could change. The map presents an overall station overview where each station in the selected VAHU6 is colored based on the most harmful status category (e.g. if a station has 8 Supporting parameter statuses, 2 Insufficient parameter statuses, and 1 Impaired status, the station will be colored red to reflect the Impaired status). The number of statuses in the most harmful category is reported for each station in the popup table (accessed by clicking the station in the map). The Choose parameter to report station status drop down is set to Overall Status by default on modal load, but any parameter specified in the Station Table may be selected to redraw the map with only that parameter status displayed. The dialog box may be closed by either clicking outside the box in the grayed out area of the application or by using the Dismiss button. The main panel of the Watershed Selection tab details information about the selected VAHU6. The interactive map describes spatially where the VAHU6 falls within the basin. Subsequent tables detail the AUs and stations within the VAHU6. The Assessment Units in Selected VAHU6 table lists all AUs that fall inside the selected VAHU6, retrieved from the previous Assessment cycle. The Stations in Selected VAHU6 that were sampled in current window table identifies all stations that fall within the VAHU6 with conventionals data in the current Assessment cycle. The stations highlighted in gray can be analyzed by the application. The stations highlighted in yellow were sampled in the current window, but cannot be analyzed by the application because they are not designated as the appropriate waterbody type (using AU naming system) in the input stations table (e.g. stations linked to lake AUs cannot be analyzed appropriately in the riverine app and are thus highlighted yellow here). The last table identifies stations in selected VAHU6 that have no data in the current window but were carried over from last cycle due to an IM designation in one of the previous IR status fields or the previous IR stations table reports the station was carried over from a more previous cycle. The purpose of this table is to identify which stations in the selected VAHU6 are carried over from a previous cycle and require review before carrying forward another cycle. These stations can be viewed in the application and stations table, but none of the parameter modules will display data as no data is available in the current window. "],["application-use-assessment-unit-review-tab.html", "5.4 Application Use: Assessment Unit Review Tab", " 5.4 Application Use: Assessment Unit Review Tab After selecting a VAHU6 to investigate on the Watershed Review tab, the Assessment Unit Review tab is where the user will find the most interactive analytical tools. The tab provides a drill-down view of all assessment data related to the selected VAHU6. Users explore the chosen VAHU6 using a heirarchical schema from VAHU6 -&gt; AUs within the VAHU6 -&gt; Stations within the AU. The table at the top of the tab reminds the user which VAHU6 is currently selected for investigation and which major basin ths VAHU6 falls in. The Assessment Unit Selection drop down allows users to choose from all AUs within the chosen VAHU6. These AUs are retrieved from the final AU spatial layer from the previous cycle. The table immediately below this drop down provides AU information for the chosen AU from the previous cycle as a summary of use attainment and AU comments in the most previous cycle. If, during the assessment process, an AU is split or renamed to a name not contained within the previous cycle final AU spatial layer and the Station Table uploaded to the application contains this new/changed AU, the previous final AU spatial layer will not contain information about this AU. However, the application will still allow the user to perform all expected analyses without this information. Based on the AU selected, all stations attributed to the AU are available in the subsequent Station Selection drop down. The stations available in the drop down are limited to stations with an ID305B_1:ID305B_10 designation equal to the selected AU as provided in the uploaded Station Table. Once a station is selected, all further visualizations and analyses pertain to that station. In the subsequent panel, current and previous information about the selected station are provided. On the left, a table provides station and WQS information, organizing AU and station type designations (pulled from the uploaded Station Table) and WQS information (pulled from pinned data on the R server, originally provided by the regional assessor during the previous metadata steps). A thumbnail map identifies the station and all associated AUs. The table on the right provides Station Table results from the two most previous assessment cycles to collate relevant historical information about the station in one area. Information specific to the current assessment cycle is presented below the general station information panel. The Station Table Results table outputs the site specific results for direct export to the Station Table. It also serves to highlight where exceedances are present and should be reviewed in the individual parameter visualization tabs below. If no station table appears, then there is no data within the assessment window for the selected station. Please investigate the Historical Station Information table above for information as to why this station is included in the application. In the Station Table Results table, parameters are highlighted in different colors to indicate further review may be necessary. Parameters highlighted in yellow have at least one violation of a standard or criteria. When BENTHIC_STAT is highlighted, it indicates there is benthic data available for that site and the assessor should review that information with information presented in the Benthic tab below and their Regional Biologist. Parameters highlighted in red exceed the 10.5% exceedance rate. Both scenarios warrant further investigation and may require comments in the Station Table and CEDS WQA. Pro Tip: The Station Table Results table is editable by the user. This is helpful when the user wants to minimize the number of applications running throughout a given assessment session. Should the assessor determine that an automated result presented in the table requires changing, they may triple-click (click three times in rapid succession) on the cell that needs the change to reveal a text entry box. The user may edit that cell to whatever they desire, preferably following the rules outlined in the Data Entry Manual. Regardless whether any cells were manipulated, the user may use the Copy/CSV/Excel buttons above the table to copy the table information to their clipboard or download the information as the chose file format. The PWS Criteria table indicates PWS criteria results if the chosen station has PWS designation in the WQS metadata attached to the station. If PWS criteria apply to the station, note that Chloride, Sulfate, Total Dissolved Solids, Iron, and Foaming Agents are secondary criteria and are only applicable to data collected at the drinking water intake. A flag is presented in this section if the station selected falls within 100 meters of a drinking water intake to improve the assessment of these parameters. No set distance is established in guidance to define a station at the intake and assessor digression is encouraged to determine whether these secondary standards apply to data collected at the station. The Assessment Unit Raw Data Review and Visualization area organizes all the standardized statewide datasets organized for automated assessment uses in one area to expedite data review for individual stations. The area is divided into multiple tabs, each with subtabs. These hierarchical tabs organize the conventionals data and all associated parameter modules, benthic data, metals data, and toxics data. Future iterations of this application may include more data review areas as the underlying data sources undergo rigorous organizational steps in order to be sourced by the automated applications and scripts. By clicking any of the above mentioned tabs, users may begin to dig into different types of data for a chosen station. 5.4.1 Conventionals Data Tab The Conventionals tab provides visualization and analysis modules for each parameter with waterbody-specific criteria or standards. 5.4.1.1 Raw Data The Raw Data subtab presents all the conventionals data filtered to the current cycle. Users may download or copy this information to their clipboard for investigation in other programs. The Data Summary section provides AU and station record counts. 5.4.1.2 Temperature The Temperature subtab displays an interactive graph of all temperature data for the chosen station as well as the applicable WQS. Users may change the WQS assigned to the station and reassess on the fly using the WQS for Analysis drop down menu. This feature controls all the exceedance and summary statistics in the station area of the modal. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. A low flow flag appears when the station selected contains data (Temperature, pH, or DO) collected on the same date where a USGS gage in the major river basin (James, York, Potomac, etc.) fell below the gage-specific 7Q10 calculation. Red bold language appears to indicate the chosen station contains data that might have been collected during a low flow event. The data that is flagged for 7Q10 is represented with a red halo behind it in the interactive plot. Users may find out more information about the proximity of this flagged gage to the station, flow statistics, and USGS NWIS links inside the modal visible by clicking the Review Low Flow Information button. The table at the bottom of that modal recalculates the stations exceedance statistics based on the assumption that the data flagged with the 7Q10 flag should not be used for assessment. It is up to the assessor to determine whether or not this data should be assessed or not due to flow on the day the sample was collected. 5.4.1.3 pH The pH subtab displays an interactive graph of all pH data for the chosen station as well as the applicable WQS. Users may change the WQS assigned to the station and reassess on the fly using the WQS for Analysis drop down menu. This feature controls all the exceedance and summary statistics in the station area of the modal. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The Display Benthic Stressor Analysis Colors on Plot checkbox allows users to visualize thresholds of potential stress to aquatic organisms for increased data context. A low flow flag appears when the station selected contains data (Temperature, pH, or DO) collected on the same date where a USGS gage in the major river basin (James, York, Potomac, etc.) fell below the gage-specific 7Q10 calculation. Red bold language appears to indicate the chosen station contains data that might have been collected during a low flow event. The data that is flagged for 7Q10 is represented with a red halo behind it in the interactive plot. Users may find out more information about the proximity of this flagged gage to the station, flow statistics, and USGS NWIS links inside the modal visible by clicking the Review Low Flow Information button. The table at the bottom of that modal recalculates the stations exceedance statistics based on the assumption that the data flagged with the 7Q10 flag should not be used for assessment. It is up to the assessor to determine whether or not this data should be assessed or not due to flow on the day the sample was collected. 5.4.1.4 Dissolved Oxygen The DO subtab displays an interactive graph of all dissolved oxygen data for the chosen station as well as the applicable WQS. Users may change the WQS assigned to the station and reassess on the fly using the WQS for Analysis drop down menu. This feature controls all the exceedance and summary statistics in the station area of the modal. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. Daily average DO exceedances and summary statistics are visible below the raw data analysis. The Display Benthic Stressor Analysis Colors on Plot checkbox allows users to visualize thresholds of potential stress to aquatic organisms for increased data context. A low flow flag appears when the station selected contains data (Temperature, pH, or DO) collected on the same date where a USGS gage in the major river basin (James, York, Potomac, etc.) fell below the gage-specific 7Q10 calculation. Red bold language appears to indicate the chosen station contains data that might have been collected during a low flow event. The data that is flagged for 7Q10 is represented with a red halo behind it in the interactive plot. Users may find out more information about the proximity of this flagged gage to the station, flow statistics, and USGS NWIS links inside the modal visible by clicking the Review Low Flow Information button. The table at the bottom of that modal recalculates the stations exceedance statistics based on the assumption that the data flagged with the 7Q10 flag should not be used for assessment. It is up to the assessor to determine whether or not this data should be assessed or not due to flow on the day the sample was collected. 5.4.1.5 Specific Conductance The Specific Conductance subtab displays an interactive graph of all specific conductivity data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The Display Benthic Stressor Analysis Colors on Plot checkbox allows users to visualize thresholds of potential stress to aquatic organisms for increased data context. 5.4.1.6 Salinity The Salinity subtab displays an interactive graph of all salinity data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. 5.4.1.7 Total Nitrogen The Total Nitrogen subtab displays an interactive graph of all TN data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The Display Benthic Stressor Analysis Colors on Plot checkbox allows users to visualize thresholds of potential stress to aquatic organisms for increased data context. 5.4.1.8 Ammonia The Ammonia subtab displays an interactive graph of all ammonia data for the chosen station as well as the applicable WQS. The Freshwater Criteria Default Analysis Settings are preset based on the WQS Class attributed to the station. All analyses presented reflect these conditions. The default WQS settings are specified to expedite application rendering time. If the default analysis settings do not meet your needs, please contact Emma Jones (emma.jones@deq.virginia.gov). Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The Combined Ammonia Criteria Analysis Results section allows users to explore this complicated ammonia standard interactively. All ammonia records that are above the acute, chronic, or four day criteria for the selected site are highlighted in the top table. The next section allows users to interactively explore data in each three year rolled window for each criteria type. By clicking any of the rows in the Three year window summaries table, users will update a table that contains all data within the chosen three year window filtered to the selected criteria analysis type. The next table summarizes all three year windows by criteria type to suggest a final decision. The last part of the modal features an Ammonia Criteria In Depth Analysis. Users may investigate each ammonia measurement by criteria type to better understand the data and criteria calculation for each data point collected. 5.4.1.9 Total Phosphorus The Total Phosphorus subtab displays an interactive graph of all TP data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The Display Benthic Stressor Analysis Colors on Plot checkbox allows users to visualize thresholds of potential stress to aquatic organisms for increased data context. 5.4.1.10 Fecal Coliform The Fecal Coliform subtab displays an interactive graph of all fecal coliform data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. 5.4.1.11 E. Coli The E. Coli subtab displays an interactive graph of all E.coli data for the chosen station. The most recent two years of the assessment period are highlighted on the graph in gray to illustrate the data that are considered for assessment decisions per assessment guidance. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The next two tables highlight any exceedances (STV or geomean) of the new recreation E.coli standard (STV= 410 CFU / 100 mL, geomean = 126 CFU / 100 mL with additional sampling requirements) on the left table and then summarizes the rolling windows to suggest an assessment decision with standardized language for input into CEDS WQA. The new bacteria standards are complicated. The New Recreation Standard In Depth Analysis section unpacks these rolling windows across the entire dataset to better explain the suggested assessment result above. The table to the left allows users to select any sample and plots a graph on the right with all data contained within that data points 90 day rolling window. The table highlights all data collected within the most recent two years of the assessment period (data valid for assessment decisions) in gray. The interactive plot on the right plots the STV and geomean criteria as well as all data and the window calculated geomean. The orange line corresponds to the window geomean; wide black dashed line corresponds to the geomean criteria; thin black dashed line corresponds to the STV limit. Below the plot is a table with specific assessment logic regarding the data included in the selected 90 day window. The Valid Assessment Window field is a boolean result communicating whether the chosen data window is valid for assessment (contain all unique data). Implementing this check ensures rolling window calculations do not proceed beyond the addition of new bacteria data. The Analyzed Data (Each window with an individual STV and geomean assessment decisions) offers detailed information on STV and geomean assessments for every 90 day window in the dataset. Data valid for recreational use assessment are colored in gray. The results of the old E.coli standard is presented at the bottom of the modal to communicate how the data compare to previous assessment cycles. 5.4.1.12 Enterococci The Enterococci subtab displays an interactive graph of all enterococci data for the chosen station. The most recent two years of the assessment period are highlighted on the graph in gray to illustrate the data that are considered for assessment decisions per assessment guidance. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The next two tables highlight any exceedances (STV or geomean) of the new recreation enteroccoci standard (STV= 130 CFU / 100 mL, geomean = 35 CFU / 100 mL with additional sampling requirements) on the left table and then summarizes the rolling windows to suggest an assessment decision with standardized language for input into CEDS WQA. The new bacteria standards are complicated. The New Recreation Standard In Depth Analysis section unpacks these rolling windows across the entire dataset to better explain the suggested assessment result above. The table to the left allows users to select any sample and plots a graph on the right with all data contained within that data points 90 day rolling window. The table highlights all data collected within the most recent two years of the assessment period (data valid for assessment decisions) in gray. The interactive plot on the right plots the STV and geomean criteria as well as all data and the window calculated geomean. The orange line corresponds to the window geomean; wide black dashed line corresponds to the geomean criteria; thin black dashed line corresponds to the STV limit. Below the plot is a table with specific assessment logic regarding the data included in the selected 90 day window. The Valid Assessment Window field is a boolean result communicating whether the chosen data window is valid for assessment (contain all unique data). Implementing this check ensures rolling window calculations do not proceed beyond the addition of new bacteria data. The Analyzed Data (Each window with an individual STV and geomean assessment decisions) offers detailed information on STV and geomean assessments for every 90 day window in the dataset. Data valid for recreational use assessment are colored in gray. The results of the old Enterococci standard is presented at the bottom of the modal to communicate how the data compare to previous assessment cycles. 5.4.1.13 Chlorophyll a The Chlorophyll a subtab displays an interactive graph of all chlorophyll a data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. 5.4.1.14 Suspended Sediments The Suspended Sediments subtab displays an interactive graph of all suspended sediment data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. 5.4.1.15 Nitrate The Nitrate subtab displays an interactive graph of all nitrate data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The Display Benthic Stressor Analysis Colors on Plot checkbox allows users to visualize thresholds of potential stress to aquatic organisms for increased data context. The Apply Public Water Supply Water Quality Standards checkbox is automatically selected if PWS standards apply to the selected station based on metadata attributed to the station. If PWS standards apply, the parameter-specific PWS criteria is added as a line to the interactive plot black solid line. The orange dashed line is the nitrate median across the assessment window (visible if the Apply Public Water Supply Water Quality Standards checkbox is selected). If PWS criteria apply to the station, tables of nitrate exceedances and exceedance summary appear below the plot. Users may toggle the Apply Public Water Supply Water Quality Standards checkbox to understand how data compare to PWS criteria even if the station is not designated within a PWS segment. 5.4.1.16 Chloride The Chloride subtab displays an interactive graph of all chloride data for the chosen station. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. The Display Benthic Stressor Analysis Colors on Plot checkbox allows users to visualize thresholds of potential stress to aquatic organisms for increased data context. The Apply Public Water Supply Water Quality Standards checkbox is automatically selected if PWS standards apply to the selected station based on metadata attributed to the station. If PWS standards apply, the parameter-specific PWS criteria is added as a line to the interactive plot black solid line. The orange dashed line is the chloride median across the assessment window (visible if the Apply Public Water Supply Water Quality Standards checkbox is selected). If PWS criteria apply to the station, tables of chloride exceedances and exceedance summary appear below the plot. As the image shows below, users may toggle the Apply Public Water Supply Water Quality Standards checkbox to understand how data compare to PWS criteria even if the station is not designated within a PWS segment. The Freshwater Chloride Criteria Analysis Results section allows users to explore this complicated chloride standard interactively. All chloride records that are above the acute or chronic criteria for the selected site are highlighted in the top table. The next section allows users to interactively explore data in each three year rolled window for each criteria type. By clicking any of the rows in the Three year window summaries table, users will update a table that contains all data within the chosen three year window filtered to the selected criteria analysis type. The next table summarizes all three year windows by criteria type to suggest a final decision. 5.4.1.17 Sulfate The Sulfate subtab displays offers total sulfate and dissolved sulfate analyses and visualizations. Users may review and download raw parameter data by clicking the Review Raw Parameter Data button to reveal a modal dialog box with a table and various copy/download options. With Total Sulfate selected in the Select Total or Dissolved Sulfate drop down, the module provides an interactive graph of all total sulfate data for the chosen station. The Apply Public Water Supply Water Quality Standards checkbox is automatically selected if PWS standards apply to the selected station based on metadata attributed to the station. If PWS standards apply, the parameter-specific PWS criteria is added as a line to the interactive plot black solid line. The orange dashed line is the total sulfate median across the assessment window (visible if the Apply Public Water Supply Water Quality Standards checkbox is selected). If PWS criteria apply to the station, tables of total sulfate exceedances and exceedance summary appear below the plot. Users may toggle the Apply Public Water Supply Water Quality Standards checkbox to understand how data compare to PWS criteria even if the station is not designated within a PWS segment. With Dissolved Sulfate selected in the Select Total or Dissolved Sulfate drop down, the module provides an interactive graph of all dissolved sulfate data for the chosen station. The Display Benthic Stressor Analysis Colors on Plot checkbox allows users to visualize thresholds of potential stress to aquatic organisms for increased data context. 5.4.2 Benthic Data Tab The Benthic Data Tab presents benthic Stream Condition Index (SCI) scores and raw metrics for all data within the assessment window. If no benthic information is presented, then there is no macroinvertebrate data for the station. Always review biologist fact sheets to assist assessment decisions. This information is greatly enhanced by detailed bioassessments provided by regional biological staff for each station sampled in the assessment window. Please discuss with your regional biologist any questions you might have about bioassessment decisions. It is very important for biological staff to participate in the bioassessment archiving process (detailed instructions available through the Bioassessment Applications How To section). This archive is the only method for standardizing and archiving bioassessment data for future staff as well as generates fact sheets for biological data to provide to citizens, assessors, and other DEQ staff. Participation expedites the regional assessors workload for biological assessments. The Default Biological Information subtab is populated regardless of regional biological staff participating in standardized bioassessment archiving processes. The Only analyze wadeable method samples checkbox is checked by default to only analyze data with approved SCI methods. Users may toggle this off to visualize all biological data. The SCI methods are only validated on wadeable methods. Boatable data should be viewed for informational purposes only. The Only analyze replicate 1 samples checkbox is checked by default. Users may toggle this off to visualize all biological data. Only replicate 1 samples should be assessed. A table provides subbasin and Level III Ecoregion information based on the sample location. This information is used to suggest the most appropriate SCI method to assess the benthic data, but confirmation from regional biological staff is highly recommended. The SCI for Analysis drop down is autopopulated with a recommended SCI based on subbasin and ecoregion information. Should a user want to change the SCI methodology and reassess using a different method, all three SCI methods are available for data exploration. An interactive graph presents of all SCI results (based on selection in the SCI for Analysis drop down above) for the chosen station. The appropriate SCI threshold is plotted along with all SCI results. The Stream Condition Index Sampling Metrics section details sampling and SCI metrics as well as yearly average SCI results in the assessment window. All raw benthic metric results are available for further exploration in the Raw Benthic Results table. The Bioassessment Additional Information subtab is populated if your regional biological staff participate in the standardized bioassessment archiving process. Two tabs divide this data by bioassessment information in the current cycle (Current Integrated Report Window Bioassessment Information) and previous cycles (Previous Integrated Report Windows Bioassessment Information). All data presented in this module was submitted by the Regional Biologist referenced in each table by submitting bioassessment data into the Bioassessment Fact Sheet Generator Tool and saved on the R server. Please follow up with that biologist to answer any questions about this station. The Current Integrated Report Window Bioassessment Information subtab offers additional bioassessment information if regional biological staff have completed the bioassessment process in the current assessment cycle. The Generate Report button autogenerates a biological fact sheet based on all biological, habtitat, and bioassessment information available for the station. Users may view this downloaded fact sheet in a browser of their choosing. The Previous Integrated Report Windows Bioassessment Information subtab offers additional bioassessment information if regional biological staff have completed the bioassessment process in any previous assessment cycle. This information is important for historical understanding of bioassessments for the selected station. 5.4.3 Metals Data Tab The Metals Data tab organizes data into three subtabs based on the data type: Water Column Metals, Sediment Metals, and Fish Tissue Metals. 5.4.3.1 Water Column Metals The Water Column Metals subtab allows users to interactively explore water column metals data for a chosen station. Where water column metals data exist for a chosen station, the Analyzed Data subtab provides an Aquatic Life Use rolling window suggested result for each water column metal criteria. The default Water Effects Ratio (WER) used for module analysis is set to 1 to expedite application rendering time, but users may reassess the data on the fly using the WER that is most appropriate to the given station. All subsequent analyses in that module are updated when a user changes the WER field. The Three Year Window Summaries section allows users to explore these complicated criteria interactively. Uusers may interactively explore data in each three year rolled window for each criteria any parameter type. By clicking any of the rows in the Three year window summaries table, users will update a table that contains all data within the chosen three year window filtered to the selected criteria analysis type. All raw water column metals data for the chosen station are presented in the Raw Data subtab. "],["lacustrine-application-how-to.html", "Chapter 6 Lacustrine Application How To", " Chapter 6 Lacustrine Application How To The Lakes Assessment App is located on the R server. update link to 2024 when pushed "],["bioassessment-applications-how-to.html", "Chapter 7 Bioassessment Applications How To", " Chapter 7 Bioassessment Applications How To There are two bioassessment tools designed to assist biological staff with the assessment process. They include the Bioassessment Dashboard and the Benthic Assessment Fact Sheet Generator. Both tools are helpful throughout the bioassessment process, providing standardized data queries, visualizations, and analyses for biologists. The CEDS Benthic Data Query Tool is a supplementary resource for more in depth benthic analyses, but that tool will not be discussed in depth in this chapter. "],["general-bioassessment-workflow.html", "7.1 General Bioassessment Workflow", " 7.1 General Bioassessment Workflow After benthic macroinvertebrate data collection, processing, identification, and CEDS data entry, regional biological staff should assess all samples collected within a given Integrated Report window and provide station-level summaries to regional assessment staff. Biologists are best suited to make biological assessment decisions due to their expertise in benthic community structure and because they visited the sites and thus have personal knowledge of the site, watershed, and sampling conditions that might affect how data should be interpreted. Historically, biologists provided bioassessment decisions to regional assessment staff through individual bioassessment fact sheets. These one page documents summarize all benthic and habitat sampling data from a given site and are excellent resources for monitoring, assessment, and TMDL staff well after the fact sheet was written. Regional assessment staff rely on these fact sheets to relay expert interpretation of biological data readily into assessment decisions. These fact sheets improve communication between the monitoring and assessment arms of the programs and can help guide water planning efforts. However, these fact sheets were time consuming to create and difficult to keep track of during and after a given assessment cycle. With the adoption of more automated methods throughout the assessment process, regional biologists received two automated tools to standardize and expedite the bioassessment process. The Bioassessment Dashboard is intended as a first stop for biologists as they begin bioassessment in a given cycle. The tool identifies all biological and habitat data collected within a given assessment cycle, along with station-specific bioassessment information from previous assessment cycles, to readily assist with biological assessment decisions. This interactive tool performs all necessary data querying, manipulation along with interactive mapping, plotting, and tabular data summaries. When a station-level biological decision has been made, the regional biologists enter that decision and other metadata into a template (download a copy of the template here) {target=\"_blank\"}. All fields must be completed in the template. When one or more bioassessment decisions are completed in the template, biologists may submit these decisions to be archived on the R server using the Benthic Assessment Fact Sheet Generator. This tool allows biologists to append their bioassessment decisions to a pinned dataset on the R server (with a point and click interface) and, once bioassessment decisions are pinned, they are able to generate and download a standardized biological fact sheet with the click of a button. There is no need to send these fact sheets to assessment staff and they have access to these fact sheets inside the Riverine Assessment Application as soon as the bioassessment data is pinned (archived) on the R server. "],["application-use-bioassessment-dashboard.html", "7.2 Application Use: Bioassessment Dashboard", " 7.2 Application Use: Bioassessment Dashboard 7.2.1 Map Tab On load, the main panel of the Bioassessment Dashboard displays all biological sampling locations from the current IR cycle on the Map tab. The map allows users to zoom and pan interactively with the mouse. In the top left corner there are a number of tools to aid users. The plus and minus buttons offer fixed zoom in/out. The home button returns the map orientation to the original zoom level in case a user gets lost. Additional layers available in the map (by hovering over the layers button on the left side) include US EPA Level 3 Ecoregions and Assessment Regions (regional assessment boundaries). A table below the map displays station metadata information for all stations currently displayed in the map. All tables in the dashboard are interactive and columns may be sorted or turned on/off (using the Column Visibility button in the upper left corner) as well as copied to the users clipboard. Before proceeding to subsequent tabs, it is beneficial to refine the working dataset by Basin, Collector, StationID, or Replicate filters available in the left side panel. Pro Tip: These filters are cascading hierarchical filters. Starting from the top down, if a user chooses one or more Basins, then only the Collectors with data within these Basins are provided in the Collector Filter box. This logic is true of the StationID Filter box as well. Not every box must be filled in to utlize the heirarchical filtering. Pro Tip: Once the user has set the query conditions to their liking, the hamburger button in the blue navigation bar will hide/show the sidebar. Hiding the sidebar can be helpful to maximize data visualization features in the browser window space. 7.2.2 SCI Scores Tab Based on the query terms, the Stream Condition Index (SCI) scores will appear in an interactive plot. The plot is composed of layers of replicate number by season by sampling method (e.g.Rep 1 Fall (Boatable) or Rep 2 Spring (Riffle)). Additionally, both VSCI and VCPMI criteria are plotted as layers. All layers may be turned on/off by clicking on them in the legend in the left hand side of the plot. Helpful interactive features of the plot include hovering over samples to reveal a popup box with additional sampling information, downloading the plot as a png, zooming to areas of the plot with mouse selection, autoscaling, resetting the axes, and more. Hover your mouse over the top right of the plot to reveal these features and more. A table below the plot displays benthic sample information for all stations currently displayed in the plot. Sample information, SCI scores, and SCI metrics can be found for each sample in this table. All tables in the dashboard are interactive and columns may be sorted or turned on/off (using the Column Visibility button in the upper left corner) as well as copied to the users clipboard. 7.2.2.1 SCI Method Decision SCI scores are provided for each station based on the station location to expedite application rendering time. Basin and Level 3 Ecoregion information spatially derived from the sample latitude and longitude provide the VCMPI method determination. Should you wish to investigate other SCI methods for a given station, the CEDS Benthic Data Query Tool provides higher level of analytical control. 7.2.3 Habitat Scores Tab Based on the query terms, the Total Habitat Scores will appear in an interactive plot. The plot is composed of layers of season by sampling method (e.g.Fall (High Gradient Method) or Spring (Low Gradient Method)). Additionally, the Benthic Stressor Analysis thresholds indicating probability of stress to aquatic life are plotted as layers. All layers may be turned on/off by clicking on them in the legend in the left hand side of the plot. Helpful interactive features of the plot include hovering over samples to reveal a popup box with additional sampling information, downloading the plot as a png, zooming to areas of the plot with mouse selection, autoscaling, resetting the axes, and more. Hover your mouse over the top right of the plot to reveal these features and more. A table below the plot displays habitat sample information for all stations currently displayed in the plot. Sample information, individual habitat metric scores, and Total Habitat Scores can be found for each sample in this table. The cells are color coded in a red scheme where darker colors highlight lower scores for a given habitat metric. If a metric is &lt;= 10, the metric is bolded. All tables in the dashboard are interactive and columns may be sorted or turned on/off (using the Column Visibility button in the upper left corner) as well as copied to the users clipboard. 7.2.4 Station Summary Tab Based on the query terms, the SCI Summary Table will display summaries of the SCI averages across various windows. Where data are available, these windows include the entire IR window (6 year average), most recent two years of the IR period average, spring sample average, fall sample average, and individual year averages. The number of samples in each window is provided in the n Samples field. All tables in the dashboard are interactive and columns may be sorted or turned on/off (using the Column Visibility button in the upper left corner) as well as copied to the users clipboard. The Total Habitat Summary Table will display summaries of the Total Habitat Scores averages across various windows. Where data are available, these windows include the entire IR window (6 year average), most recent two years of the IR period average, spring sample average, fall sample average, and individual year averages. The number of samples in each window is provided in the n Samples field. All tables in the dashboard are interactive and columns may be sorted or turned on/off (using the Column Visibility button in the upper left corner) as well as copied to the users clipboard. The Previous Integrated Report Cycle Bioassessment Information Table will display bioassessment decision information from previous IR windows, where this information is available in the standardized archived format. This information can be helpful to review when making biological assessments to understand any historical decisions. 7.2.5 Assessment Decision Tab Based on the query terms, the Assessment Decision Summary Table displays bioassessment information for the current IR cycle. If you have uploaded information about the chosen station(s) to the VDEQ Benthic Assessment Fact Sheet Generator, you will see information that is saved on the server in the table below. If you upload new data about the selected station(s) to the VDEQ Benthic Assessment Fact Sheet Generator, that information will be reflect here after you refresh the dashboard. "],["application-use-benthic-assessment-fact-sheet-generator.html", "7.3 Application Use: Benthic Assessment Fact Sheet Generator", " 7.3 Application Use: Benthic Assessment Fact Sheet Generator In an effort to make DEQs custom applications more dynamic, the Benthic Assessment Fact Sheet Generator generates two types of standardized Benthic Fact Sheets for DEQ stations. The Assessment Fact Sheet tab uploads bioassessment decisions (from a template) to be archived on the R server as pinned data and generates fact sheets for data collected within a chosen data window. This tool was developed to be used after a regional biologist assesses stations using the Bioassessment Dashboard. Assessment decisions archived as pinned data on the R server feed the Riverine Assessment application. The General Purpose Biological Fact Sheet tab generates a generic fact sheet with benthic macroinvertebrate and habitat information. The format of this report follows the same structure of the Assessment Fact Sheet report, but it allows users to select specific sample windows from all available benthic data and leaves most of the comment sections blank for the user to fill out in the Word document. This version does not save any information to the R server. 7.3.1 Assessment Fact Sheet Tab On the Assessment Fact Sheet Tab, users may choose from stations that have existing assessment decisions saved on the R server or upload data to the application from a template to be archived on the R server. Once the data are archived on the R server as pinned data, these new decisions are available to generate benthic fact sheets. Once a station and IR window is chosen, the Generate Report button compiles benthic and habitat data within the chosen IR window with biologist assessment decisions and writes a report, downloaded to the users Downloads folder. 7.3.2 General Purpose Biological Fact Sheet Tab On the General Purpose Biological Fact Sheet Tab, users may choose from any station that has benthic macroinvertebrate information. Once a station is chosen, the Generate Report button compiles benthic and habitat data within the selected collection window chosen by the user and writes a Microsoft Word report with some basic plots and tables, downloaded to the users Downloads folder. This version is sparse and is meant to be completed by the biologist for the intended audience. Due to the nature of MS Word documents, the interactive color coded (and extremely wide) html tables do not translate well to that medium. Users may choose from a long version of SCI and habitat metrics (included as a default for their chosen station and collection window) or save images of the html tables presented in the application for manual inclusion in their report. "]]
