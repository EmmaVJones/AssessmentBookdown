[["index.html", "DEQ Water Quality Automated Assessment User Guide Chapter 1 Introduction", " DEQ Water Quality Automated Assessment User Guide DEQ Automated Assessment Team Last Updated: 2022-08-24 Chapter 1 Introduction This manual serves as a companion document to DEQs Water Quality Assessment (WQA) automated assessment methodology for completion of the biennial 305b/303d Integrated Report. Virginia is among a handful of state agencies that have organized concerted efforts to systematically automate and expedite the water quality assessment process. These entities have all taken different approaches to best meet their state-specific analysis and reporting needs. Through the Tools for Automated Data Assessment (TADA) working group, EPA is organizing an effort to share code and create national tools to assist partners with assessment analyses. Considering the technological advances in water monitoring that have increased the quantity and types of data requiring consideration during an assessment period, as well as the static or decreasing staff time allocated to the assessment process, automation appears the only solution to keep up with Clean Water Act requirements and keep the public informed of the status of waterbody health and restoration timelines for those waterbodies not meeting water quality standards. By embracing automated assessment procedures, entities aim to provide constituents with higher quality, standardized, and transparent water quality assessment results while adhering to federally mandated deadlines. Virginia has taken a hybrid approach to the process of automating the water quality assessments. DEQ relies upon an automated process to manipulate hundreds of thousands of water quality data collected throughout the state during a given assessment period (six year window). These scripts evaluate each record for exceedances of appropriate Water Quality Standards. Regional assessment staff may use interactive web-based analytical tools provide more context to tabular results of exceedance analyses to assist with Quality Assurance/Quality Control (QA/QC) prior to submitting data to EPAs Assessment, Total Maximum Daily Load (TMDL) Tracking and Implementation System (ATTAINS) system through DEQs internal Comprehensive Environmental Data System (CEDS). This approach maximizes the benefits of each partner in the assessment process, allowing computers to excel at systematic and expedited data manipulation and analysis at scale and allowing humans to digest multiple lines of evidence to identify any potential data collection or analysis errors or areas of environmental concern. "],["HowToUseDocument.html", "1.1 How to Use Document", " 1.1 How to Use Document The following report is an agency effort to facilitate the further adoption of automated assessment methods statewide. The intention of this project is to provide a non-programmer audience with accessible and understandable narratives describing the automated water quality assessment process. This document is not a comprehensive WQA Guidance Manual, nor is it an introduction to using the R programming language. Here, we document the overall automated assessment process, explaining reasons certain decisions were made, and how to unpack analyses. "],["ProjectHistory.html", "1.2 Project History", " 1.2 Project History The agency began efforts toward automating components of the IR with a dissolved metals assessment written in SAS. These tabular results were provided to regional assessment staff along with raw data queries encompassing data stored in CEDS for each IR data window. The Freshwater Probabilistic Monitoring program began automating analyses and a final report for inclusion in the 2016 IR using the open source R programming language. These practices have been further refined each IR cycle, improving report quality and graphics as well as significantly reducing the amount of time required to generate a report after data collection. These methods were identified as a possible solution to the ongoing challenges in the WQA program to complete vast amounts of data analysis on increasingly short timelines with limited staff. An effort to systematically analyze all water quality data stored in CEDS for assessments began in 2017. As a pilot project, lakes and reservoirs in the Blue Ridge Regional Office (BRRO) assessment region were selected for first waterbody type to undergo automation and receive an interactive web-based tool to assist regional assessment staff for the 2018 IR. These initial automated assessment efforts were completed using the open source R programming language and interactive applications were built using the Shiny package. Rivers and streams followed suit for the 2020 IR, joining lacustrine waterbodies with automated assessment methods and interactive assessment tools. The 2020 IR automated methods were scaled from just the BRRO region to statewide applicability. A pilot project to incorporate citizen monitoring data requiring assessment was undertaken in the BRRO assessment region for the 2020 IR. This pilot proved effective in standardizing and increasing efficiency of incorporating this disparate data and was officially adopted as a process for future IR windows. After thorough QA from regional assessment staff across the state, the riverine and lacustrine automated assessment tools were rebooted for the 2022 IR with increased functionality and the ability to assess more parameters, including those not stored in CEDS. However, due to delays organizing citizen monitoring data statewide, automated results for these data were not provided for the 2022 IR. A new database schema for archiving station-specific water quality standards and assessment unit information was implemented. This system benefited the assessment process as well as numerous DEQ water programs that previously did not have access to WQS information at that spatial scale. Appendices and fact sheets for the IR were generated using R and Rmarkdown for the first time during the 2022 IR. The 2024 IR further refines improvements to the riverine and lacustrine automated assessment methods and interactive tools. By partnering with the Chesapeake Monitoring Cooperative (CMC), DEQ leveraged an existing public-facing citizen monitoring data portal to expand utility outside the Chesapeake Bay watershed and incorporate all of Virginias citizen monitoring data. These data are now automatically cleaned and stored by the CMC and can more easily be incorporated in the automated assessment process with web scraping techniques. To date, no estuarine-specific assessment methods have been completed, but the WQA program is investigating utility and potential adoption among regions with estuarine waters. "],["Acknowledgements.html", "1.3 Acknowledgements", " 1.3 Acknowledgements Many Water Quality Assessment (WQA) and Water Quality Standards (WQA) staff have contributed to this effort: Emma Jones (emma.jones@deq.virginia.gov) Kristen Bretz (kristen.bretz@deq.virginia.gov) Jason Hill (jason.hill@deq.virginia.gov) Sandy Mueller (sandra.mueller@deq.virginia.gov) Cleo Baker (cleo.baker@deq.virginia.gov) Amanda Shaver (amanda.shaver@deq.virginia.gov) Tish Robertson (tish.robertson@deq.virginia.gov) Paula Main (paula.main@deq.virginia.gov) Mary Dail (mary.dail@deq.virginia.gov) Martha Chapman (martha.chapman@deq.virginia.gov) Sara Jordan (sara.jordan@deq.virginia.gov) Kristie Britt (kristie.britt@deq.virginia.gov) Jennifer Palmore (jennifer.palmore@deq.virginia.gov) Kelley West (kelley.west@deq.virginia.gov) Rebecca Shoemaker (rebecca.shoemaker@deq.virginia.gov) Please direct any project questions to Emma Jones (emma.jones@deq.virginia.gov). "],["data-organization.html", "Chapter 2 Data Organization", " Chapter 2 Data Organization The assessment process requires hundreds of thousands of rows of data collected and stored by DEQ, other state agencies, and citizen partners. These disparate data sources require various levels of data manipulation prior to any analysis. SAS and R are used for the majority of the assessment data cleaning and manipulation processes. 2.0.1 Data Location Most data used for assessments are stored in DEQs internal Comprehensive Environmental Data System (CEDS) and made available through a direct connection to the reporting database known as ODS. The agency is working towards storing all data required for assessments in CEDS, but as of the time of writing the following datasets are stored in locations outside of CEDS: * Fish Tissue Data * PCB Data * VDH Data * Citizen Monitoring Data * Station Metadata It is important to note that although the data that consitute the conventionals dataset are derive from CEDS/ODS, official assessment records of this data are only stored locally in Microsoft Excel outputs. 2.0.2 Data Availablility Most of the following data are provided at the beginning of the assessment process (approximately March of an assessment year). Any delays in data availability have ripple effects on the ability of regional assessors to complete their work on time. Should data be provided for assessment after the expected availability date, assessors may not be able to include said data in a given assessment window. Exceptions to the March data availability date usually apply to Citizen Monitoring, bioassessment, and fish tissue data. Citizen Monitoring data have historically been provided to regional assessment staff in April of a given assessment year. Bioassessment data for the most recent two years of an assessment window trickles in to the assessment process through the summer of a given assessment year due to the lengthy identification process associated with benthic macroinvertebrate samples. Fish tissue data requires protracted laboratory analyses before it is provided back to DEQ from contractor labs for assessment purposes. Due to these data delays, regional assessment staff generally have to delay certain assessment steps until all data is available for a given station/Assessment Unit, making automated assessment methods ever more important when data are provided. "],["conventionals-data.html", "2.1 Conventionals Data", " 2.1 Conventionals Data The conventionals dataset contains the bulk of the data analyzed during any given assessment window. Historically, this dataset has been queried using SAS and a direct connection to the raw monitoring data in the ODS reporting database (the database that makes CEDS data accessible to reporting tools). Recently, efforts to streamline the assessment process and standardize data provided across agency programs produced an effort to convert these SAS scripts into R. The conventionals query combines WQM field and analyte data with data handling steps that standardize data discrepancies like multiple lab values returned for identical sample date/times, full parameter speciation but no total value, etc. This R query and data standardization method is considered to be under development as code review is a constant part of data improvement strategies. The current version of the R based conventionals query is available here. In order to access the data the conventionals function calls, users must have a direct connection to ODS from their environment. Please see the DEQ R Methods Encyclopedia article on ODS for more information. A sample conventionals dataset is available in the exampleData directory for you to download and use when practicing the automated scripts. conventionals &lt;- readRDS(&#39;exampleData/conventionals.RDS&#39;) conventionals[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["water-column-metals.html", "2.2 Water Column Metals", " 2.2 Water Column Metals query from Roger, for now "],["sediment-metals.html", "2.3 Sediment Metals", " 2.3 Sediment Metals query from Roger, for now "],["fish-tissue-data.html", "2.4 Fish Tissue Data", " 2.4 Fish Tissue Data provided by Rick &amp; Gabe, for now "],["pcb-data.html", "2.5 PCB Data", " 2.5 PCB Data From Mark Richards, not in CEDS "],["vdh-data.html", "2.6 VDH Data", " 2.6 VDH Data Beach closure, HAB event "],["citizen-monitoring-data.html", "2.7 Citizen Monitoring Data", " 2.7 Citizen Monitoring Data Citizen Monitoring data have historically been provided to DEQ in various data formats and schema using numerous digital and analog storage methods. This data system required multiple iterations of lengthy data standardization and QA/QC processes in order to ensure the data were utilized for assessments. A standardized system requiring citizen groups to either upload their data to the Chesapeake Monitoring Cooperative (CMC) Data Explorer and DEQ scraping the CMC API using automated R scripts or provide all data to DEQ in a standardized template has replaced previous data receiving methods. Citizen data not stored in the CMC database live on DEQ staff computers and require further storage solutions. A sample citizen monitoring dataset is available in the exampleData directory for you to download and use when practicing the automated scripts. citmon &lt;- readRDS(&#39;exampleData/citmon.RDS&#39;) citmon[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["station-metadata.html", "2.8 Station Metadata", " 2.8 Station Metadata Station metadata are critical to ensuring that stations are assessed appropriately whether an assessment is conducted by hand or with automation. These metadata include the appropriate Assessment Unit and Water Quality Standard information that apply to the station. After this information is identified for a given station, it may be assessed according the the assessment rules that apply to those standards for the specified waterbody type. Metadata are provided for stations through a combination of automated spatial analyses and human review. No metadata are ever linked to stations without regional assessor review. The Metadata Attribution section covers the details of attributing metadata to each station. "],["automated-assessment-methods.html", "Chapter 3 Automated Assessment Methods", " Chapter 3 Automated Assessment Methods This chapter details the specific steps required to transform the raw data outlined in the Data Organization chapter into preliminary assessment results. The general process the automated assessment specifies is to perform data manipulation and organization steps to prepare the data for analysis, apply specific assessment functions to the appropriate data that specify the assessment guidance and Water Quality Standards (WQS), and output a stations table that uses data from the assessment window to summarize each station for review and bulk upload into CEDS. See the Automated Output section to understand the information provided in the stations table output. The automated assessment scripts are trained to systematically apply assessment guidance and WQS to all data for which the appropriate metadata is provided. This supervised system merely applies the rules that developers have programmed to mimic assessment protocols. These scripts do not provide assessment decisions. It is up to the human reviewer (regional assessor) to either accept the automated result or use best professional judgment to e.g. redact questionable data or interpret complex natural systems. "],["metadata-attribution.html", "3.1 Metadata Attribution", " 3.1 Metadata Attribution The automated assessment process hinges on each station having the appropriate metadata to analyze all raw data. The required metadata for each station include what Water Quality Standards apply to the station and which Assessment Unit(s) describe the station. One or more station can be included in an Assessment Units. It is ultimately the Assessment Units that are used to determine whether or not designated uses are met, which is where the automation stops and the human analysis component is required. In practice, metadata are spatially joined to stations by a rigorous data organization, spatial joining, and QA process that is detailed below. This automated process runs on the Assessment Data Analysts computer and results are provided to regional assessment staff for individual review. This application is known as the Regional Metadata Validation Tool and is hosted on the R server. It is up to each regional assessor to manually review each suggested metadata link prior to the assessment start date. After stations have completed the manual review process, they can be analyzed using the automated assessment scripts. Detailed intructions on how to use the Regional Metadata Validation Tool is available in the Regional Metadata Validation Tool How To section. 3.1.1 Distinct Sites Before station metadata can be linked to stations, a list of unique stations that were sampled in a given assessment window is required. Because multiple data sources are combined for an assessment, each unique station from each data source with data in the given IR window are included in this list. For the purposes of the Automated Assessment User Guide, we will overview the process with a snippet of conventionals and citizen monitoring data types. Please see the official script for more information. library(tidyverse) library(sf) ## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3 conventionals &lt;- readRDS(&#39;exampleData/conventionals.RDS&#39;) %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% dplyr::select(FDT_STA_ID, Latitude, Longitude) %&gt;% mutate(Data_Source = &#39;DEQ&#39;) conventionals[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) citmon &lt;- readRDS(&#39;exampleData/citmon.RDS&#39;) %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% dplyr::select(FDT_STA_ID, Latitude, Longitude, Data_Source) citmon[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) distinctSites &lt;- bind_rows(conventionals, citmon) distinctSites[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) This process is repeated for all datasets that have data included in the assessment window. These multiple datasets are combined into a single object named distinctSites that we will then compare to existing WQS and AU information. Where stations in our distinctSites object lack either of these pieces of metadata, we must attribute them. 3.1.2 Join Assessment Region and Subbasin Information For rendering purposes in the metadata review application, it is important to have each station linked to the appropriate Assessment Region and Subbasin to limit the amount of data called into the application just to essential information. This information is also important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing. The next chunk reads in the necessary assessment region and subbasin spatial data and spatially joins all sites to these layers. If the sites are missing from the distinctSites_sf object, that means the point plots outside either the assessment region or subbasin polygon. These missingSites are dealt with individually and forced to join to the nearest assessment region and subbasin before rejoining the distinctSites_sf object. assessmentLayer &lt;- st_read(&#39;GIS/AssessmentRegions_VA84_basins.shp&#39;) %&gt;% st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection subbasinLayer &lt;- st_read(&#39;GIS/DEQ_VAHUSB_subbasins_EVJ.shp&#39;) %&gt;% rename(&#39;SUBBASIN&#39; = &#39;SUBBASIN_1&#39;) distinctSites_sf &lt;- st_as_sf(distinctSites, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) %&gt;% # add coordinate reference system, needs to be geographic for now bc entering lat/lng, st_intersection(assessmentLayer ) %&gt;% st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) # if any joining issues occur, that means that there are stations that fall outside the joined polygon area # we need to go back in and fix them manually if(nrow(distinctSites_sf) &lt; nrow(distinctSites)){ missingSites &lt;- filter(distinctSites, ! FDT_STA_ID %in% distinctSites_sf$FDT_STA_ID) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) closest &lt;- mutate(assessmentLayer[0,], FDT_STA_ID =NA) %&gt;% dplyr::select(FDT_STA_ID, everything()) for(i in seq_len(nrow(missingSites))){ closest[i,] &lt;- assessmentLayer[which.min(st_distance(assessmentLayer, missingSites[i,])),] %&gt;% mutate(FDT_STA_ID = missingSites[i,]$FDT_STA_ID) %&gt;% dplyr::select(FDT_STA_ID, everything()) } missingSites &lt;- left_join(missingSites, closest %&gt;% st_drop_geometry(), by = &#39;FDT_STA_ID&#39;) %&gt;% st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) %&gt;% dplyr::select(-c(geometry), geometry) %&gt;% dplyr::select(names(distinctSites_sf)) distinctSites_sf &lt;- rbind(distinctSites_sf, missingSites) } 3.1.3 Join Stations to WQS All stations are then spatially joined to the current WQS spatial layers to link each unique StationID to a unique WQS_ID. These unique WQS_IDs are a concatination of the waterbody type, basin code (e.g. 2A, 2B, 2C, etc.), and a number associated with each segment in the spatial layer. Waterbody types include: RL = Riverine Line LP = Lacustrine Polygon EL = Estuarine Line EP = Estuarine Polygon Since transitioning the WQS storage from local (individual assessor files) to a centralized system (on the R server for multiple program uses), the number of stations that require WQS attribution decreases significantly each IR cycle. To attribute each station to WQS information, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can send that information to an interactive application for humans to manually verify. Where WQS_ID information is already available for stations, they are first removed from the WQS snapping process as to not repeat efforts unnecessarily. WQS_IDs are maintained across WQS layer updates, so any new WQS information will be updated when the stations are joined to the WQS metadata. You can view the available WQSlookup table stored on the server by using the below script. Please see the DEQ R Methods Encyclopedia for information on how to retrieve pinned data from the server. WQStableExisting &lt;- pin_get(&#39;ejones/WQSlookup&#39;, board = &#39;rsconnect&#39;) distinctSitesToDoWQS &lt;- filter(distinctSites_sf, ! FDT_STA_ID %in% WQStableExisting$StationID) Here is the table used to store link information from stations to appropriate WQS. WQStable &lt;- tibble(StationID = NA, WQS_ID = NA) 3.1.4 Spatially Join Polygons Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first. 3.1.4.0.1 Estuarine Polygons Here we find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas. This process also removes any estuarine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS). We will bring in (source) a custom function that runs the analysis for us, feed it our latest WQS information, and let the function run across all input stations. You can see the latest version of the sourced script in this repository. source(&#39;preprocessingModules/WQS_estuaryPoly.R&#39;) # Bring in estuary layer estuarinePolys &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_transform(4326) WQStable &lt;- estuaryPolygonJoin(estuarinePolys, distinctSitesToDoWQS, WQStable) rm(estuarinePolys) # clean up workspace Remove stations that fell inside estuarine polygons from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) 3.1.4.0.2 Lake Polygons Next find any sites that fall into a lake WQS polygon. This method is applied to all subbasins at once as it is a simple spatial join. You can see the latest version of the sourced script in this repository. source(&#39;preprocessingModules/WQS_lakePoly.R&#39;) lakesPoly &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_transform(4326) WQStable &lt;- lakePolygonJoin(lakesPoly, distinctSitesToDoWQS, WQStable) rm(lakesPoly) # clean up workspace Remove stations that fell inside lake polygons from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) 3.1.5 Spatially Join WQS Lines Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS and where stations remain we will try the estuarine WQS snap. 3.1.5.0.1 Riverine Lines To do this join, we will buffer all sites that dont fall into a polygon layer by a set sequence of distances. The output will add a field called Buffer Distance to the WQStable to indicate distance required for snapping. If more than one segment is found within a set buffer distance, then many rows will be attached to the WQStable with the single identifying station name. It is up to the QA tool to help the user determine which of these UIDs are correct and drop the other records. We then remove any riverine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS). source(&#39;snappingFunctions/snapWQS.R&#39;) riverine &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) WQStable &lt;- snapAndOrganizeWQS(distinctSitesToDoWQS, &#39;FDT_STA_ID&#39;, riverine, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters WQStable) rm(riverine) # clean up workspace Remove stations that attached to riverine segments from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) We can use this one last opportunity to test stations that didnt connect to the riverine WQS against the estuarine lines WQS as a one last hope of attributing some WQS information. We will take all stations from the WQStable that didnt snap to any WQS segments (Buffer Distance ==No connections within 80 m) and add those back in to our distinctSitesToDoWQS list to try to snap them to the estuarine lines spatial data. distinctSitesToDoWQS &lt;- filter(WQStable, `Buffer Distance` ==&#39;No connections within 80 m&#39;) %&gt;% left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %&gt;% rename(&#39;StationID&#39;= &#39;FDT_STA_ID&#39;), by=&#39;StationID&#39;) %&gt;% dplyr::select(-c(geometry)) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = T, # don&#39;t remove these lat/lon cols from df crs = 4326) 3.1.5.0.2 Estuarine Lines If a site doesnt attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time. Removes any estuary lines sites from the data frame of unique sites that need WQS information. source(&#39;snappingFunctions/snapWQS.R&#39;) estuarineLines &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp&#39; , fid_column_name = &quot;OBJECTID&quot;) #%&gt;% #st_transform(102003) # forcing to albers from start bc such a huge layer #st_transform(4326) # Only send sites to function that could be in estuarine environment WQStable &lt;- snapAndOrganizeWQS(filter(distinctSitesToDoWQS, SUBBASIN %in% c(&quot;Potomac River&quot;, &quot;Rappahannock River&quot;, &quot;Atlantic Ocean Coastal&quot;, &quot;Chesapeake Bay Tributaries&quot;, &quot;Chesapeake Bay - Mainstem&quot;, &quot;James River - Lower&quot;, &quot;Appomattox River&quot;, &quot;Chowan River&quot;, &quot;Atlantic Ocean - South&quot; , &quot;Dismal Swamp/Albemarle Sound&quot;)), &#39;StationID&#39;, estuarineLines, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters WQStable) rm(estuarineLines) Remove stations that attached to estuarine segments from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! StationID %in% WQStable$StationID) Double check no stations were lost in these processes. #Make sure all stations from original distinct station list have some sort of record (blank or populated) int the WQStable. distinctSitesToDoWQS &lt;- filter(distinctSitesToDo, ! FDT_STA_ID %in% WQStableExisting$StationID) distinctSitesToDoWQS$FDT_STA_ID[!(distinctSitesToDoWQS$FDT_STA_ID %in% unique(WQStable$StationID))] 3.1.6 Assign something to WQS_ID so sites will not fall through the cracks when application filtering occurs We dont want to give everyone all the stations that didnt snap to something, so we need to at least partially assign a WQS_ID so the stations get into the correct subbasin on initial filter. If a station snapped to nothing, we will assigning it a RL WQS_ID and subbasin it falls into by default. WQStableMissing &lt;- filter(WQStable, is.na(WQS_ID)) %&gt;% # drop from list if actually fixed by snap to another segment filter(! StationID %in% filter(WQStable, str_extract(WQS_ID, &quot;^.{2}&quot;) %in% c(&#39;EL&#39;,&#39;LP&#39;,&#39;EP&#39;))$StationID) %&gt;% left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, BASIN_CODE) %&gt;% st_drop_geometry(), by = c(&#39;StationID&#39; = &#39;FDT_STA_ID&#39;)) %&gt;% # some fixes for missing basin codes so they will match proper naming conventions for filtering mutate(BASIN_CODE1 = case_when(is.na(BASIN_CODE) ~ str_pad( ifelse(grepl(&#39;-&#39;, str_extract(StationID, &quot;^.{2}&quot;)), str_extract(StationID, &quot;^.{1}&quot;), str_extract(StationID, &quot;^.{2}&quot;)), width = 2, side = &#39;left&#39;, pad = &#39;0&#39;), TRUE ~ as.character(BASIN_CODE)), BASIN_CODE2 = str_pad(BASIN_CODE1, width = 2, side = &#39;left&#39;, pad = &#39;0&#39;), WQS_ID = paste0(&#39;RL_&#39;, BASIN_CODE2,&#39;_NA&#39;)) %&gt;% dplyr::select(-c(BASIN_CODE, BASIN_CODE1, BASIN_CODE2)) WQStable &lt;- filter(WQStable, !is.na(WQS_ID)) %&gt;% filter(! StationID %in% WQStableMissing$StationID) %&gt;% bind_rows(WQStableMissing) 3.1.7 Join Stations to AUs 3.1.8 Where does this information go? The above analyses help populate the Regional Metadata Validation Tool. After assessors manually review each station that requires WQS/AU information, this data is consolidated and stored on the R server as a pinned dataset (see for details on that process). That dataset feeds subsequent tools including the Riverine Assessment App, Lakes Assessment App, and Bioassessment Dashboard in addition to querying tools including the CEDS WQM Data Query Tool and CEDS Benthic Data Query Tool. The data is provided via the internal GIS services in the WQM Stations (All stations with full attributes) layer hosted on the GIS REST service and GIS Staff Application. In addition to the published tools that source this data, the dataset is included in countless data analysis projects and products. The entire dataset may be pulled from the R Connect service using the following script. Please see the DEQ R Methods Encyclopedia for information on how to retrieve pinned data from the server. pin_get(&#39;ejones/WQSlookup&#39;, board = &#39;rsconnect&#39;) # raw lookup table pin_get(&quot;ejones/WQSlookup-withStandards&quot;, board = &quot;rsconnect&quot;) # lookup table with WQS information joined "],["organize-metadata.html", "3.2 Organize Metadata", " 3.2 Organize Metadata After all stations for a given assessment window have the appropriate WQS and AU information attributed, there are a number of data organization steps that still need to happen before the data are ready for the automated assessment scripts. These steps include: Organizing all new WQS/AU metadata from the R server and adding new data to pinned datasets Reorganizing the conventionals dataset to match schema for automated assessment scripts Adding Secchi Depth to conventionals dataset Clean up PCB dataset Clean up Fish Tissue dataset 3.2.1 3.2.2 Organizing all new WQS/AU metadata from the R server 3.2.2.1 Adding new WQS/AU data to pinned datasets 3.2.3 Reorganizing the conventionals dataset to match schema for automated assessment script The official (SAS) conventionals data schema tends to vary from IR to IR. It is essential that this data are provided to the R scripts exactly how the scripts expect data, so each cycle the provided conventionals dataset must be meticulously QAed to ensure all data are of expected name/type. Additionally, in order to automate the assessment of citizen monitoring data, we must augment the provided conventionals dataset to accommodate the level schema for each citizen monitoring parameter. The nuanced data manipulation steps required to convert the conventionals data and citizen monitoring data from the provided format to the required format for automated analyses are beyond the scope of this book. Instead, the required data schema for automated assessment is provided below. conventionalsSchema &lt;- readRDS(&#39;exampleData/schemaFin.RDS&#39;) conventionalsSchema %&gt;% # preview data DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) 3.2.4 Adding Secchi Depth to conventionals dataset This data are not included in the official (SAS) conventionals data pull but are required for Trophic State Index (TSI) analyses for some lacustrine stations. 3.2.5 Clean up PCB dataset 3.2.6 Clean up Fish Tissue dataset "],["individual-parameter-analyses.html", "3.3 Individual Parameter Analyses", " 3.3 Individual Parameter Analyses 3.3.1 Temperature 3.3.2 Dissolved Oxygen 3.3.3 pH 3.3.4 Bacteria 3.3.5 Nutrients 3.3.6 Ammonia 3.3.7 PCB 3.3.8 Metals 3.3.9 Fish Tissue 3.3.10 Benthics "],["automated-output.html", "3.4 Automated Output", " 3.4 Automated Output "],["regional-metadata-validation-tool-how-to.html", "Chapter 4 Regional Metadata Validation Tool How To", " Chapter 4 Regional Metadata Validation Tool How To The Regional Metadata Validation Tool is located on the R server. The purpose of this tool is to streamline the QA of metadata attached to individual stations that was performed by automated scripts. This metadata include Water Quality Standards (WQS) and Assessment Unit (AU) information that are necessary to properly analyze and organize assessment results. A scripted process automates the data organization of stations from disparate data sources, spatially joins these sites to the appropriate WQS and AUs where information is not available from previous cycles, and runs basic QA on the output to ensure all stations that were expected to receive metadata did in fact get attributed. These scripts are detailed in the Metadata Attribution section. It is the responsibility of each regional assessor to review the metadata linked through the automated snapping process to ensure the closest snapped information does in fact apply to the given station. This application offers an opportunity to perform QA on the spatial WQS and AU layers and communicate with Central Office Assessment/WQS staff when mistakes are found. "],["application-orientation.html", "4.1 Application Orientation", " 4.1 Application Orientation This application expedites the review of the spatially joined output by organizing stations into the appropriate waterbody type, Assessment Region, and subbasin. The application is divided into two tabs (corresponding to AU review and WQS review) that can be navigated between using the navigation bar at the top of the page. The application opens to the Assessment Unit Review tab by default, but there is no specified order in which a reviewer must interact with either tab. Data for each tab are saved back to the R server independently. Each of these tabs have a drop down option that is accessible by clicking the triangle adjacent to the name of the tab. These options are identical for both the Assessment Unit QA and Water Quality Standards QA tabs. The top option called Watershed Selection allows users to choose certain types of stations to review and the bottom option called Manual Review allows users to visualize the stations selected in the Watershed Selection tab option in more detail. The final tab called About offers instructions to the user similar to what is detailed below. The following application workflow is applicable to both the Assessment Unit QA and Water Quality Standards QA sections of the application. For brevity, this user guide will only go over instructions for the WQS side of the application, but the application flow applied to the WQS side of the application mimics the application flow applied on the AU side of the application. "],["application-use-water-quality-standards-qa-watershed-selection-tab.html", "4.2 Application Use: Water Quality Standards QA- Watershed Selection Tab", " 4.2 Application Use: Water Quality Standards QA- Watershed Selection Tab On load, the tab will provide three related drop down menus on the left hand side of the page. These interact in a top-down hierarchical system where the selection in the top drop down, Select Waterbody Type, dictates available options in the middle drop down, Select DEQ Assessment Region, which then dictates the available options in the bottom drop down, Select Subbasin. Users navigate through these cascading filters to identify the precise waterbody type (e.g. riverine, lacustrine, or estuarine), assessment region, and subbasin to tackle QA in a given session. Once the user is happy with their selection, they must click the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button. This button uses the user provided information to retrieve all the applicable stations that require QA. This information is sourced from the R server in addition to all the applicable spatial layers necessary to produce meaningful maps in the subsequent Manual Review tab option. Once this information is retrieved from the R server, a basic interactive map of the selected subbasin populates the tab as well as verbal breakdowns of the type of information that needs to be reviewed for the selected subbasin (Preprocessing Data Recap for Selected Region/Subbasin/Type Combination). Based on the summarized information about the stations in the selected subbasin, the user may choose to begin reviewing that subbasin or they might scroll through other waterbody type/region/subbasin combinations until a more desirable to do list is identified. Each time a user changes any of the drop down inputs, they must press the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button to retrieve the associated station breakdown from the chosen subbasin. A small progress bar appears in the lower right corner of the browser window to communicate that large spatial data are being transmitted to the app. The popup disappears when the map re-renders and loads the new spatial information. Once a user is ready to review the stations in the selected subbasin, the user must navigate to the Manual Review tab under the Water Quality Standards QA drop down on the navigation bar. "],["application-use-water-quality-standards-qa-manual-review-tab.html", "4.3 Application Use: Water Quality Standards QA- Manual Review Tab", " 4.3 Application Use: Water Quality Standards QA- Manual Review Tab On load, the Manual Review tab renders three buttons, an interactive map of Virginia, and an area for output information from the map that is blank until user input dictates. 4.3.1 Map Orientation The map allows users to zoom and pan interactively with the mouse. In the top left corner there are a number of tools to aid users. The plus and minus buttons offer fixed zoom in/out. The home button returns the map orientation to the original zoom level in case a user gets lost. The magnifying glass button allows users to search for and zoom to a selected station. To use this feature, click on the magnifying glass, begin typing the StationID of interest in the text box (after each character entered, a dropdown of options based on the characters entered appears), once a station is selected, the map will zoom to that station and highlight it with a red circle. You may notice that if there isnt the All Stations in Basin layer on, then no station point appears inside the red circle. Additional layers available in the map (by hovering over the layers button on the left side) include All Stations in Basin (all other stations identified in the basin with data for the given IR window) and Assessment Regions (regional assessment boundaries). 4.3.2 What do the buttons do? The three buttons correspond to the station snapping summaries presented on the Watershed Selection tab. In the example above, we can see this subbasin has: * There are 8 stations that snapped to 1 WQS segment in preprocessing. * There are 1 stations that snapped to &gt; 1 WQS segment in preprocessing. * There are 1 stations that snapped to 0 WQS segments in preprocessing. By clicking the Plot stations that snapped to 1 WQS Segment, users will see all the stations that only retrieved one WQS segment in the spatial snapping process. They are colored from green to light red based on the buffer distance required to snap to a segment (within the smallest set buffer distance of the sequence dictated to the automated snapping functions). Green stations snapped to segments at closer distances compared to red sites. The legend for the buffer distance stations colors is presented in the right corner of the app. By clicking the Plot stations that snapped to &gt;1 WQS Segment, users will see all the stations that retrieved multiple WQS segments in the spatial snapping process (within the smallest set buffer distance of the sequence dictated to the automated snapping functions). They are colored to red to indicate these stations need more attention. By clicking the Plot stations that snapped to 0 WQS Segment, users will see all the stations that retrieved no WQS segments in the spatial snapping process (within the largest buffer distance of the sequence dictated to the automated snapping functions). They are colored to orange to indicate these stations need the most attention. If any of the categories listed above had 0 stations, a warning message will appear in the lower right corner of the app indicating there is nothing for the button to plot. 4.3.3 Application Operation The goal of this section of the application is to review each station and either accept or adjust the provided WQS information snapped to the site. To review a site, users may zoom to a selected map area and click on a site. By clicking a site, users are selecting a site to QA, as indicated by the green halo around the selected site. The associated metadata is now populated in the Stations Data and Spatially Joined WQS tab below the map. The Selected Station Information table details information about the station including: * the WQS_ID of snapped segment(s) * the Buffer Distance (distance required to snap to the linked segment(s)) * n (the number of segments linked to the station during spatial processing) * a hyperlink to the internal GIS Web Application that pulls up the appropriate WQS layer and station information in a new browser tab * additional station metadata information The Spatially Joined WQS Information table details information about the WQS segment(s) attached to the selected station including: * WQS_ID (unique code attributed to each WQS segment) * GNIS_Name (name of the WQS segment) * SEC (WQS section) * CLASS (WQS class) * SPSTDS (WQS special standards) * SECTION_DESCRIPTION (narrative description of the section location) * PWS (whether or not the section is designated as a Public Water Supply) * Tier_III (whether or not the section is designated as a Tier III water) * additional WQS segment information Users may click on more than one station and subsequently reveal information about more than one station in the Stations Data and Spatially Joined WQS tab below the map. If more than one row is presented in a table, a y scroll bar on the right side of the table will automatically appear such that the user may access all the table information. 4.3.3.1 Application Operation: Clear Selection To deselect a station or stations, click the blue Clear Selection button below the map. ` 4.3.4 Application Operation: Station that Snapped to 1 WQS Segment Should a user select a station for review that snapped to 1 WQS segment, the process for QAing that station is simple. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. Additionally, when the Plot stations that snapped to 1 WQS Segment button was pressed, additional spatial layers became available in the layers drop down of the map. The user may now turn on the layer called WQS Segments of Stations Snapped to 1 Segment to reveal all the segments in that category. These segments may be clicked on in the map to reveal the same information from the Selected Station Information table in a popup on the map. 4.3.4.1 Application Operation: Accept If the user is satisfied with the snapped WQS segment, they may press the blue Accept button to reveal a modal window named Accept Snapped WQS. This modal shows the information from the Selected Station Information table one last time for review. There is a textbox below the table where users may input any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. Once the user presses the Accept button in the Accept Snapped WQS modal, the station and WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.4.2 Application Operation: Manual WQS Adjustment (1 segment) If the user is not satisfied with the snapped WQS segment, they may press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. This modal shows the information from the Selected Station Information table one last time for review. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Pro Tip: By clicking the record in the WQS_ID cell of the table on the modal window three time quickly, the cell contents will be highlighted. Users may use the keyboard shortcuts to copy(ctrl+c)/paste(ctrl+v) this information into the text box below the table. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.4.3 Application Operation: Export Reviews All the work the user has completed so far accepting or manually adjusting WQS are only saved in the local application environment. The only way to preserve the work completed in the application is to press the green Export reviews button. It is highly recommended that users click the Export reviews button frequently (every 3-4 stations or after a particularly challenging review) to ensure their work is in fact saved on the server. Users may undo any accidental work still in their local application environment by either closing the app or returning to the Watershed Selection tab without pressing the Export reviews button. This work flow would look like: User selects station, Accept User selects station, Accept User selects station, Accept User presses Export reviews to save work to the R server User selects station, Accept- user realizes they made a mistake and doesnt want to save that accept to the server User closes the app or navigates back to the Watershed Selection tab without pressing the Export reviews button- all information completed in the app since the last time Export reviews was pressed will not be saved User opens the app or clicks the Begin Review With Subbasin Selection (Retrieves Last Saved Result) button on the Watershed Selection tab to pull the last information saved on the server User proceeds to the Manual Review tab and continues work 4.3.5 Application Operation: Station that Snapped to &gt;1 WQS Segment Should a user select a station for review that snapped to &gt;1 WQS segment, the process for QAing that station is relatively simple. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. Additionally, when the Plot stations that snapped to &gt;1 WQS Segment button was pressed, additional spatial layers became available in the layers drop down of the map. The user may now turn on the layer called WQS Segments of Stations Snapped to &gt;1 Segment to reveal all the segments in that category. These segments are plotted on a color ramp to distinguish them from one another and may be clicked on in the map to reveal the same information from the Selected Station Information table in a popup on the map. The GIS Web App hyperlinks may also be useful to use at this stage. 4.3.5.1 Application Operation: Manual WQS Adjustment (&gt; 1 segment) If the user needs to choose between multiple WQS segments, they must press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. The user cannot accept a single StationID that is linked to multiple WQS_IDs using the Accept button. The Manually Adjust WQS modal shows the information from the Selected Station Information table one last time for review. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Pro Tip: By clicking the record in the WQS_ID cell of the table on the modal window three time quickly, the cell contents will be highlighted. Users may use the keyboard shortcuts to copy(ctrl+c)/paste(ctrl+v) this information into the text box below the table. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.6 Application Operation: Station that Snapped to 0 WQS Segments Should a user select a station for review that snapped to no WQS segments, the process for QAing that station is more involved. With the station highlighted with the green halo, the user reviews all the information presented below the map in the Stations Data and Spatially Joined WQS tab. To preserve application rendering time, additional spatial layers are not available in the layers drop down of the map for this scenario. Instead, the user should click the Open Link In New Tab button in the Selected Station Information table to navigate to the GIS Web App to visualize all WQS segments around the station. 4.3.6.1 Application Operation: Manual WQS Adjustment (0 segments) Once the user has chosen an appropriate WQS segment, they must press the blue Manual WQS Adjustment button to reveal a modal window named Manually Adjust WQS. The user cannot accept a single StationID that is linked to no WQS_IDs using the Accept button. The Manually Adjust WQS modal shows the information from the Selected Station Information table one last time for review. This will show information about the station but no WQS_ID. There are two textboxes below the table where users may input the desired WQS_ID to attach to the StationID and any additional comments and documentation to archive with the snapped StationID and WQS segment information. This could be why the selection was made, notes to future assessors about this site/segment, or nothing at all. At this point the user may navigate away from the modal by pressing Dismiss or Cancel if they choose to not proceed with that station. It is highly recommended that users copy and paste the desired WQS_ID from either this application or the GIS Web App to ensure the correct WQS_ID information is entered into the text box. Once the user presses the Accept button in the Manually Adjust WQS modal, the station and manually entered WQS segment are linked, the modal disappears, the station changes to a purple (completed) color, and the count of stations to do in that category at the top of the page goes down by one. 4.3.7 Application Operation: There are no more stations to attribute Congratulations! That means you have completed the review of all the stations in this subbasin. Make sure you press the Export reviews button one more time to ensure all your hard work is saved on the server. At this point, the user may navigate back to the Watershed selection tab to choose another subbasin to review or close the application to end their metadata review session. "],["having-problems.html", "4.4 Having problems?", " 4.4 Having problems? Please contact Emma Jones (emma.jones@deq.virginia.gov) should you encounter any problems using the application, wonky situations to review, or long rendering times/app crashing. "],["riverine-application-how-to.html", "Chapter 5 Riverine Application How To", " Chapter 5 Riverine Application How To The Riverine Assessment App is located on the R server. update link to 2024 when pushed "],["lacustrine-application-how-to.html", "Chapter 6 Lacustrine Application How To", " Chapter 6 Lacustrine Application How To The Lakes Assessment App is located on the R server. update link to 2024 when pushed "]]
