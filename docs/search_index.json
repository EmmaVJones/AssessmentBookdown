[["index.html", "DEQ Water Quality Automated Assessment User Guide Chapter 1 Introduction", " DEQ Water Quality Automated Assessment User Guide DEQ Automated Assessment Team Last Updated: 2022-07-25 Chapter 1 Introduction This manual serves as a companion document to DEQs Water Quality Assessment (WQA) automated assessment methodology for completion of the biennial 305b/303d Integrated Report. Virginia is among a handful of state agencies that have organized concerted efforts to systematically automate and expedite the water quality assessment process. These entities have all taken different approaches to best meet their state-specific analysis and reporting needs. Through the Tools for Automated Data Assessment (TADA) working group, EPA is organizing an effort to share code and create national tools to assist partners with assessment analyses. Considering the technological advances in water monitoring that have increased the quantity and types of data requiring consideration during an assessment period, as well as the static or decreasing staff time allocated to the assessment process, automation appears the only solution to keep up with Clean Water Act requirements and keep the public informed of the status of waterbody health and restoration timelines for those waterbodies not meeting water quality standards. By embracing automated assessment procedures, entities aim to provide constituents with higher quality, standardized, and transparent water quality assessment results while adhering to federally mandated deadlines. Virginia has taken a hybrid approach to the process of automating the water quality assessments. DEQ relies upon an automated process to manipulate hundreds of thousands of water quality data collected throughout the state during a given assessment period (six year window). These scripts evaluate each record for exceedances of appropriate Water Quality Standards. Regional assessment staff may use interactive web-based analytical tools provide more context to tabular results of exceedance analyses to assist with Quality Assurance/Quality Control (QA/QC) prior to submitting data to EPAs Assessment, Total Maximum Daily Load (TMDL) Tracking and Implementation System (ATTAINS) system through DEQs internal Comprehensive Environmental Data System (CEDS). This approach maximizes the benefits of each partner in the assessment process, allowing computers to excel at systematic and expedited data manipulation and analysis at scale and allowing humans to digest multiple lines of evidence to identify any potential data collection or analysis errors or areas of environmental concern. "],["HowToUseDocument.html", "1.1 How to Use Document", " 1.1 How to Use Document The following report is an agency effort to facilitate the further adoption of automated assessment methods statewide. The intention of this project is to provide a non-programmer audience with accessible and understandable narratives describing the automated water quality assessment process. This document is not a comprehensive WQA Guidance Manual, nor is it an introduction to using the R programming language. Here, we document the overall automated assessment process, explaining reasons certain decisions were made, and how to unpack analyses. "],["ProjectHistory.html", "1.2 Project History", " 1.2 Project History The agency began efforts toward automating components of the IR with a dissolved metals assessment written in SAS. These tabular results were provided to regional assessment staff along with raw data queries encompassing data stored in CEDS for each IR data window. The Freshwater Probabilistic Monitoring program began automating analyses and a final report for inclusion in the 2016 IR using the open source R programming language. These practices have been further refined each IR cycle, improving report quality and graphics as well as significantly reducing the amount of time required to generate a report after data collection. These methods were identified as a possible solution to the ongoing challenges in the WQA program to complete vast amounts of data analysis on increasingly short timelines with limited staff. An effort to systematically analyze all water quality data stored in CEDS for assessments began in 2017. As a pilot project, lakes and reservoirs in the Blue Ridge Regional Office (BRRO) assessment region were selected for first waterbody type to undergo automation and receive an interactive web-based tool to assist regional assessment staff for the 2018 IR. These initial automated assessment efforts were completed using the open source R programming language and interactive applications were built using the Shiny package. Rivers and streams followed suit for the 2020 IR, joining lacustrine waterbodies with automated assessment methods and interactive assessment tools. The 2020 IR automated methods were scaled from just the BRRO region to statewide applicability. A pilot project to incorporate citizen monitoring data requiring assessment was undertaken in the BRRO assessment region for the 2020 IR. This pilot proved effective in standardizing and increasing efficiency of incorporating this disparate data and was officially adopted as a process for future IR windows. After thorough QA from regional assessment staff across the state, the riverine and lacustrine automated assessment tools were rebooted for the 2022 IR with increased functionality and the ability to assess more parameters, including those not stored in CEDS. However, due to delays organizing citizen monitoring data statewide, automated results for these data were not provided for the 2022 IR. A new database schema for archiving station-specific water quality standards and assessment unit information was implemented. This system benefited the assessment process as well as numerous DEQ water programs that previously did not have access to WQS information at that spatial scale. Appendices and fact sheets for the IR were generated using R and Rmarkdown for the first time during the 2022 IR. The 2024 IR further refines improvements to the riverine and lacustrine automated assessment methods and interactive tools. By partnering with the Chesapeake Monitoring Cooperative (CMC), DEQ leveraged an existing public-facing citizen monitoring data portal to expand utility outside the Chesapeake Bay watershed and incorporate all of Virginias citizen monitoring data. These data are now automatically cleaned and stored by the CMC and can more easily be incorporated in the automated assessment process with web scraping techniques. To date, no estuarine-specific assessment methods have been completed, but the WQA program is investigating utility and potential adoption among regions with estuarine waters. "],["Acknowledgements.html", "1.3 Acknowledgements", " 1.3 Acknowledgements Many Water Quality Assessment (WQA) and Water Quality Standards (WQA) staff have contributed to this effort: Emma Jones (emma.jones@deq.virginia.gov) Kristen Bretz (kristen.bretz@deq.virginia.gov) Jason Hill (jason.hill@deq.virginia.gov) Sandy Mueller (sandra.mueller@deq.virginia.gov) Cleo Baker (cleo.baker@deq.virginia.gov) Amanda Shaver (amanda.shaver@deq.virginia.gov) Tish Robertson (tish.robertson@deq.virginia.gov) Paula Main (paula.main@deq.virginia.gov) Mary Dail (mary.dail@deq.virginia.gov) Martha Chapman (martha.chapman@deq.virginia.gov) Sara Jordan (sara.jordan@deq.virginia.gov) Kristie Britt (kristie.britt@deq.virginia.gov) Jennifer Palmore (jennifer.palmore@deq.virginia.gov) Kelley West (kelley.west@deq.virginia.gov) Rebecca Shoemaker (rebecca.shoemaker@deq.virginia.gov) Please direct any project questions to Emma Jones (emma.jones@deq.virginia.gov). "],["data-organization.html", "Chapter 2 Data Organization", " Chapter 2 Data Organization The assessment process requires hundreds of thousands of rows of data collected and stored by DEQ, other state agencies, and citizen partners. These disparate data sources require various levels of data manipulation prior to any analysis. SAS and R are used for the majority of the assessment data cleaning and manipulation processes. 2.0.1 Data Location Most data used for assessments are stored in DEQs internal Comprehensive Environmental Data System (CEDS) and made available through a direct connection to the reporting database known as ODS. The agency is working towards storing all data required for assessments in CEDS, but as of the time of writing the following datasets are stored in locations outside of CEDS: * Fish Tissue Data * PCB Data * VDH Data * Citizen Monitoring Data * Station Metadata It is important to note that although the data that consitute the conventionals dataset are derive from CEDS/ODS, official assessment records of this data are only stored locally in Microsoft Excel outputs. 2.0.2 Data Availablility Most of the following data are provided at the beginning of the assessment process (approximately March of an assessment year). Any delays in data availability have ripple effects on the ability of regional assessors to complete their work on time. Should data be provided for assessment after the expected availability date, assessors may not be able to include said data in a given assessment window. Exceptions to the March data availability date usually apply to Citizen Monitoring, bioassessment, and fish tissue data. Citizen Monitoring data have historically been provided to regional assessment staff in April of a given assessment year. Bioassessment data for the most recent two years of an assessment window trickles in to the assessment process through the summer of a given assessment year due to the lengthy identification process associated with benthic macroinvertebrate samples. Fish tissue data requires protracted laboratory analyses before it is provided back to DEQ from contractor labs for assessment purposes. Due to these data delays, regional assessment staff generally have to delay certain assessment steps until all data is available for a given station/Assessment Unit, making automated assessment methods ever more important when data are provided. "],["conventionals-data.html", "2.1 Conventionals Data", " 2.1 Conventionals Data The conventionals dataset contains the bulk of the data analyzed during any given assessment window. Historically, this dataset has been queried using SAS and a direct connection to the raw monitoring data in the ODS reporting database (the database that makes CEDS data accessible to reporting tools). Recently, efforts to streamline the assessment process and standardize data provided across agency programs produced an effort to convert these SAS scripts into R. The conventionals query combines WQM field and analyte data with data handling steps that standardize data discrepancies like multiple lab values returned for identical sample date/times, full parameter speciation but no total value, etc. This R query and data standardization method is considered to be under development as code review is a constant part of data improvement strategies. The current version of the R based conventionals query is available here. In order to access the data the conventionals function calls, users must have a direct connection to ODS from their environment. Please see the DEQ R Methods Encyclopedia article on ODS for more information. A sample conventionals dataset is available in the exampleData directory for you to download and use when practicing the automated scripts. conventionals &lt;- readRDS(&#39;exampleData/conventionals.RDS&#39;) conventionals[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["water-column-metals.html", "2.2 Water Column Metals", " 2.2 Water Column Metals query from Roger, for now "],["sediment-metals.html", "2.3 Sediment Metals", " 2.3 Sediment Metals query from Roger, for now "],["fish-tissue-data.html", "2.4 Fish Tissue Data", " 2.4 Fish Tissue Data provided by Rick &amp; Gabe, for now "],["pcb-data.html", "2.5 PCB Data", " 2.5 PCB Data From Mark Richards, not in CEDS "],["vdh-data.html", "2.6 VDH Data", " 2.6 VDH Data Beach closure, HAB event "],["citizen-monitoring-data.html", "2.7 Citizen Monitoring Data", " 2.7 Citizen Monitoring Data Citizen Monitoring data have historically been provided to DEQ in various data formats and schema using numerous digital and analog storage methods. This data system required multiple iterations of lengthy data standardization and QA/QC processes in order to ensure the data were utilized for assessments. A standardized system requiring citizen groups to either upload their data to the Chesapeake Monitoring Cooperative (CMC) Data Explorer and DEQ scraping the CMC API using automated R scripts or provide all data to DEQ in a standardized template has replaced previous data receiving methods. Citizen data not stored in the CMC database live on DEQ staff computers and require further storage solutions. A sample citizen monitoring dataset is available in the exampleData directory for you to download and use when practicing the automated scripts. citmon &lt;- readRDS(&#39;exampleData/citmon.RDS&#39;) citmon[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["station-metadata.html", "2.8 Station Metadata", " 2.8 Station Metadata Station metadata are critical to ensuring that stations are assessed appropriately whether an assessment is conducted by hand or with automation. These metadata include the appropriate Assessment Unit and Water Quality Standard information that apply to the station. After this information is identified for a given station, it may be assessed according the the assessment rules that apply to those standards for the specified waterbody type. Metadata are provided for stations through a combination of automated spatial analyses and human review. No metadata are ever linked to stations without regional assessor review. The Metadata Attribution section covers the details of attributing metadata to each station. "],["automated-assessment-methods.html", "Chapter 3 Automated Assessment Methods", " Chapter 3 Automated Assessment Methods This chapter details the specific steps required to transform the raw data outlined in the Data Organization chapter into preliminary assessment results. The general process the automated assessment specifies is to perform data manipulation and organization steps to prepare the data for analysis, apply specific assessment functions to the appropriate data that specify the assessment guidance and Water Quality Standards (WQS), and output a stations table that uses data from the assessment window to summarize each station for review and bulk upload into CEDS. See the Automated Output section to understand the information provided in the stations table output. The automated assessment scripts are trained to systematically apply assessment guidance and WQS to all data for which the appropriate metadata is provided. This supervised system merely applies the rules that developers have programmed to mimic assessment protocols. These scripts do not provide assessment decisions. It is up to the human reviewer (regional assessor) to either accept the automated result or use best professional judgment to e.g. redact questionable data or interpret complex natural systems. "],["metadata-attribution.html", "3.1 Metadata Attribution", " 3.1 Metadata Attribution The automated assessment process hinges on each station having the appropriate metadata to analyze all raw data. The required metadata for each station include what Water Quality Standards apply to the station and which Assessment Unit(s) describe the station. One or more station can be included in an Assessment Units. It is ultimately the Assessment Units that are used to determine whether or not designated uses are met, which is where the automation stops and the human analysis component is required. 3.1.1 Distinct Sites Before station metadata can be linked to stations, a list of unique stations that were sampled in a given assessment window is required. Because multiple data sources are combined for an assessment, each unique station from each data source with data in the given IR window are included in this list. For the purposes of the Automated Assessment User Guide, we will overview the process with a snippet of conventionals and citizen monitoring data types. Please see the official script for more information. library(tidyverse) library(sf) ## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3 conventionals &lt;- readRDS(&#39;exampleData/conventionals.RDS&#39;) %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% dplyr::select(FDT_STA_ID, Latitude, Longitude) %&gt;% mutate(Data_Source = &#39;DEQ&#39;) conventionals[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) citmon &lt;- readRDS(&#39;exampleData/citmon.RDS&#39;) %&gt;% distinct(FDT_STA_ID, .keep_all = T) %&gt;% dplyr::select(FDT_STA_ID, Latitude, Longitude, Data_Source) citmon[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) distinctSites &lt;- bind_rows(conventionals, citmon) distinctSites[1:50,] %&gt;% # preview first 50 rows DT::datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) This process is repeated for all datasets that have data included in the assessment window. These multiple datasets are combined into a single object named distinctSites that we will then compare to existing WQS and AU information. Where stations in our distinctSites object lack either of these pieces of metadata, we must attribute them. 3.1.2 Join Assessment Region and Subbasin Information For rendering purposes in the metadata review application, it is important to have each station linked to the appropriate Assessment Region and Subbasin to limit the amount of data called into the application just to essential information. This information is also important for processing each region through a loop for AU connection and WQS attachment. Subbasin information is important for WQS processing. The next chunk reads in the necessary assessment region and subbasin spatial data and spatially joins all sites to these layers. If the sites are missing from the distinctSites_sf object, that means the point plots outside either the assessment region or subbasin polygon. These missingSites are dealt with individually and forced to join to the nearest assessment region and subbasin before rejoining the distinctSites_sf object. assessmentLayer &lt;- st_read(&#39;GIS/AssessmentRegions_VA84_basins.shp&#39;) %&gt;% st_transform( st_crs(4326)) # transform to WQS84 for spatial intersection subbasinLayer &lt;- st_read(&#39;GIS/DEQ_VAHUSB_subbasins_EVJ.shp&#39;) %&gt;% rename(&#39;SUBBASIN&#39; = &#39;SUBBASIN_1&#39;) distinctSites_sf &lt;- st_as_sf(distinctSites, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) %&gt;% # add coordinate reference system, needs to be geographic for now bc entering lat/lng, st_intersection(assessmentLayer ) %&gt;% st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) # if any joining issues occur, that means that there are stations that fall outside the joined polygon area # we need to go back in and fix them manually if(nrow(distinctSites_sf) &lt; nrow(distinctSites)){ missingSites &lt;- filter(distinctSites, ! FDT_STA_ID %in% distinctSites_sf$FDT_STA_ID) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) closest &lt;- mutate(assessmentLayer[0,], FDT_STA_ID =NA) %&gt;% dplyr::select(FDT_STA_ID, everything()) for(i in seq_len(nrow(missingSites))){ closest[i,] &lt;- assessmentLayer[which.min(st_distance(assessmentLayer, missingSites[i,])),] %&gt;% mutate(FDT_STA_ID = missingSites[i,]$FDT_STA_ID) %&gt;% dplyr::select(FDT_STA_ID, everything()) } missingSites &lt;- left_join(missingSites, closest %&gt;% st_drop_geometry(), by = &#39;FDT_STA_ID&#39;) %&gt;% st_join(dplyr::select(subbasinLayer, BASIN_NAME, BASIN_CODE, SUBBASIN), join = st_intersects) %&gt;% dplyr::select(-c(geometry), geometry) %&gt;% dplyr::select(names(distinctSites_sf)) distinctSites_sf &lt;- rbind(distinctSites_sf, missingSites) } 3.1.3 Join WQS to Stations All stations are then spatially joined to the current WQS spatial layers to link each unique StationID to a unique WQS_ID. Since transitioning the WQS storage from local (individual assessor files) to a centralized system (on the R server for multiple program uses), the number of stations that require WQS attribution decreases significantly each IR cycle. To attribute each station to WQS information, we first need to do all spatial joins to new WQS layer to get appropriate UID information, then we can send that information to an interactive application for humans to manually verify. Where WQS_ID information is already available for stations, they are first removed from the WQS snapping process as to not repeat efforts unnecessarily. WQS_IDs are maintained across WQS layer updates, so any new WQS information will be updated when the stations are joined to the WQS metadata. You can view the available WQSlookup table stored on the server by using the below script. Please see the DEQ R Methods Encyclopedia for information on how to retrieve pinned data from the server. WQStableExisting &lt;- pin_get(&#39;ejones/WQSlookup&#39;, board = &#39;rsconnect&#39;) distinctSitesToDoWQS &lt;- filter(distinctSites_sf, ! FDT_STA_ID %in% WQStableExisting$StationID) Here is the table used to store link information from stations to appropriate WQS. WQStable &lt;- tibble(StationID = NA, WQS_ID = NA) 3.1.4 Spatially Join Polygons Since it is much faster to look for spatial joins by polygons compared to snapping to lines, we will run spatial joins by estuarine polys and reservoir layers first. 3.1.4.0.1 Estuarine Polygons Here we find any sites that fall into an estuary WQS polygon. This method is only applied to subbasins that intersect estuarine areas. This process also removes any estuarine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS). We will bring in (source) a custom function that runs the analysis for us, feed it our latest WQS information, and let the function run across all input stations. You can see the latest version of the sourced script in this repository. source(&#39;preprocessingModules/WQS_estuaryPoly.R&#39;) # Bring in estuary layer estuarinePolys &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinepolygons_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_transform(4326) WQStable &lt;- estuaryPolygonJoin(estuarinePolys, distinctSitesToDoWQS, WQStable) rm(estuarinePolys) # clean up workspace Remove stations that fell inside estuarine polygons from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) 3.1.4.0.2 Lake Polygons Next find any sites that fall into a lake WQS polygon. This method is applied to all subbasins at once as it is a simple spatial join. You can see the latest version of the sourced script in this repository. source(&#39;preprocessingModules/WQS_lakePoly.R&#39;) lakesPoly &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/lakes_reservoirs_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) %&gt;% st_transform(4326) WQStable &lt;- lakePolygonJoin(lakesPoly, distinctSitesToDoWQS, WQStable) rm(lakesPoly) # clean up workspace Remove stations that fell inside lake polygons from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) 3.1.5 Spatially Join WQS Lines Now on to the more computationally heavy WQS line snapping methods. First we will try to attach riverine WQS and where stations remain we will try the estuarine WQS snap. 3.1.5.0.1 Riverine Lines To do this join, we will buffer all sites that dont fall into a polygon layer by a set sequence of distances. The output will add a field called Buffer Distance to the WQStable to indicate distance required for snapping. This does not get transported to the data of record, but it is useful to keep for now for QA purposes. If more than one segment is found within a set buffer distance, then many rows will be attached to the WQStable with the single identifying station name. It is up to the QA tool to help the user determine which of these UIDs are correct and drop the other records. We then remove any riverine sites from the data frame of unique sites that need WQS information (distinctSitesToDoWQS). source(&#39;snappingFunctions/snapWQS.R&#39;) riverine &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/riverine_draft2022.shp&#39;, fid_column_name = &quot;OBJECTID&quot;) WQStable &lt;- snapAndOrganizeWQS(distinctSitesToDoWQS, &#39;FDT_STA_ID&#39;, riverine, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters WQStable) rm(riverine) # clean up workspace Remove stations that attached to riverine segments from the to do list. distinctSitesToDoWQS &lt;- filter(distinctSitesToDoWQS, ! FDT_STA_ID %in% WQStable$StationID) We can use this one last opportunity to test stations that didnt connect to the riverine WQS against the estuarine lines WQS as a one last hope of attributing some WQS information. We will take all stations from the WQStable that didnt snap to any WQS segments (Buffer Distance ==No connections within 80 m) and add those back in to our distinctSitesToDoWQS list to try to snap them to the estuarine lines spatial data. distinctSitesToDoWQS &lt;- filter(WQStable, `Buffer Distance` ==&#39;No connections within 80 m&#39;) %&gt;% left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, Latitude, Longitude, SUBBASIN) %&gt;% rename(&#39;StationID&#39;= &#39;FDT_STA_ID&#39;), by=&#39;StationID&#39;) %&gt;% dplyr::select(-c(geometry)) %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = T, # don&#39;t remove these lat/lon cols from df crs = 4326) 3.1.5.0.2 Estuarine Lines If a site doesnt attach to a riverine segment, our last step is to try to attach estuary line segments before throwing an empty site to the users for the wild west of manual QA. Only send sites that could be in estuarine subbasin to this function to not waste time. Removes any estuary lines sites from the data frame of unique sites that need WQS information. source(&#39;snappingFunctions/snapWQS.R&#39;) estuarineLines &lt;- st_read(&#39;../GIS/draft_WQS_shapefiles_2022/estuarinelines_draft2022.shp&#39; , fid_column_name = &quot;OBJECTID&quot;) #%&gt;% #st_transform(102003) # forcing to albers from start bc such a huge layer #st_transform(4326) # Only send sites to function that could be in estuarine environment WQStable &lt;- snapAndOrganizeWQS(filter(distinctSitesToDoWQS, SUBBASIN %in% c(&quot;Potomac River&quot;, &quot;Rappahannock River&quot;, &quot;Atlantic Ocean Coastal&quot;, &quot;Chesapeake Bay Tributaries&quot;, &quot;Chesapeake Bay - Mainstem&quot;, &quot;James River - Lower&quot;, &quot;Appomattox River&quot;, &quot;Chowan River&quot;, &quot;Atlantic Ocean - South&quot; , &quot;Dismal Swamp/Albemarle Sound&quot;))[1:25,], &#39;StationID&#39;, estuarineLines, bufferDistances = seq(20,80,by=20), # buffering by 20m from 20 - 80 meters WQStable) rm(estuarineLines) Remove stations that attached to estuarine segments from the to do list. distinctSitesToDo &lt;- filter(distinctSitesToDo, ! StationID %in% WQStable$StationID) 3.1.6 Assign something to WQS_ID so sites will not fall through the cracks when application filtering occurs We dont want to give everyone all the stations that didnt snap to something, so we need to at least partially assign a WQS_ID so the stations get into the correct subbasin on initial filter. If a station snapped to nothing, we will assigning it a RL WQS_ID and subbasin it falls into by default. WQStableMissing &lt;- filter(WQStable, is.na(WQS_ID)) %&gt;% # drop from list if actually fixed by snap to another segment filter(! StationID %in% filter(WQStable, str_extract(WQS_ID, &quot;^.{2}&quot;) %in% c(&#39;EL&#39;,&#39;LP&#39;,&#39;EP&#39;))$StationID) %&gt;% left_join(dplyr::select(distinctSites_sf, FDT_STA_ID, BASIN_CODE) %&gt;% st_drop_geometry(), by = c(&#39;StationID&#39; = &#39;FDT_STA_ID&#39;)) %&gt;% # some fixes for missing basin codes so they will match proper naming conventions for filtering mutate(BASIN_CODE1 = case_when(is.na(BASIN_CODE) ~ str_pad( ifelse(grepl(&#39;-&#39;, str_extract(StationID, &quot;^.{2}&quot;)), str_extract(StationID, &quot;^.{1}&quot;), str_extract(StationID, &quot;^.{2}&quot;)), width = 2, side = &#39;left&#39;, pad = &#39;0&#39;), TRUE ~ as.character(BASIN_CODE)), BASIN_CODE2 = str_pad(BASIN_CODE1, width = 2, side = &#39;left&#39;, pad = &#39;0&#39;), WQS_ID = paste0(&#39;RL_&#39;, BASIN_CODE2,&#39;_NA&#39;)) %&gt;% dplyr::select(-c(BASIN_CODE, BASIN_CODE1, BASIN_CODE2)) WQStable &lt;- filter(WQStable, !is.na(WQS_ID)) %&gt;% filter(! StationID %in% WQStableMissing$StationID) %&gt;% bind_rows(WQStableMissing) 3.1.7 Join AU to Stations "],["individual-parameter-analyses.html", "3.2 Individual Parameter Analyses", " 3.2 Individual Parameter Analyses 3.2.1 Temperature 3.2.2 Dissolved Oxygen 3.2.3 pH 3.2.4 Bacteria 3.2.5 Nutrients 3.2.6 Ammonia 3.2.7 PCB 3.2.8 Metals 3.2.9 Fish Tissue 3.2.10 Benthics "],["automated-output.html", "3.3 Automated Output", " 3.3 Automated Output "],["riverine-application.html", "Chapter 4 Riverine Application ", " Chapter 4 Riverine Application "],["RiverineAppHowTo.html", "4.1 How To", " 4.1 How To "],["lacustrine-application.html", "Chapter 5 Lacustrine Application ", " Chapter 5 Lacustrine Application "],["LacustrineAppHowTo.html", "5.1 How To", " 5.1 How To "]]
